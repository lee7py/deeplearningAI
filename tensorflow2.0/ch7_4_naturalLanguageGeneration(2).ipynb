{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch7_4_naturalLanguageGeneration(2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOYBpJUD5rrxPmnw/gASWM/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch7_4_naturalLanguageGeneration(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_qDfvmcn_Za",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "title: \"Tensorflow 2.0 Tutorial ch7.4 - (2) 단어 단위 생성\"\n",
        "date: 2020-04-27T10:20:30+09:00\n",
        "tags:\n",
        "  - \"Deep Learning\"\n",
        "  - \"Python\"\n",
        "  - \"Google Colab\"\n",
        "  - \"Tensorflow 2.0\"\n",
        "  - \"Binary Classification\"\n",
        "  - \"Classification\"\n",
        "  - \"Sentiment Analysis\"\n",
        "  - \"Natural Language Generation\"\n",
        "  - \"자연어 생성\"\n",
        "  - \"텐서플로 2.0\"\n",
        "  - \"텐서플로 2.0 튜토리얼\"\n",
        "  - \"Tensorflow 2.0 Tutorial\"\n",
        "categories:\n",
        "  - \"Deep Learning\"\n",
        "  - \"딥러닝\"\n",
        "  - \"텐서플로 2.0\"\n",
        "  - \"Python\"\n",
        "  - \"Tensorflow 2.0\"\n",
        "  - \"텐서플로 2.0 튜토리얼\"\n",
        "  - \"Tensorflow 2.0 Tutorial\"\n",
        "menu: \n",
        "  python:\n",
        "    name: Tensorflow 2.0 Tutorial ch7.4 - (1) 단어 단위 생성\n",
        "---\n",
        "\n",
        "## 공지\n",
        "\n",
        "- 본 Tutorial은 교재 `시작하세요 텐서플로 2.0 프로그래밍`의 강사에게 국비교육 강의를 듣는 사람들에게 자료 제공을 목적으로 제작하였습니다. \n",
        "\n",
        "- 강사의 주관적인 판단으로 압축해서 자료를 정리하였기 때문에, 자세하게 공부를 하고 싶으신 분은 반드시 교재를 구매하실 것을 권해드립니다. \n",
        "\n",
        "![](/img/tensorflow2.0/book.jpg)<!-- -->\n",
        "\n",
        "\n",
        "- 본 교재 외에 강사가 추가한 내용에 대한 Reference를 확인하셔서, 추가적으로 학습하시는 것을 권유드립니다. \n",
        "\n",
        "\n",
        "## Tutorial\n",
        "\n",
        "이전 강의가 궁금하신 분들은 아래에서 선택하여 추가 학습 하시기를 바랍니다. \n",
        "\n",
        "- [Google Colab Tensorflow 2.0 Installation](https://chloevan.github.io/python/tensorflow2.0/googlecolab/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.1 - 난수 생성 및 시그모이드 함수](https://chloevan.github.io/python/tensorflow2.0/ch3_3_1_random_signoid/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.2 - 난수 생성 및 시그모이드 함수 편향성](https://chloevan.github.io/python/tensorflow2.0/ch3_3_2_random_signoid_bias/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.3 - 첫번째 신경망 네트워크 - AND](https://chloevan.github.io/python/tensorflow2.0/ch3_3_3_network_and/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.4 - 두번째 신경망 네트워크 - OR](https://chloevan.github.io/python/tensorflow2.0/ch3_3_4_network_or/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.5 - 세번째 신경망 네트워크 - XOR](https://chloevan.github.io/python/tensorflow2.0/ch3_3_5_network_xor/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.1 - 선형회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_1_linear_regression/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.2 - 다항회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_2_multiple_linear_regression/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.3 - 딥러닝 네트워크를 이용한 회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_3_regression_with_deeplearning/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.4 - 보스턴 주택 가격 데이터세트](https://chloevan.github.io/python/tensorflow2.0/ch4_4_boston_housing_deeplearning/)\n",
        "- [Tensorflow 2.0 Tutorial ch5.1 - 분류](https://chloevan.github.io/python/tensorflow2.0/ch5_1_binary_classification/)\n",
        "- [Tensorflow 2.0 Tutorial ch5.2 - 다항분류](https://chloevan.github.io/python/tensorflow2.0/ch5_2_multi_classification/)\n",
        "- [Tensorflow 2.0 Tutorial ch5.3 - Fashion MNIST](https://chloevan.github.io/python/tensorflow2.0/ch5_3_fashion_mnist/)\n",
        "- [Tensorflow 2.0 Tutorial ch6.1-2 - CNN 이론](https://chloevan.github.io/python/tensorflow2.0/ch6_1_2_cnn_theory/)\n",
        "- [Tensorflow 2.0 Tutorial ch6.3 - Fashion MNIST with CNN 실습](https://chloevan.github.io/python/tensorflow2.0/ch6_3_fashion_mnist_with_cnn/)\n",
        "- [Tensorflow 2.0 Tutorial ch6.4 - 모형의 성능 높이기](https://chloevan.github.io/python/tensorflow2.0/ch6_4_improve_performance/)\n",
        "- [Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (1)](https://chloevan.github.io/python/tensorflow2.0/ch7_1_2_rnn_theory1/)\n",
        "- [Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (2)](https://chloevan.github.io/python/tensorflow2.0/ch7_1_2_rnn_theory2/)\n",
        "- [Tensorflow 2.0 Tutorial ch7.3 - 긍정, 부정 감성 분석](https://chloevan.github.io/python/tensorflow2.0/ch7_3_sentimentanalysis/)\n",
        "- [Tensorflow 2.0 Tutorial ch7.4 - (1) 단어 단위 생성](https://chloevan.github.io/python/tensorflow2.0/ch7_4_naturallanguagegeneration1/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcD2TKoazzaJ",
        "colab_type": "text"
      },
      "source": [
        "## I. 개요\n",
        "\n",
        "테슬라 `AI Director`인 안드레아 카르파티(`Andrej Karpathy`)는 `The Unreasonable Effectiveness of Recurrent Neural Networks`라는 글을 개인 블로그에 작성했는데, 짧게 요약하면 문자 단위의 순환 신경망이 셰익스피어의 희곡, 소스코드, `Latex`등을 재생산하는데 순환 신경망이 효과적이라는 것을 보여줍니다. \n",
        "\n",
        "이를 바탕으로, 한글 원본 텍스트를 자소 단위와 단어 단위로 나눠서 순환 신경망으로 생성해보도록 합니다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4iORyGw2waj",
        "colab_type": "text"
      },
      "source": [
        "## II. 자소 단위 생성\n",
        "자소 단위 생성을 하기 위해서는 한글을 자소 단위로 분리하고 다시 합칠 수 있는 라이브러리가 필요합니다. 이러한 작업을 할 수 있도록 신해빈 님이 만든 [jamotools](https://github.com/HaebinShin/jamotools)가 있습니다. \n",
        "\n",
        "구글 코랩에서 배시 셀 명령어를 이용해 라이브러리를 설치합니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o0enwIPJQh2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "a35e5c8c-8589-439d-cd43-3a4228b2785e"
      },
      "source": [
        "!pip install jamotools"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jamotools\n",
            "  Downloading https://files.pythonhosted.org/packages/3d/d6/ec13c68f7ea6a8085966390d256d183bf8488f8b9770028359acb86df643/jamotools-0.1.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from jamotools) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jamotools) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from jamotools) (0.16.0)\n",
            "Installing collected packages: jamotools\n",
            "Successfully installed jamotools-0.1.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht3OGWdoJf7m",
        "colab_type": "text"
      },
      "source": [
        "jamotools의 기능을 테스트하기 위해 조선왕조실록 텍스트를 다시 사용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GMR8X5K3C3n",
        "colab_type": "text"
      },
      "source": [
        "### (1) 데이터 로드\n",
        "데이터 파일은 약 62MB 정도입니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1gWw2ee1tMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 텐서플로 2 버전 선택\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import jamotools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEdUWPfI1th4",
        "colab_type": "code",
        "outputId": "32b656ae-fd5a-4dd9-d506-a863c2177819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "path_to_train_file = tf.keras.utils.get_file('input.txt', 'https://raw.githubusercontent.com/greentec/greentec.github.io/master/public/other/data/chosundynasty/corpus.txt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/greentec/greentec.github.io/master/public/other/data/chosundynasty/corpus.txt\n",
            "62013440/62012502 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLH4ycbituC3",
        "colab_type": "text"
      },
      "source": [
        "데이터를 메모리에 불러옵니다. 인코딩 형식으로 `utf-8`을 지정한 뒤 텍스트가 총 몇 자인지 확인해봅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlgmgrnU4JBi",
        "colab_type": "code",
        "outputId": "0711f878-2720-4721-f449-2a76827d9195",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "train_text = open(path_to_train_file, 'rb').read().decode(encoding='utf-8')\n",
        "s = train_text[:100]\n",
        "print(s)\n",
        "\n",
        "# 한글 텍스트를 자모 단위로 분리합니다. 한자 등에는 영향이 없습니다. \n",
        "s_split = jamotools.split_syllables(s)\n",
        "print(s_split)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 \n",
            "태조 강헌 지인 계운 성문 신무 대왕(太祖康獻至仁啓運聖文神武大王)의 성은 이씨(李氏)요, 휘\n",
            "﻿ㅌㅐㅈㅗ ㅇㅣㅅㅓㅇㄱㅖ ㅅㅓㄴㄷㅐㅇㅢ ㄱㅏㄱㅖ. ㅁㅗㄱㅈㅗ ㅇㅣㅇㅏㄴㅅㅏㄱㅏ ㅈㅓㄴㅈㅜㅇㅔㅅㅓ ㅅㅏㅁㅊㅓㄱ·ㅇㅢㅈㅜㄹㅡㄹ ㄱㅓㅊㅕ ㅇㅏㄹㄷㅗㅇㅇㅔ ㅈㅓㅇㅊㅏㄱㅎㅏㄷㅏ \n",
            "ㅌㅐㅈㅗ ㄱㅏㅇㅎㅓㄴ ㅈㅣㅇㅣㄴ ㄱㅖㅇㅜㄴ ㅅㅓㅇㅁㅜㄴ ㅅㅣㄴㅁㅜ ㄷㅐㅇㅘㅇ(太祖康獻至仁啓運聖文神武大王)ㅇㅢ ㅅㅓㅇㅇㅡㄴ ㅇㅣㅆㅣ(李氏)ㅇㅛ, ㅎㅟ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDMiPd1dKHoL",
        "colab_type": "text"
      },
      "source": [
        "`split_syllables()`함수를 이용해서 100글자의 한글이 자모 단위로 정상적으로 분리되는 것을 확인할 수 있습니다. `jamotools`는 영문이나 한자 등에는 영향을 주지 않습니다. 분리한 자모를 다시 합칠 수도 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONZ2TIiUKX3_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f024f0a5-f5b1-4695-8e32-07799d0d68d3"
      },
      "source": [
        "s2 = jamotools.join_jamos(s_split)\n",
        "print(s2)\n",
        "print(s == s2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 \n",
            "태조 강헌 지인 계운 성문 신무 대왕(太祖康獻至仁啓運聖文神武大王)의 성은 이씨(李氏)요, 휘\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-2i93KIKe9z",
        "colab_type": "text"
      },
      "source": [
        "자모를 분리했다가 다시 합친 `s2`는 `s`와 같다는 것을 두 번째 출력이 `True`인 것에서 확인할 수 있습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6qjahajK6I9",
        "colab_type": "text"
      },
      "source": [
        "### (2) 자모 토큰화\n",
        "그럼 이제 자모를 토큰화합니다. 여기서는 따로 텍스트 전처리를 하지 않습니다. 괄호, 한자 등이 토큰에 모두 포함될 것입니다. \n",
        "\n",
        "- 텍스트를 자모 단위로 나눕니다. 데이터가 크기 때문에 약간 시간이 걸립니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk17-5RkLEve",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d23e9869-f6ca-4229-a029-766d2dde1f54"
      },
      "source": [
        "train_text_X = jamotools.split_syllables(train_text)\n",
        "vocab = sorted(set(train_text_X))\n",
        "vocab.append('UNK')\n",
        "\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6198 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey9Vrg7avMWd",
        "colab_type": "text"
      },
      "source": [
        "- 이제, vocal list를 숫자로 매핑하고, 반대도 실행합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAbH1A9VMPLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "test_as_int = np.array([char2idx[c] for c in train_text_X])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_LN2g1_MfCe",
        "colab_type": "text"
      },
      "source": [
        "- 이제 word2idx의 일부를 알아보기 쉽게 출력합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK1w0kplMoS5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "bbaec9fe-d3f6-472f-b92e-e1d734fd6d9b"
      },
      "source": [
        "print('{')\n",
        "for char, _ in zip(char2idx, range(10)):\n",
        "  print(' {:4s}: {:3d}, '.format(repr(char), char2idx[char]))\n",
        "print('   ...\\n}')\n",
        "\n",
        "print('index of UNK: {}'.format(char2idx['UNK']))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            " '\\n':   0, \n",
            " ' ' :   1, \n",
            " '!' :   2, \n",
            " '\"' :   3, \n",
            " \"'\" :   4, \n",
            " '(' :   5, \n",
            " ')' :   6, \n",
            " '+' :   7, \n",
            " ',' :   8, \n",
            " '-' :   9, \n",
            "   ...\n",
            "}\n",
            "index of UNK: 6197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3yfMFZBM-iT",
        "colab_type": "text"
      },
      "source": [
        "자모를 토큰화하고 혹시 사전에 정의되지 않은 기호를 만날수도 있으므로 `UNK`도 사전에 추가합니다. 이렇게 중복되 않은 자모는 총 `6197`개가 나옵니다. 단어에 비해 매우 적은 숫자임을 알 수 있습니다. \n",
        "\n",
        "이제 토큰 데이터를 출력합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdKLJw5hNNa3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3b575ace-931b-4f29-a5e0-abbecf1120a6"
      },
      "source": [
        "print(train_text_X[:20])\n",
        "print(test_as_int[:20])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿ㅌㅐㅈㅗ ㅇㅣㅅㅓㅇㄱㅖ ㅅㅓㄴㄷㅐㅇ\n",
            "[6158   83   87   79   94    1   78  106   76   90   78   56   93    1\n",
            "   76   90   59   62   87   78]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOUh8uwTNjHD",
        "colab_type": "text"
      },
      "source": [
        "`ㅌ`은 6158, `ㅐ`는 83 등으로 토큰화가 된 것을 확인할 수 있습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lZB3aiANtAz",
        "colab_type": "text"
      },
      "source": [
        "### (3) 데이터 생성\n",
        "단어 생성 단위에 있던 학습 데이터세트를 생성합니다. 이 부분의 코드는 큰 변경사항이 없습니다. \n",
        "- `sentence_dataet`에서 `char_dataset`으로 변동하시면 됩니다. \n",
        "- `idx2word`에서 `idx2char`으로 변동하시면 됩니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlViGcpQObh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "30c2a2d5-91a0-4784-9097-250e904fec83"
      },
      "source": [
        "seq_length = 80\n",
        "examples_per_epoch = len(text_as_int) // seq_length\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "char_dataset = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "for item in char_dataset.take(1):\n",
        "    print(idx2char[item.numpy()])\n",
        "    print(item.numpy())\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    return [chunk[:-1], chunk[-1]]\n",
        "\n",
        "train_dataset = char_dataset.map(split_input_target)\n",
        "for x,y in train_dataset.take(1):\n",
        "    print(idx2char[x.numpy()])\n",
        "    print(x.numpy())\n",
        "    print(idx2char[y.numpy()])\n",
        "    print(y.numpy())\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\ufeff' 'ㅌ' 'ㅐ' 'ㅈ' 'ㅗ' ' ' 'ㅇ' 'ㅣ' 'ㅅ' 'ㅓ' 'ㅇ' 'ㄱ' 'ㅖ' ' ' 'ㅅ' 'ㅓ' 'ㄴ'\n",
            " 'ㄷ' 'ㅐ' 'ㅇ' 'ㅢ' ' ' 'ㄱ' 'ㅏ' 'ㄱ' 'ㅖ' '.' ' ' 'ㅁ' 'ㅗ' 'ㄱ' 'ㅈ' 'ㅗ' ' ' 'ㅇ'\n",
            " 'ㅣ' 'ㅇ' 'ㅏ' 'ㄴ' 'ㅅ' 'ㅏ' 'ㄱ' 'ㅏ' ' ' 'ㅈ' 'ㅓ' 'ㄴ' 'ㅈ' 'ㅜ' 'ㅇ' 'ㅔ' 'ㅅ' 'ㅓ'\n",
            " ' ' 'ㅅ' 'ㅏ' 'ㅁ' 'ㅊ' 'ㅓ' 'ㄱ' '·' 'ㅇ' 'ㅢ' 'ㅈ' 'ㅜ' 'ㄹ' 'ㅡ' 'ㄹ' ' ' 'ㄱ' 'ㅓ'\n",
            " 'ㅊ' 'ㅕ' ' ' 'ㅇ' 'ㅏ' 'ㄹ' 'ㄷ' 'ㅗ' 'ㅇ' 'ㅇ']\n",
            "[6158   83   87   79   94    1   78  106   76   90   78   56   93    1\n",
            "   76   90   59   62   87   78  105    1   56   86   56   93   10    1\n",
            "   72   94   56   79   94    1   78  106   78   86   59   76   86   56\n",
            "   86    1   79   90   59   79   99   78   91   76   90    1   76   86\n",
            "   72   81   90   56   36   78  105   79   99   64  104   64    1   56\n",
            "   90   81   92    1   78   86   64   62   94   78   78]\n",
            "['\\ufeff' 'ㅌ' 'ㅐ' 'ㅈ' 'ㅗ' ' ' 'ㅇ' 'ㅣ' 'ㅅ' 'ㅓ' 'ㅇ' 'ㄱ' 'ㅖ' ' ' 'ㅅ' 'ㅓ' 'ㄴ'\n",
            " 'ㄷ' 'ㅐ' 'ㅇ' 'ㅢ' ' ' 'ㄱ' 'ㅏ' 'ㄱ' 'ㅖ' '.' ' ' 'ㅁ' 'ㅗ' 'ㄱ' 'ㅈ' 'ㅗ' ' ' 'ㅇ'\n",
            " 'ㅣ' 'ㅇ' 'ㅏ' 'ㄴ' 'ㅅ' 'ㅏ' 'ㄱ' 'ㅏ' ' ' 'ㅈ' 'ㅓ' 'ㄴ' 'ㅈ' 'ㅜ' 'ㅇ' 'ㅔ' 'ㅅ' 'ㅓ'\n",
            " ' ' 'ㅅ' 'ㅏ' 'ㅁ' 'ㅊ' 'ㅓ' 'ㄱ' '·' 'ㅇ' 'ㅢ' 'ㅈ' 'ㅜ' 'ㄹ' 'ㅡ' 'ㄹ' ' ' 'ㄱ' 'ㅓ'\n",
            " 'ㅊ' 'ㅕ' ' ' 'ㅇ' 'ㅏ' 'ㄹ' 'ㄷ' 'ㅗ' 'ㅇ']\n",
            "[6158   83   87   79   94    1   78  106   76   90   78   56   93    1\n",
            "   76   90   59   62   87   78  105    1   56   86   56   93   10    1\n",
            "   72   94   56   79   94    1   78  106   78   86   59   76   86   56\n",
            "   86    1   79   90   59   79   99   78   91   76   90    1   76   86\n",
            "   72   81   90   56   36   78  105   79   99   64  104   64    1   56\n",
            "   90   81   92    1   78   86   64   62   94   78]\n",
            "ㅇ\n",
            "78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUQmNB2JPZCP",
        "colab_type": "text"
      },
      "source": [
        "- seq_length의 의미는 자소 단위에서는 80개의 자소를 입력받았을 때 1개의 자소를 출력하도록 합니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEmes1c4Pltm",
        "colab_type": "text"
      },
      "source": [
        "### (4) 자소 단위 생성 모델 정의 및 학습\n",
        "자소 단위 생성 모델에서는 겹쳐진 순환 신경망을 사용하지 않고 `LSTM`레이어를 하나만 사용했습니다. 대신 하나의 `LSTM`레이어에서는 사용하는 뉴런의 수를 4배로 늘렸습니다. 또 단어의 수보다 자소의 수가 훨씬 적기 때문에 마지막 `Dense`레이어의 뉴런 수가 적어집니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_AnFyY3P-U-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "ea469176-1bcd-428a-e550-60a5a9933d98"
      },
      "source": [
        "total_chars = len(vocab)\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(total_chars, 100, input_length=seq_length), \n",
        "  tf.keras.layers.LSTM(units=400), \n",
        "  tf.keras.layers.Dense(total_chars, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 80, 100)           619800    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 400)               801600    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6198)              2485398   \n",
            "=================================================================\n",
            "Total params: 3,906,798\n",
            "Trainable params: 3,906,798\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXWT5HZiQlI2",
        "colab_type": "text"
      },
      "source": [
        "모형을 학습시킵니다. 이 때 주의해야 하는 것은 `testmodel` 함수에서, `jamotools` 입력 부분을 추가해야 합니다. 이 소스코드의 위치를 살펴보시기를 바랍니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9pZAvn7Nahc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def testmodel(epoch, logs):\n",
        "    if epoch % 5 != 0 and epoch != 49:\n",
        "        return\n",
        "    test_sentence = train_text[0]\n",
        "    test_sentence = jamotools.split_syllables(test_sentence)\n",
        "\n",
        "    next_chars = 300\n",
        "    for _ in range(next_chars):\n",
        "        test_text_X = test_sentence.split(' ')[-seq_length:]\n",
        "        test_text_X = np.array([char2idx[c] if c in word2idx else char2idx['UNK'] for c in test_text_X])\n",
        "        test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=char2idx['UNK'])\n",
        "\n",
        "        output_idx = model.predict_classes(test_text_X)\n",
        "        test_sentence += ' ' + idx2char[output_idx[0]]\n",
        "    \n",
        "    print()\n",
        "    print(test_sentence)\n",
        "    print()\n",
        "\n",
        "testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avYnp1KjROSI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9917ef82-c7e6-4f2f-d1f6-7d29b49adb55"
      },
      "source": [
        "history = model.fit(train_dataset.repeat(), epochs=100, steps_per_epoch=steps_per_epoch, callbacks=[testmodelcb], verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcbNeokmRSm8",
        "colab_type": "text"
      },
      "source": [
        "- 모형 학습 시, 굉장히 많은 시간이 소요됩니다. 따라서 모형을 확인하려면 시간을 충분히 가지시고, 모형 학습을 진행합니다. \n",
        "- 단어 생성 모델처럼 같은 문장을 자소 단위로 넣어서 에포크가 끝날 때마다 생성 결과를 확인합니다. \n",
        "- 단어 생성 모델과 마찬가지로 처음에는 반복되는 패턴이 자주 나타나지만 점점 그럴듯한 결과를 만들어내기 시작합니다. \n",
        "- 특히 한자와 괄호를 그대로 학습에 사용하고 있기 때문에 한글과 한자의 병기도 잘 하는 것을 볼 수 있습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-c_R_NxR78C",
        "colab_type": "text"
      },
      "source": [
        "### (5) 학습모형 테스트\n",
        "\n",
        "이제 임의의 문장을 통해서 학습을 진행합니다. 마찬가지로 기존 코드와 크게 달라진 것은 없으나 몇몇 변수만 수정하면 됩니다. \n",
        "- next_words에서 next_chars로 변경\n",
        "- word2idx에서 char2idx로 변경\n",
        "- idx2word에서 idx2char로 변경\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahvQ7-BaSI-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "test_sentence = '동헌에 나가 공무를 본 후 활 십오 순을 쏘았다'\n",
        "test_sentence = jamotools.split_syllables(test_sentence)\n",
        "\n",
        "next_chars = 300\n",
        "for _ in range(next_chars):\n",
        "    test_text_X = test_sentence[-seq_length:]\n",
        "    test_text_X = np.array([char2idx[c] if c in char2idx else char2idx['UNK'] for c in test_text_X])\n",
        "    test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=char2idx['UNK'])\n",
        "    \n",
        "    output_idx = model.predict_classes(test_text_X)\n",
        "    test_sentence += idx2char[output_idx[0]]\n",
        "    \n",
        "\n",
        "print(jamotools.join_jamos(test_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mj8pO1KSt_7",
        "colab_type": "text"
      },
      "source": [
        "여기서도 뒤로 가면 비슷한 문장이 다시 나오고 있지만, 한글을 정확하게 조합할 수 있도록 네트워크가 자소를 생성하고 있음을 보여주고 있습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHTDFE18Uh3a",
        "colab_type": "text"
      },
      "source": [
        "## III. 정리\n",
        "여기서 정리해야 하는 것은 순환신경망이 언제 쓰이는 것인지, LSTM, SimpleRNN, GRU레이어에 대해 학습하였습니다. \n",
        "\n",
        "기본적인 이론을 중심으로 재 학습을 하는 걸 권해드립니다. 특히, 자연어처리는 아직도 연구중인 분야고, 사실 굉장히 까다롭기 때문에, 학습자가 특별한 Mission이 있지 않으면 감정 분석 정도에서 마무리하는 것이 좋습니다. 자연어처리를 통한 비즈니스 활용 연구는 실제로 대기업에서 본격적인 연구가 가능합니다. \n",
        "\n",
        "자연어처리는 학습을 시켜서 아시겠지만, 학습시간이 매우 오래 걸리는 대신, 결과물이 사실 애매모호한 경우가 많습니다. 기본적인 이론을 바탕으로 실무에서는 클라우드를 활용한 서비스 개발(예: 챗봇)에 집중하시는 것을 권유드립니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIplmAzGoXTf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## IV. 연습 파일\n",
        "- [구글 Colab에서 직접 연습해보자](https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch7_4_naturalLanguageGeneration(2).ipynb) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QXGf2VuqEeL",
        "colab_type": "text"
      },
      "source": [
        "## V. Reference\n",
        "\n",
        "김환희. (2020). 시작하세요! 텐서플로 2.0 프로그래밍: 기초 이론부터 실전 예제까지 한번에 끝내는 머신러닝, 딥러닝 핵심 가이드. 서울: 위키북스.\n",
        "\n",
        "Karpathy, A. (2015). The Unreasonable Effectiveness of Recurrent Neural Networks. Retrieved April 26, 2020, from http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
      ]
    }
  ]
}