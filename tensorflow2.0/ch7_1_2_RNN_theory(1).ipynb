{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch7_1_2_RNN_theory(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIaZmOPiyDkPQQTtYrPK5w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch7_1_2_RNN_theory(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_qDfvmcn_Za",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "title: \"Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (1)\"\n",
        "date: 2020-04-22T15:08:30+09:00\n",
        "tags:\n",
        "  - \"Deep Learning\"\n",
        "  - \"Python\"\n",
        "  - \"Google Colab\"\n",
        "  - \"Tensorflow 2.0\"\n",
        "  - \"Binary Classification\"\n",
        "  - \"Classification\"\n",
        "  - \"순환 신경망\"\n",
        "  - \"Recurrent Neural Network\"\n",
        "  - \"RNN\"\n",
        "  - \"SimpleRNN\"\n",
        "  - \"LSTM\"\n",
        "  - \"GRU\"\n",
        "  - \"텐서플로 2.0\"\n",
        "  - \"텐서플로 2.0 튜토리얼\"\n",
        "  - \"Image Augmentation\"\n",
        "  - \"Tensorflow 2.0 Tutorial\"\n",
        "categories:\n",
        "  - \"Deep Learning\"\n",
        "  - \"딥러닝\"\n",
        "  - \"텐서플로 2.0\"\n",
        "  - \"Python\"\n",
        "  - \"Tensorflow 2.0\"\n",
        "  - \"텐서플로 2.0 튜토리얼\"\n",
        "  - \"Tensorflow 2.0 Tutorial\"\n",
        "menu: \n",
        "  python:\n",
        "    name: Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (1)\n",
        "---\n",
        "\n",
        "## 공지\n",
        "\n",
        "- 본 Tutorial은 교재 `시작하세요 텐서플로 2.0 프로그래밍`의 강사에게 국비교육 강의를 듣는 사람들에게 자료 제공을 목적으로 제작하였습니다. \n",
        "\n",
        "- 강사의 주관적인 판단으로 압축해서 자료를 정리하였기 때문에, 자세하게 공부를 하고 싶은 반드시 교재를 구매하실 것을 권해드립니다. \n",
        "\n",
        "![](/img/tensorflow2.0/book.jpg)<!-- -->\n",
        "\n",
        "\n",
        "- 본 교재 외에 강사가 추가한 내용에 대한 Reference를 확인하셔서, 추가적으로 학습하시는 것을 권유드립니다. \n",
        "\n",
        "\n",
        "## Tutorial\n",
        "\n",
        "이전 강의가 궁금하신 분들은 아래에서 선택하여 추가 학습 하시기를 바랍니다. \n",
        "\n",
        "- [Google Colab Tensorflow 2.0 Installation](https://chloevan.github.io/python/tensorflow2.0/googlecolab/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.1 - 난수 생성 및 시그모이드 함수](https://chloevan.github.io/python/tensorflow2.0/ch3_3_1_random_signoid/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.2 - 난수 생성 및 시그모이드 함수 편향성](https://chloevan.github.io/python/tensorflow2.0/ch3_3_2_random_signoid_bias/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.3 - 첫번째 신경망 네트워크 - AND](https://chloevan.github.io/python/tensorflow2.0/ch3_3_3_network_and/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.4 - 두번째 신경망 네트워크 - OR](https://chloevan.github.io/python/tensorflow2.0/ch3_3_4_network_or/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.5 - 세번째 신경망 네트워크 - XOR](https://chloevan.github.io/python/tensorflow2.0/ch3_3_5_network_xor/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.1 - 선형회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_1_linear_regression/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.2 - 다항회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_2_multiple_linear_regression/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.3 - 딥러닝 네트워크를 이용한 회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_3_regression_with_deeplearning/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.4 - 보스턴 주택 가격 데이터세트](https://chloevan.github.io/python/tensorflow2.0/ch4_4_boston_housing_deeplearning/)\n",
        "- [Tensorflow 2.0 Tutorial ch5.1 - 분류](https://chloevan.github.io/python/tensorflow2.0/ch5_1_binary_classification/)\n",
        "- [Tensorflow 2.0 Tutorial ch5.2 - 다항분류](https://chloevan.github.io/python/tensorflow2.0/ch5_2_multi_classification/)\n",
        "- [Tensorflow 2.0 Tutorial ch5.3 - Fashion MNIST](https://chloevan.github.io/python/tensorflow2.0/ch5_3_fashion_mnist/)\n",
        "- [Tensorflow 2.0 Tutorial ch6.1-2 - CNN 이론](https://chloevan.github.io/python/tensorflow2.0/ch6_1_2_cnn_theory/)\n",
        "- [Tensorflow 2.0 Tutorial ch6.3 - Fashion MNIST with CNN 실습](https://chloevan.github.io/python/tensorflow2.0/ch6_3_fashion_mnist_with_cnn/)\n",
        "- [Tensorflow 2.0 Tutorial ch6.4 - 모형의 성능 높이기](https://chloevan.github.io/python/tensorflow2.0/ch6_4_improve_performance/)\n",
        "\n",
        "\n",
        "## I. 개요\n",
        "\n",
        "순환 신경망(Recurrent Neural Network; RNN)은 지금까지 살펴본 네트워크와는 입력을 받아들이는 방식과 처리하는 방식에 약간 차이가 있습니다. 순환 신경망은 순서가 있는 데이터를 입력으로 받고, 같은 네트워크를 이용해 변화하는 입력에 대한 출력을 얻어냅니다. \n",
        "\n",
        "순서가 있는 데이터는 음악, 자연어, 날씨, 주가 등 시간의 흐름에 따라 변화하고 그 변화가 의미를 갖는 데이터입니다. \n",
        "\n",
        "## II. 순환 신경망의 구조\n",
        "\n",
        "우선 `CNN`과 `RNN`의 딥러닝 구조의 차이점에 대해 이미지[^1]로 확인하면 보다 직관적으로 이해가 될 수 있습니다. \n",
        "\n",
        "CNN의 구조는 본 교재를 계속 따라오셨다면 익숙하다시피, 아래와 같은 구조로 되어 있습니다. \n",
        "\n",
        "![](/img/tensorflow2.0/tutorial_07_01_2/tutorial_01_CNN.png)\n",
        "\n",
        "그러나, RNN의 구조는 아래에서 확인할 수 있는 것처럼, 순환 모양의 화살표가 있다는 것이 차이점입니다. \n",
        "\n",
        "![](/img/tensorflow2.0/tutorial_07_01_2/tutorial_01_RNN.png)\n",
        "\n",
        "순환 신경망의 특징에 대해 간단하게 요약하면 다음과 같습니다. \n",
        "- 입력 X를 받아서, 출력 Y를 반환합니다.\n",
        "- 순환구조를 가지고 있다; 어떤 레이어의 출력을 다시 입력으로 받는 구조를 말합니다. \n",
        "- 순환 신경망은 입력과 출력의 길이에 제한이 없습니다. \n",
        "- 순환 신경망은 이미지에 대한 설명을 생성하는 이미지 설명 생성, 문장의 긍정/부정을 판단하는 감성 분석, 하나의 언어를 다른 언어로 번역하는 기계 번역(Machine Translation) 등 다양한 용도로 활용됩니다. \n",
        "\n",
        "순환 신경망의 이론에 대한 자세한 설명은 교재 (`p. 174-5`)를 참조하시기를 바랍니다. \n",
        "\n",
        "## III. 주요 레이어 정리\n",
        "순환 신경망의 가장 기초적인 레이어는 `SimpleRNN` 레이어이며, 이 레이어에서 출발한 `LSTM` 레이어 또는 `GRU`레이어가 주로 쓰입니다. 그리고, 자연어 처리를 위해서 꼭 알아둬야 하는 임베딩(`Embedding`)레이어도 같이 알아봅니다. \n",
        "\n",
        "### (1) SimpleRNN 레이어\n",
        "`SimpleRNN`레이어는 가장 간단한 형태의 `RNN`레이업니다. 수식에 대한 설명은 교재(`p. 176`)를 참고합니다. 이 때 주로 사용되는 활성화 함수로는 `tanh`가 사용됩니다. `tanh`는 실수 입력을 받아 -1에서 1사이의 출력 값을 반환하는 활성하 함수이며, 이 활성화 함수 자리에 `ReLU`같은 다른 활성화함수를 쓸 수도 있습니다. \n",
        "\n",
        "`SimpleRNN` 레이어는 `tf.keras`에서 한 줄로 간단하게 생성이 가능합니다.\n",
        "\n",
        "```python\n",
        "rnn1 = tf.keras.layers.SimpleRNN(units=1, activation='tanh', return_sequences=True)\n",
        "```\n",
        "\n",
        "- `units`는 `SimpleRNN`의 레이어에 존재하는 뉴런의 수를 의미합니다. `return_sequences`는 출력으로 시퀀스 전체를 출력할지 여부를 나타내는 옵션이며, 여러 개의 `RNN 레이어`를 쌓을 때 쓰입니다. \n",
        "\n",
        "간단한 예제를 통해서 학습을 해봅니다. \n",
        "\n",
        "\n",
        "[^1]: Different between CNN，RNN（Quote） Retrieved from https://medium.com/@Aj.Cheng/different-between-cnn-rnn-quote-7c224795db58\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1gWw2ee1tMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 텐서플로 2 버전 선택\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm4jB5GQzUDT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "965bbdf9-48eb-4289-a8d6-c3c6f6fc0708"
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i in range(6):\n",
        "  # [0, 1, 2, 3], [1, 2, 3, 4]\n",
        "  lst = list(range(i,i+4))\n",
        "\n",
        "  # 위에서 구한 시퀀스의 숫자들을 각각 10으로 나눈 다음 저장합니다. \n",
        "  # SimpleRNN에 각 타임스텝에 하나씩 숫자가 들어가기 때문에 여기서도 하나씩 분리해서 배열에 저장합니다. \n",
        "  X.append(list(map(lambda c:[c/10], lst)))\n",
        "\n",
        "  # 정답에 해당하는 4, 5 등의 정수 역시 앞에서처럼 10으로 나눠서 저장합니다. \n",
        "  Y.append((i+4)/10)\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "for i in range(len(X)): \n",
        "  print(X[i], Y[i])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. ]\n",
            " [0.1]\n",
            " [0.2]\n",
            " [0.3]] 0.4\n",
            "[[0.1]\n",
            " [0.2]\n",
            " [0.3]\n",
            " [0.4]] 0.5\n",
            "[[0.2]\n",
            " [0.3]\n",
            " [0.4]\n",
            " [0.5]] 0.6\n",
            "[[0.3]\n",
            " [0.4]\n",
            " [0.5]\n",
            " [0.6]] 0.7\n",
            "[[0.4]\n",
            " [0.5]\n",
            " [0.6]\n",
            " [0.7]] 0.8\n",
            "[[0.5]\n",
            " [0.6]\n",
            " [0.7]\n",
            " [0.8]] 0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8ajfVcJ4o9i",
        "colab_type": "text"
      },
      "source": [
        "이제 `SimpleRNN` 레이어를 사용한 네트워크를 정의합니다. 모델 구조는 지금까지 계속 봐온 시퀀셜 모델이고, 출력을 위한 `Dense` 레이어가 뒤에 추가되어 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU6FkwUD49JJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "38d120ac-7569-468c-887a-f496a9b851cd"
      },
      "source": [
        "# 7.3 시퀀스 예측 모델 정의\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.SimpleRNN(units=10, return_sequences=False, input_shape=[4,1]),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_1 (SimpleRNN)     (None, 10)                120       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 131\n",
            "Trainable params: 131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wakc-TmH5ZUV",
        "colab_type": "text"
      },
      "source": [
        "여기에서 주목해야 하는 코드는 `input_shape`입니다. 여기에서 `[4,1]`은 각각 `timesteps`, `input_dim`을 나타냅니다. 타입스텝은(timesteps)이란 순환 신경망이 입력에 대해 계산을 반복하는 횟수를 말하고, `input_dim`은 벡터의 크기를 나타냅니다. \n",
        "\n",
        "```\n",
        "[[0. ]\n",
        " [0.1]\n",
        " [0.2]\n",
        " [0.3]] 0.4\n",
        "```\n",
        "\n",
        "두번째의 4는 타임스텝, 세번째의 1은 `input_dim`이 됩니다. 그림을 참조하면 훨씬 이해하기 쉽습니다. (교재, p.180)\n",
        "\n",
        "시퀀스 예측 모델은 4 타임스텝에 걸쳐 입력을 받고, 마지막에 출력값을 다음 레이어로 반환합니다. 우리가 추가한 `Dense`레이어에는 별도의 활성화함수가 없기 때문에 $h_{3}$는 바로 $y_{3}$이 됩니다. 그리고 이 값과 0.4와의 차이가 `mse`, 즉 평균 제곱 오차(`Mean Squared Error`)가 됩니다. \n",
        "\n",
        "이제 훈련을 시킵니다. 이 때, `verbose`값을 0으로 놓으면 훈련 과정에서의 출력이 나오지 않습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpFME3mM7tUf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "37df4603-3832-4b06-b926-359f733a626a"
      },
      "source": [
        "model.fit(X, Y, epochs=100, verbose=0)\n",
        "print(model.predict(X))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.37582147]\n",
            " [0.5110225 ]\n",
            " [0.6267948 ]\n",
            " [0.72202194]\n",
            " [0.7992587 ]\n",
            " [0.86209536]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqGJUwnj8kCW",
        "colab_type": "text"
      },
      "source": [
        "X가 주어졌을 때 학습된 모델이 시퀀스를 어떻게 예측하는지 확인해보면 얼추 비슷하게 예측하고 있음을 확인할 수 있습니다. 그렇다면 학습과정에서 본 적이 없는 테스트 데이터를 넣으면 어떨까요? `X`의 범위가 0.0~0.9 였으니, 양쪽으로 한 칸씩 더 나간 데이터를 입력합니다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc1ADK7n83Ek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cd9b36c4-814c-4722-911b-336147eb4d58"
      },
      "source": [
        "print(model.predict(np.array([[[0.6], [0.7], [0.8], [0.9]]])))\n",
        "print(model.predict(np.array([[[-0.1], [0.0], [0.1], [0.2]]])))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.9137889]]\n",
            "[[0.22816285]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dMW7X6N9JpN",
        "colab_type": "text"
      },
      "source": [
        "1을 예측하기를 원한 데이터의 출력으로는 0.91을 0.3을 예측하기 원한 데이터의 출력으로는 0.22의 값을 반환했습니다. \n",
        "\n",
        "실무에서는 `SimpleRNN`보다는 `LSTM` 레이어와 `GRU`레이어를 사용합니다. \n",
        "\n",
        "### (2) LSTM 레이어\n",
        "\n",
        "`SimpleRNN` 레이어에는 한 가지 치명적인 단점이 존재합니다. 입력 데이터가 길어질수록, 즉 데이터의 타임스텝이 길어질수록 학습 능력이 떨어진다는 점입니다. 이를 장기의존성(Long-Term Dependency)문제라고 하며, 입력 데이터와 출력 사이의 길이가 멀어질수록 연관 관계가 적어집니다. \n",
        "\n",
        "![](/img/tensorflow2.0/tutorial_07_01_2/tutorial_02_LongTermDependency.png)\n",
        "\n",
        "위 그림이 이러한 문제를 적절하게 표현한 것입니다. 입력 데이터가 길어지면 길어질수록 출력값의 연관 관계가 적어지는 것을 볼 수 있습니다. \n",
        "\n",
        "이러한 문제점을 해결하기 위해 `LSTM`이 제안 되었습니다.[^2] 셀로 나타낸 SimpleRNN과 LSTM의 계산 흐름을 보면 조금 이해가 될 것입니다. \n",
        "\n",
        "- 먼저 SimpleRNN의 그림은 아래와 같습니다. \n",
        "\n",
        "![](/img/tensorflow2.0/tutorial_07_01_2/tutorial_02_SimpleRNN.png)\n",
        "\n",
        "여기에서는 타임스텝의 방향으로 $h_{t}$만 전달되고 있음을 확인할 수 있습니다. \n",
        "\n",
        "![](/img/tensorflow2.0/tutorial_07_01_2/tutorial_02_LSTM.png)\n",
        "\n",
        "그런데, 여기에서는 셀 상테인 $c_{t}$가 평생선을 그리며 함께 전달되고 있습니다. 이처럼 타임스텝을 가로지르며 `LSTM` 셀 상태가 보존되기 때문에 장기의존성 문제를 해결할 수 있다는 것이 `LSTM`의 핵심 아이디어입니다. \n",
        "\n",
        "교재 184페이지를 보면 위 셀에 대한 수식이 존재합니다만, 수식에 대한 구체적인 이해가 자료가 필요하다면 크리스토퍼 올라(`Christopher Olah`)의 블로그 글을 참고합니다.[^3]\n",
        "\n",
        "`LSTM`의 학습 능력을 확인하기 위한 예제는 `LSTM`을 처음 제안한 논문에 나온 실험 여섯개 중 다섯 번째인 곱셈 문제(`Multiplication Problem`)입니다. 이 문제는 말 그대로 실수에 대해 곱셈을 하는 문제인데, 고려해야 할 실수의 범위가 100개이고 그 중에서 마킹된 두개의 숫자만 곱해야 한다는 특이한 문제입니다. \n",
        "\n",
        "[^2]: 1997년 셉 호흐라이터(Sepp Hochreiter) 유르겐 슈미트후버(Jurgen Schmidhuber)에 의해 제안됨, (S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997. https://www.bioinf.jku.at/publications/older/2604.pdf \n",
        "\n",
        "[^3]: Olah, Christopher. “Understanding LSTM Networks.” Understanding LSTM Networks -- Colah's Blog, colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFzH05KBbX9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 텐서플로 2 버전 선택\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwswtlnMXDX_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0cf4184f-017e-44bd-fe7f-fb1234faaf2a"
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "for i in range(3000): \n",
        "  # 0 ~ 1 범위의 랜덤한 숫자 100개를 만듭니다. \n",
        "  lst = np.random.rand(100)\n",
        "\n",
        "  # 마킹할 숫자 2개의 인덱스를 뽑습니다. \n",
        "  idx = np.random.choice(100, 2, replace=False)\n",
        "\n",
        "  # 마킹 인덱스가 저장된 원-핫 인코딩 벡터를 만듭니다. \n",
        "  zeros=np.zeros(100)\n",
        "  zeros[idx]=1\n",
        "  \n",
        "  # 마킹 인덱스와 랜덤한 숫자를 합쳐서 X에 저장합니다. \n",
        "  X.append(np.array(list(zip(zeros, lst))))\n",
        "  # 마킹 인덱스가 1인 값만 서로 곱해서 Y에 저장합니다. \n",
        "  Y.append(np.prod(lst[idx]))\n",
        "\n",
        "print(X[0], Y[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.56858055]\n",
            " [0.         0.69588629]\n",
            " [0.         0.67735798]\n",
            " [0.         0.27871065]\n",
            " [0.         0.35586034]\n",
            " [0.         0.12100308]\n",
            " [1.         0.14539513]\n",
            " [0.         0.43342875]\n",
            " [0.         0.49999051]\n",
            " [0.         0.89730127]\n",
            " [0.         0.07487465]\n",
            " [0.         0.49529167]\n",
            " [0.         0.08552878]\n",
            " [0.         0.59300619]\n",
            " [0.         0.24378501]\n",
            " [0.         0.64154043]\n",
            " [0.         0.44297653]\n",
            " [0.         0.51702394]\n",
            " [0.         0.54928648]\n",
            " [0.         0.18993096]\n",
            " [0.         0.31863405]\n",
            " [0.         0.45684867]\n",
            " [0.         0.59404741]\n",
            " [0.         0.92512103]\n",
            " [0.         0.63156737]\n",
            " [0.         0.12422549]\n",
            " [0.         0.08770544]\n",
            " [0.         0.02174353]\n",
            " [0.         0.94593373]\n",
            " [0.         0.04072264]\n",
            " [0.         0.45340749]\n",
            " [0.         0.05259858]\n",
            " [0.         0.27368318]\n",
            " [0.         0.86271116]\n",
            " [0.         0.96327191]\n",
            " [0.         0.98042503]\n",
            " [0.         0.5772363 ]\n",
            " [1.         0.21461413]\n",
            " [0.         0.95933064]\n",
            " [0.         0.22111469]\n",
            " [0.         0.24241146]\n",
            " [0.         0.75950882]\n",
            " [0.         0.51717453]\n",
            " [0.         0.83854396]\n",
            " [0.         0.17229124]\n",
            " [0.         0.38445763]\n",
            " [0.         0.98125712]\n",
            " [0.         0.51566873]\n",
            " [0.         0.79224611]\n",
            " [0.         0.59290756]\n",
            " [0.         0.90996967]\n",
            " [0.         0.83921291]\n",
            " [0.         0.19683029]\n",
            " [0.         0.11577404]\n",
            " [0.         0.87179873]\n",
            " [0.         0.74935635]\n",
            " [0.         0.86429408]\n",
            " [0.         0.11419532]\n",
            " [0.         0.37452045]\n",
            " [0.         0.33285476]\n",
            " [0.         0.88090182]\n",
            " [0.         0.23588378]\n",
            " [0.         0.16764104]\n",
            " [0.         0.38098711]\n",
            " [0.         0.42247432]\n",
            " [0.         0.8942098 ]\n",
            " [0.         0.13487394]\n",
            " [0.         0.17895608]\n",
            " [0.         0.94037255]\n",
            " [0.         0.70379263]\n",
            " [0.         0.3280325 ]\n",
            " [0.         0.2025446 ]\n",
            " [0.         0.63576295]\n",
            " [0.         0.0113754 ]\n",
            " [0.         0.20068108]\n",
            " [0.         0.50822385]\n",
            " [0.         0.89660019]\n",
            " [0.         0.21449341]\n",
            " [0.         0.24495643]\n",
            " [0.         0.31119842]\n",
            " [0.         0.49868416]\n",
            " [0.         0.99545065]\n",
            " [0.         0.79226827]\n",
            " [0.         0.09709113]\n",
            " [0.         0.87462116]\n",
            " [0.         0.54006067]\n",
            " [0.         0.47876731]\n",
            " [0.         0.83982891]\n",
            " [0.         0.72340332]\n",
            " [0.         0.46549331]\n",
            " [0.         0.71396963]\n",
            " [0.         0.60434442]\n",
            " [0.         0.00359206]\n",
            " [0.         0.60084677]\n",
            " [0.         0.98805388]\n",
            " [0.         0.52347822]\n",
            " [0.         0.15958562]\n",
            " [0.         0.3700633 ]\n",
            " [0.         0.55055634]\n",
            " [0.         0.20978592]] 0.03120384949137252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdcnvh2aY58c",
        "colab_type": "text"
      },
      "source": [
        "입력된 값이 길지만, 1은 두번만 들어가 있기 때문에, 1이 찍인 원소를 찾습니다. `[1.    0.08361932]`과 `[1.         0.66439549]`이 확인이 됩니다. \n",
        "\n",
        "`0.08361932`와 `0.66439549`를 곱하면 `0.055556298045436`값이 나옵니다. `SimpleRNN` 레이어를 이용한 곱셈 문제 모델을 정의합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4sS-sjlaCWv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "c7f497ce-4e05-43f1-b599-b69f7136a51f"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.SimpleRNN(units=30, return_sequences=True, input_shape=[100,2]), \n",
        "  tf.keras.layers.SimpleRNN(units=30), \n",
        "  tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       (None, 100, 30)           990       \n",
            "_________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)     (None, 30)                1830      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 31        \n",
            "=================================================================\n",
            "Total params: 2,851\n",
            "Trainable params: 2,851\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RnA6vqeaY7O",
        "colab_type": "text"
      },
      "source": [
        "`RNN` 레이어를 겹치기 위해 첫 번째 `SimpleRNN`레이어에서 `return_sequences=True`로 설정된 것을 확인할 수 있습니다. `return_sequences`는 레이어의 출력을 다음 레이어로 그대로 넘겨주게 됩니다. \n",
        "\n",
        "겹치는 레이어의 구조에 대한 이론 설명은 교재 `188페이지`를 참조하시기를 바랍니다. `RNN`은 `CNN`보다 학습 시간이 오래 걸리는 편이기 때문에 반드시 가속기를 `GPU`로 바꿔줍니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mZ_ynLObBMt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d2297c4-a43d-470e-e853-2e584e1fd0d3"
      },
      "source": [
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "# 2560개의 데이터만 학습시킵니다. 검증 데이터는 20%로 저장합니다. \n",
        "history=model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "64/64 [==============================] - 8s 118ms/step - loss: 0.0559 - val_loss: 0.0494\n",
            "Epoch 2/100\n",
            "64/64 [==============================] - 8s 119ms/step - loss: 0.0493 - val_loss: 0.0480\n",
            "Epoch 3/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0480 - val_loss: 0.0479\n",
            "Epoch 4/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0487 - val_loss: 0.0471\n",
            "Epoch 5/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0478 - val_loss: 0.0539\n",
            "Epoch 6/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0477 - val_loss: 0.0518\n",
            "Epoch 7/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0481 - val_loss: 0.0473\n",
            "Epoch 8/100\n",
            "64/64 [==============================] - 7s 112ms/step - loss: 0.0472 - val_loss: 0.0490\n",
            "Epoch 9/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0468 - val_loss: 0.0486\n",
            "Epoch 10/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0464 - val_loss: 0.0507\n",
            "Epoch 11/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0482 - val_loss: 0.0481\n",
            "Epoch 12/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0481 - val_loss: 0.0488\n",
            "Epoch 13/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0475 - val_loss: 0.0491\n",
            "Epoch 14/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0468 - val_loss: 0.0473\n",
            "Epoch 15/100\n",
            "64/64 [==============================] - 7s 112ms/step - loss: 0.0464 - val_loss: 0.0496\n",
            "Epoch 16/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0474 - val_loss: 0.0493\n",
            "Epoch 17/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0458 - val_loss: 0.0495\n",
            "Epoch 18/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0458 - val_loss: 0.0495\n",
            "Epoch 19/100\n",
            "64/64 [==============================] - 7s 111ms/step - loss: 0.0452 - val_loss: 0.0487\n",
            "Epoch 20/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0456 - val_loss: 0.0505\n",
            "Epoch 21/100\n",
            "64/64 [==============================] - 7s 112ms/step - loss: 0.0449 - val_loss: 0.0490\n",
            "Epoch 22/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0447 - val_loss: 0.0475\n",
            "Epoch 23/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0444 - val_loss: 0.0494\n",
            "Epoch 24/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0456 - val_loss: 0.0492\n",
            "Epoch 25/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0446 - val_loss: 0.0501\n",
            "Epoch 26/100\n",
            "64/64 [==============================] - 7s 112ms/step - loss: 0.0450 - val_loss: 0.0508\n",
            "Epoch 27/100\n",
            "64/64 [==============================] - 7s 112ms/step - loss: 0.0442 - val_loss: 0.0519\n",
            "Epoch 28/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0433 - val_loss: 0.0489\n",
            "Epoch 29/100\n",
            "64/64 [==============================] - 7s 115ms/step - loss: 0.0436 - val_loss: 0.0497\n",
            "Epoch 30/100\n",
            "64/64 [==============================] - 8s 118ms/step - loss: 0.0441 - val_loss: 0.0519\n",
            "Epoch 31/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0442 - val_loss: 0.0501\n",
            "Epoch 32/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0422 - val_loss: 0.0488\n",
            "Epoch 33/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0427 - val_loss: 0.0564\n",
            "Epoch 34/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0429 - val_loss: 0.0511\n",
            "Epoch 35/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0417 - val_loss: 0.0525\n",
            "Epoch 36/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0411 - val_loss: 0.0520\n",
            "Epoch 37/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0429 - val_loss: 0.0525\n",
            "Epoch 38/100\n",
            "64/64 [==============================] - 7s 112ms/step - loss: 0.0412 - val_loss: 0.0502\n",
            "Epoch 39/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0410 - val_loss: 0.0556\n",
            "Epoch 40/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0407 - val_loss: 0.0520\n",
            "Epoch 41/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0404 - val_loss: 0.0493\n",
            "Epoch 42/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0388 - val_loss: 0.0541\n",
            "Epoch 43/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0391 - val_loss: 0.0563\n",
            "Epoch 44/100\n",
            "64/64 [==============================] - 7s 116ms/step - loss: 0.0392 - val_loss: 0.0506\n",
            "Epoch 45/100\n",
            "64/64 [==============================] - 7s 111ms/step - loss: 0.0400 - val_loss: 0.0556\n",
            "Epoch 46/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0390 - val_loss: 0.0554\n",
            "Epoch 47/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0385 - val_loss: 0.0515\n",
            "Epoch 48/100\n",
            "64/64 [==============================] - 7s 115ms/step - loss: 0.0373 - val_loss: 0.0522\n",
            "Epoch 49/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0373 - val_loss: 0.0556\n",
            "Epoch 50/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0379 - val_loss: 0.0587\n",
            "Epoch 51/100\n",
            "64/64 [==============================] - 7s 112ms/step - loss: 0.0371 - val_loss: 0.0550\n",
            "Epoch 52/100\n",
            "64/64 [==============================] - 7s 115ms/step - loss: 0.0362 - val_loss: 0.0537\n",
            "Epoch 53/100\n",
            "64/64 [==============================] - 7s 112ms/step - loss: 0.0364 - val_loss: 0.0591\n",
            "Epoch 54/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0356 - val_loss: 0.0537\n",
            "Epoch 55/100\n",
            "64/64 [==============================] - 7s 112ms/step - loss: 0.0348 - val_loss: 0.0591\n",
            "Epoch 56/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0351 - val_loss: 0.0572\n",
            "Epoch 57/100\n",
            "64/64 [==============================] - 7s 115ms/step - loss: 0.0339 - val_loss: 0.0585\n",
            "Epoch 58/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0336 - val_loss: 0.0593\n",
            "Epoch 59/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0337 - val_loss: 0.0587\n",
            "Epoch 60/100\n",
            "64/64 [==============================] - 7s 115ms/step - loss: 0.0339 - val_loss: 0.0574\n",
            "Epoch 61/100\n",
            "64/64 [==============================] - 7s 112ms/step - loss: 0.0323 - val_loss: 0.0582\n",
            "Epoch 62/100\n",
            "64/64 [==============================] - 7s 115ms/step - loss: 0.0328 - val_loss: 0.0586\n",
            "Epoch 63/100\n",
            "64/64 [==============================] - 7s 112ms/step - loss: 0.0329 - val_loss: 0.0572\n",
            "Epoch 64/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0318 - val_loss: 0.0610\n",
            "Epoch 65/100\n",
            "64/64 [==============================] - 7s 116ms/step - loss: 0.0315 - val_loss: 0.0537\n",
            "Epoch 66/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0309 - val_loss: 0.0599\n",
            "Epoch 67/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0305 - val_loss: 0.0585\n",
            "Epoch 68/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0303 - val_loss: 0.0599\n",
            "Epoch 69/100\n",
            "64/64 [==============================] - 7s 115ms/step - loss: 0.0297 - val_loss: 0.0633\n",
            "Epoch 70/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0296 - val_loss: 0.0600\n",
            "Epoch 71/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0289 - val_loss: 0.0612\n",
            "Epoch 72/100\n",
            "64/64 [==============================] - 8s 120ms/step - loss: 0.0284 - val_loss: 0.0619\n",
            "Epoch 73/100\n",
            "64/64 [==============================] - 7s 115ms/step - loss: 0.0287 - val_loss: 0.0637\n",
            "Epoch 74/100\n",
            "64/64 [==============================] - 7s 115ms/step - loss: 0.0291 - val_loss: 0.0568\n",
            "Epoch 75/100\n",
            "64/64 [==============================] - 7s 111ms/step - loss: 0.0287 - val_loss: 0.0617\n",
            "Epoch 76/100\n",
            "64/64 [==============================] - 7s 115ms/step - loss: 0.0287 - val_loss: 0.0605\n",
            "Epoch 77/100\n",
            "64/64 [==============================] - 7s 115ms/step - loss: 0.0272 - val_loss: 0.0614\n",
            "Epoch 78/100\n",
            "64/64 [==============================] - 7s 116ms/step - loss: 0.0269 - val_loss: 0.0606\n",
            "Epoch 79/100\n",
            "64/64 [==============================] - 7s 116ms/step - loss: 0.0264 - val_loss: 0.0630\n",
            "Epoch 80/100\n",
            "64/64 [==============================] - 8s 117ms/step - loss: 0.0258 - val_loss: 0.0701\n",
            "Epoch 81/100\n",
            "64/64 [==============================] - 7s 117ms/step - loss: 0.0263 - val_loss: 0.0633\n",
            "Epoch 82/100\n",
            "64/64 [==============================] - 7s 115ms/step - loss: 0.0267 - val_loss: 0.0635\n",
            "Epoch 83/100\n",
            "64/64 [==============================] - 7s 117ms/step - loss: 0.0261 - val_loss: 0.0666\n",
            "Epoch 84/100\n",
            "64/64 [==============================] - 8s 119ms/step - loss: 0.0254 - val_loss: 0.0624\n",
            "Epoch 85/100\n",
            "64/64 [==============================] - 8s 119ms/step - loss: 0.0253 - val_loss: 0.0601\n",
            "Epoch 86/100\n",
            "64/64 [==============================] - 7s 115ms/step - loss: 0.0263 - val_loss: 0.0647\n",
            "Epoch 87/100\n",
            "64/64 [==============================] - 7s 116ms/step - loss: 0.0238 - val_loss: 0.0676\n",
            "Epoch 88/100\n",
            "64/64 [==============================] - 7s 115ms/step - loss: 0.0239 - val_loss: 0.0661\n",
            "Epoch 89/100\n",
            "64/64 [==============================] - 8s 118ms/step - loss: 0.0237 - val_loss: 0.0641\n",
            "Epoch 90/100\n",
            "64/64 [==============================] - 7s 116ms/step - loss: 0.0239 - val_loss: 0.0680\n",
            "Epoch 91/100\n",
            "64/64 [==============================] - 7s 116ms/step - loss: 0.0222 - val_loss: 0.0672\n",
            "Epoch 92/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0230 - val_loss: 0.0659\n",
            "Epoch 93/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0222 - val_loss: 0.0668\n",
            "Epoch 94/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0235 - val_loss: 0.0634\n",
            "Epoch 95/100\n",
            "64/64 [==============================] - 7s 114ms/step - loss: 0.0231 - val_loss: 0.0660\n",
            "Epoch 96/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0224 - val_loss: 0.0654\n",
            "Epoch 97/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0211 - val_loss: 0.0657\n",
            "Epoch 98/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0217 - val_loss: 0.0689\n",
            "Epoch 99/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0212 - val_loss: 0.0667\n",
            "Epoch 100/100\n",
            "64/64 [==============================] - 7s 113ms/step - loss: 0.0216 - val_loss: 0.0664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xEw56Lvbof2",
        "colab_type": "text"
      },
      "source": [
        "훈련 데이터의 손실(`loss`)과 검증 데이터의 손실(`var_loss`)는 감소하지 않고 오히려 증가하는 것 같습니다. 경향을 직관적으로 파악하기 위해 `history` 변수에 저장된 값으로 그래프를 그려봅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZapF9f-yb-KJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "c149cbc1-1ef3-4cb6-b3b1-fc809cc69739"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'], 'b-', label='loss')\n",
        "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLoCsg7hcHAf",
        "colab_type": "text"
      },
      "source": [
        "학습 결과는 전형적인 과적합 그래프를 보여줍니다. 테스트 데이터에 대한 예측은 어떨까요? 논문에서는 오차가 0.04 이상일 때 오답으로 처리합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQh_p6-Mdvgy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "959f44ee-d5b6-4507-8386-5b30de25d004"
      },
      "source": [
        "model.evaluate(X[2560:], Y[2560:])\n",
        "prediction=model.predict(X[2560:2560+5])\n",
        "\n",
        "# 5개 테스트 데이터에 대한 예측을 표시합니다. \n",
        "for i in range(5): \n",
        "  print(Y[2560+i], '\\t', prediction[i][0], '\\tdiff:', abs(prediction[i][0] - Y[2560+i]))\n",
        "\n",
        "prediction = model.predict(X[2560:])\n",
        "fail = 0\n",
        "for i in range(len(prediction)):\n",
        "  # 오차가 0.04 이상이면 오답입니다. \n",
        "  if abs(prediction[i][0] - Y[2560+i]) > 0.04:\n",
        "    fail +=1\n",
        "\n",
        "print('correctness:', (440-fail)/440*100, '%')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 0s 16ms/step - loss: 0.0667\n",
            "0.009316712705671063 \t -0.05508966 \tdiff: 0.06440637269455123\n",
            "0.1595728709352136 \t 0.051416673 \tdiff: 0.10815619816900496\n",
            "0.343633615267837 \t 0.27744463 \tdiff: 0.06618898440655463\n",
            "0.047836290850227836 \t 0.23675384 \tdiff: 0.1889175454239192\n",
            "0.07471709800989841 \t 0.19511518 \tdiff: 0.12039808081357266\n",
            "correctness: 12.727272727272727 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q27ljmgLe9Yx",
        "colab_type": "text"
      },
      "source": [
        "먼저 전체에 대한 평가는 `0.0667`의 `loss`가 나왔습니다. 위에서 본 100번째의 에포크의 `val_loss`인 `0.0664`보다도 높은 값으로, 네트워크가 학습 과정에서 한번도 못 본 테스트 데이터에 대해서는 잘 예측하지 못합니다. 5개의 테스트 데이터에 대한 샘플은 오차가 `0.01`에서 `0.18`까지 다양하게 나타나며, 가장 중요한 정확도는 `12.72`로 확인 됩니다. \n",
        "\n",
        "그렇다면 `LSTM`레이어는 어떨까요? 이 문제를 풀기 위해 시퀀셜 모델을 정의합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOgxXjCLh48d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "cf53942b-c5b2-44c4-e5fe-f2a74b312f4f"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.LSTM(units=30, return_sequences=True, input_shape=[100,2]), \n",
        "  tf.keras.layers.LSTM(units=30), \n",
        "  tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 100, 30)           3960      \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 30)                7320      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 31        \n",
            "=================================================================\n",
            "Total params: 11,311\n",
            "Trainable params: 11,311\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFfHHPbGi8mW",
        "colab_type": "text"
      },
      "source": [
        "차이점은 `SimpleRNN`을 `LSTM`으로 바꾼 것 뿐입니다. 네트워크의 학습코드도 동일합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0ocxwtgjFXF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c152178-6816-46e8-a9c2-b2f5b1cb63ed"
      },
      "source": [
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "# 2560개의 데이터만 학습시킵니다. 검증 데이터는 20%로 저장합니다. \n",
        "history=model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'], 'b-', label='loss')\n",
        "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0507 - val_loss: 0.0470\n",
            "Epoch 2/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0472 - val_loss: 0.0469\n",
            "Epoch 3/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0473 - val_loss: 0.0482\n",
            "Epoch 4/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0474 - val_loss: 0.0482\n",
            "Epoch 5/100\n",
            "64/64 [==============================] - 3s 44ms/step - loss: 0.0474 - val_loss: 0.0471\n",
            "Epoch 6/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0474 - val_loss: 0.0476\n",
            "Epoch 7/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0471 - val_loss: 0.0474\n",
            "Epoch 8/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0477 - val_loss: 0.0474\n",
            "Epoch 9/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0472 - val_loss: 0.0472\n",
            "Epoch 10/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0469 - val_loss: 0.0471\n",
            "Epoch 11/100\n",
            "64/64 [==============================] - 3s 44ms/step - loss: 0.0471 - val_loss: 0.0484\n",
            "Epoch 12/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0473 - val_loss: 0.0472\n",
            "Epoch 13/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0467 - val_loss: 0.0483\n",
            "Epoch 14/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0466 - val_loss: 0.0471\n",
            "Epoch 15/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0470 - val_loss: 0.0470\n",
            "Epoch 16/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0466 - val_loss: 0.0471\n",
            "Epoch 17/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0464 - val_loss: 0.0470\n",
            "Epoch 18/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0464 - val_loss: 0.0483\n",
            "Epoch 19/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 0.0465 - val_loss: 0.0467\n",
            "Epoch 20/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 0.0462 - val_loss: 0.0465\n",
            "Epoch 21/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0481 - val_loss: 0.0468\n",
            "Epoch 22/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0467 - val_loss: 0.0474\n",
            "Epoch 23/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 0.0465 - val_loss: 0.0468\n",
            "Epoch 24/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0463 - val_loss: 0.0471\n",
            "Epoch 25/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0461 - val_loss: 0.0478\n",
            "Epoch 26/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 0.0457 - val_loss: 0.0464\n",
            "Epoch 27/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0456 - val_loss: 0.0462\n",
            "Epoch 28/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0459 - val_loss: 0.0485\n",
            "Epoch 29/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0454 - val_loss: 0.0460\n",
            "Epoch 30/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0474 - val_loss: 0.0464\n",
            "Epoch 31/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0457 - val_loss: 0.0475\n",
            "Epoch 32/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 0.0455 - val_loss: 0.0461\n",
            "Epoch 33/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 0.0451 - val_loss: 0.0455\n",
            "Epoch 34/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0448 - val_loss: 0.0450\n",
            "Epoch 35/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0438 - val_loss: 0.0524\n",
            "Epoch 36/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0453 - val_loss: 0.0448\n",
            "Epoch 37/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0427 - val_loss: 0.0408\n",
            "Epoch 38/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0481 - val_loss: 0.0462\n",
            "Epoch 39/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0457 - val_loss: 0.0455\n",
            "Epoch 40/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0447 - val_loss: 0.0443\n",
            "Epoch 41/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 0.0419 - val_loss: 0.0390\n",
            "Epoch 42/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0353 - val_loss: 0.0254\n",
            "Epoch 43/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 0.0223 - val_loss: 0.0199\n",
            "Epoch 44/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 0.0188 - val_loss: 0.0155\n",
            "Epoch 45/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0117 - val_loss: 0.0099\n",
            "Epoch 46/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0092 - val_loss: 0.0070\n",
            "Epoch 47/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0060 - val_loss: 0.0046\n",
            "Epoch 48/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0062 - val_loss: 0.0043\n",
            "Epoch 49/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0131 - val_loss: 0.0063\n",
            "Epoch 50/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0050 - val_loss: 0.0041\n",
            "Epoch 51/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0035 - val_loss: 0.0033\n",
            "Epoch 52/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0032 - val_loss: 0.0034\n",
            "Epoch 53/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0029 - val_loss: 0.0029\n",
            "Epoch 54/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0033 - val_loss: 0.0024\n",
            "Epoch 55/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 0.0034 - val_loss: 0.0025\n",
            "Epoch 56/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0026 - val_loss: 0.0031\n",
            "Epoch 57/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 58/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 59/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0017 - val_loss: 0.0023\n",
            "Epoch 60/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 0.0018 - val_loss: 0.0032\n",
            "Epoch 61/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 62/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0018 - val_loss: 0.0040\n",
            "Epoch 63/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 64/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 65/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 66/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 67/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 0.0016 - val_loss: 0.0021\n",
            "Epoch 68/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 69/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 70/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 8.9019e-04 - val_loss: 0.0012\n",
            "Epoch 71/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 9.5466e-04 - val_loss: 9.1598e-04\n",
            "Epoch 72/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 73/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 9.1245e-04 - val_loss: 9.0190e-04\n",
            "Epoch 74/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 7.8530e-04 - val_loss: 8.0207e-04\n",
            "Epoch 75/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 9.1405e-04 - val_loss: 8.6976e-04\n",
            "Epoch 76/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 8.3703e-04 - val_loss: 8.9048e-04\n",
            "Epoch 77/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 7.5276e-04 - val_loss: 8.5232e-04\n",
            "Epoch 78/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 7.6209e-04 - val_loss: 8.4933e-04\n",
            "Epoch 79/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 6.7965e-04 - val_loss: 6.3633e-04\n",
            "Epoch 80/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 7.7308e-04 - val_loss: 6.0886e-04\n",
            "Epoch 81/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 0.0020 - val_loss: 7.1028e-04\n",
            "Epoch 82/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 5.6736e-04 - val_loss: 6.5136e-04\n",
            "Epoch 83/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 0.0012 - val_loss: 0.0010\n",
            "Epoch 84/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 5.5983e-04 - val_loss: 6.3431e-04\n",
            "Epoch 85/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 5.7588e-04 - val_loss: 6.2150e-04\n",
            "Epoch 86/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 5.1412e-04 - val_loss: 4.9473e-04\n",
            "Epoch 87/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 4.5491e-04 - val_loss: 6.0971e-04\n",
            "Epoch 88/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 5.9710e-04 - val_loss: 5.0932e-04\n",
            "Epoch 89/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 3.7854e-04 - val_loss: 7.4333e-04\n",
            "Epoch 90/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 5.4616e-04 - val_loss: 5.6270e-04\n",
            "Epoch 91/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 4.5601e-04 - val_loss: 5.5043e-04\n",
            "Epoch 92/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 4.3344e-04 - val_loss: 4.6876e-04\n",
            "Epoch 93/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 5.3680e-04 - val_loss: 4.5829e-04\n",
            "Epoch 94/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 7.5234e-04 - val_loss: 6.1782e-04\n",
            "Epoch 95/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 3.7421e-04 - val_loss: 3.6062e-04\n",
            "Epoch 96/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 3.9064e-04 - val_loss: 5.2667e-04\n",
            "Epoch 97/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 4.5003e-04 - val_loss: 4.0708e-04\n",
            "Epoch 98/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 5.4694e-04 - val_loss: 0.0011\n",
            "Epoch 99/100\n",
            "64/64 [==============================] - 3s 42ms/step - loss: 5.4764e-04 - val_loss: 7.6580e-04\n",
            "Epoch 100/100\n",
            "64/64 [==============================] - 3s 43ms/step - loss: 5.1314e-04 - val_loss: 4.5973e-04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXxU1d3H8c8vCwlrwhLWsClBhEBYAoggblXBagF3bVVQilVQWxWXWnefWrUW91or8iBPVRCRgiBaAYsLIotBNoGwJCSsCRDWEJL8nj/OTRhCQgZImMzk93695pWZe8/cOXcGvnPm3HPPFVXFGGNM6AoLdAWMMcZULgt6Y4wJcRb0xhgT4izojTEmxFnQG2NMiIsIdAVKatSokbZp0ybQ1TDGmKCyePHiLFWNK21dlQv6Nm3asGjRokBXwxhjgoqIpJW1zrpujDEmxFnQG2NMiLOgN8aYEFfl+uiNMdXT4cOHycjIIDc3N9BVqdKio6OJj48nMjLS7+dY0BtjqoSMjAzq1q1LmzZtEJFAV6dKUlWys7PJyMigbdu2fj/Pum6MMVVCbm4uDRs2tJA/DhGhYcOGJ/yrx4LeGFNlWMiX72TeIwt6c/osWwZffx3oWhhT7VgfvTl9unRxf+0aCKaKqlOnDvv27Qt0NSqctejN6WdBb8xpZUFvTr8QbDGZ0KKqjB49msTERDp37szEiRMB2LJlC/3796dr164kJiby9ddfU1BQwNChQ4vLjhkzJsC1P5Z13ZjTZ98+yMuDunUDXRNTxf3+95CSUrHb7NoVXn7Zv7JTpkwhJSWFpUuXkpWVRc+ePenfvz/vv/8+l112GY8++igFBQUcOHCAlJQUMjMzWb58OQC7d++u2IpXAGvRm9Ondm2oXz/QtTCmXN988w033ngj4eHhNGnShPPPP5+FCxfSs2dPxo0bx5NPPsmyZcuoW7cuZ5xxBuvXr+fuu+9m1qxZ1KtXL9DVP0ZIteh37nRZEhUV6JqYYyxf7ppTK1bAH/8IV14Z6BqZKszflvfp1r9/f+bNm8eMGTMYOnQo9913H7fccgtLly7l888/56233mLSpEm8++67ga7qUfxq0YvIABFZLSKpIvJwKeujRGSit36BiLTxlrcRkYMikuLd3qrY6h8xbx40bGij96qsJUtg7Fj4/nt335gq7LzzzmPixIkUFBSwY8cO5s2bR69evUhLS6NJkyb89re/Zfjw4SxZsoSsrCwKCwu5+uqrefbZZ1lSBf99l9uiF5Fw4A3gEiADWCgi01R1pU+x24FdqtpORG4Angeu99atU9WuFVzvY3TtCmFh8M038ItfVParmROWkeH+1qt35L4xVdSQIUOYP38+SUlJiAgvvPACTZs2Zfz48bz44otERkZSp04d3nvvPTIzMxk2bBiFhYUAPPfccwGu/bH86brpBaSq6noAEfkQGAT4Bv0g4Env/mTgdTnNp7jVq+eGaX/77el8VeO3jAz3k6t1a8jMDHRtjClV0Rh6EeHFF1/kxRdfPGr9rbfeyq233nrM86piK96XP103LYBNPo8zvGWlllHVfCAHaOitaysiP4rIf0XkvNJeQERGiMgiEVm0Y8eOE9oBX/36wfz5kJ9/0pswlSUjA+LjoUULC3pjTrPKHnWzBWilqt2A+4D3ReSYQ9Kq+raqJqtqclxcqZc89Eu/frB/PyxdevIVNpVkzx4X9J06uZa9Mea08afrJhNo6fM43ltWWpkMEYkAYoBsVVXgEICqLhaRdUB7oFIuCtu3r/v7zTfQo0dlvII5aV995X5qRYTUQC9jgoI/LfqFQIKItBWRGsANwLQSZaYBRR1X1wBzVFVFJM47mIuInAEkAOsrpurHio93XcDffFNZr2BOiYW8MQFRbtB7fe6jgM+BVcAkVV0hIk+LyK+8YmOBhiKSiuuiKRqC2R/4SURScAdpf6eqOyt6J3z16+eC3qZTqUIyM+H662HhQvjpJzjnHFiwINC1Mqba8KuPXlVnqmp7VT1TVf/HW/a4qk7z7ueq6rWq2k5VexWN0FHVj1W1k6p2VdXuqjq98nbF6dcPtm6F9ZX2u+EEeUOuqrV162DSJMjJgchIF/Lr1gW6VsZUGyE3BUK/fu5vcffNgQNw881wySVw7bVwxx2uVVnRFi6Ejz8+etnEidCkCXz3XcW/nmfNGnfSaYV6+20YOtTNS1MRisbNx8e7m++yEvbssSGyxlS0kAv6jh0hNtYnLNLS3IN9+2DlSnj/fRg4ELZvr5gX/PRT+PJLePRRGDbsSIClpsLw4ZCVBb/5jUuwE/HCC+7b6s47y2z9FhbCr34FF14Iu3adwLYPHTr++n/+ExYvhv/+t2L6wIrekxYt3IRmdeuWOcTyzTfdl3WV+UVmTBnq1KlT5rqNGzeSmJh4GmtzfCEX9GFhbvRNcYv+7LNh9Wo3wH7FCrfi2mshJubIk1Rh5kz48MOjtrV/v8vZq68uI3j27oURI1zIv/mmG1Xyu9+57UVHw8UXu22mpcE99/i/E198AQ895L6Uxo2DMs60+/JLt2tZWfDUU35s98ABuOkmiItzX0SlWbMGFi1yXwaXXuq+HE9VRoZ7v4tmrYyPL7NFv2KF+/vRR6f+ssYYj6pWqVuPHj30VP35z6ph5Ovev7yuB3ce0PR01alTVR98UPX881W7dVPt2lW1V5eDevVZy3RBnQtVQTNjOug336gWFqpmDx6m39W6WD/iav1bxGi9LHK2Pv/MIc3L83mhBx9UBdUFC1RVteCvf1MFXfiHf2l+vk+5F19U/eQT/yp/8KBqu3aqCQnu/t13q0ZEqG7YcEzRK65QbdxYddgw1fBw1XUfLVb94AO3AyWlp6t2764qohoVpTpoUOmv/8QTrsyiRe7v008Xr8rLUx06VHX+fFXdt0/1yivd65Xn979X7dv3yOO77lJ94IFSi/bq5d7S7t3L36wJLStXrjx6wfnnH3t74w23bv/+0tePG+fW79hx7LpyPPTQQ/r6668XP37iiSf0mWee0Ysuuki7deumiYmJOnXq1OL1tWvXLnNbGzZs0E6dOqmq6sGDB3Xo0KGamJioXbt21Tlz5qiq6vLly7Vnz56alJSknTt31jVr1ui+ffv08ssv1y5duminTp30ww8/9O+9UlVgkZaRqwEP9pK3igj6efNUR/KaKujVfKSuia0aGanau7fLp+EXr9fNtc7QfAnXnMgG+nqnN/Ws2psUVM8+W/Uf4b/TBZF9dW/rjloYGakKOoOBesYZqs88o5r9+MtaGBmpu68apv/4h+q116o2jM3XpXR2r3thtmZllajYoUOqo0drwbv/q2vGf6d5M75QHT9edfVqt37fPtU//MFV9vPP3bKMDNUaNVRHjDhqU+vWuRz+059Ut29XjY1Vvf3cle65Awa45/l67TXVunVVp09Xfekl1Yce0qO/jdR9QSQkqF50kXvct69qUlLx6o8/dps/5xzVwtxDWvzGHrOjJ6ewUDUmRrV2bbfZtWsrZLMmSAQ66JcsWaL9+/cvfnz22Wdrenq65uTkeJvcoWeeeaYWeg0pf4P+r3/9qw4bNkxVVVetWqUtW7bUgwcP6qhRo/T//u//VFX10KFDeuDAAZ08ebIOHz68eDu7d+8udfvVO+hfekm1Rg0trFdP8yVcf25zqf7Ps4X697+rfvutayAXy81VveYa17r0gmrvXtV33nH5dvXVqlu3emX37FGdOlXnP/2Fnn++ak32q4LuklhtzFYF1WbNXGt32ivr9YfLn9C6kQe1TRvVxYuPvGThmrWaH1HjSEAW3d580xX47jv3+Lrrjt6vu+5y31JpacWL7r/fteI3f52q+qc/6ctjClUo0KUjXletVcsl/29+o/r8896LF6pu2nT89+/AAfdFM2nSkfcTVFNTVVX1kkvcl4tQoHPnqupPP6mGhamOHOnvJ3Rc27a5l7v/fvf3z3+ukM2aIFFaeJ1uHTp00MzMTE1JSdFzzz1X8/LydOTIkdq5c2dNSkrS6Oho3bJli6r6H/SDBw/W2bNnF6/r16+fLl26VP/1r39px44d9S9/+YuuWbNGVVVXr16trVu31gcffFDnzZtX5vard9DPm+daqvfeq3rffaqZmSe/rePYsL5QX3w4S++8Lkvfess1yEv2lixYoBof74KxeXPXJdGtm2o4h/WSlqt08tDpekPz/+qZrNWbrzmgX36pumdjtuqnn6p6LYgi+RvSdfe1w3X3igzVwkI9+PVC/XWtKbqk6QD3AnXrat7y1Xr22arR0aojL12j2zv008JatVz3UllmzToS6qXu6Ab3T+SVV3TtWnf3pd+t0VXhnfSuc7xvsLvuct84y5eXvo28PPfN6fs6kye7N6XE5zNvnnuNzz5T7dPHda+Z6qMqBP1jjz2mr7zyij7yyCP6yiuv6Lhx4/S6667TPK/PtnXr1rrB60Y91aBXVU1NTdVXXnlF27VrV1wmOztbJ0yYoP3799ennnqq1O1X76CvYrZvV33qKdeHfsklLuz//nct7uc/eFD1ySddlzm4xnHnzq7sFVeoDhni+qqjo7W46+l3F6wq/iWQG9dC9bHHilvq69apjhql2rSpKxIuBdquneuOf/JJ9+VTUOBVrrBQ9YILXMv/gQdUP/xQdeJEnwKepUtVCwr0gQdU+4Z9p3lnd9YDNetrS9J04UJ1P5FjY1VvvbX0NyEtzVXmn/88suzf/3bLfvjhqKLvvOMWr1+vOmaMu1/Uq2VCX1UI+uXLl2ufPn00ISFBN2/erC+//LKOGjVKVVXnzJmjwAkH/UsvvaS33XabqroWe6tWrTQ3N1fXrVtX3A10//3365gxYzQzM1MPel0P06dP10FlHEuzoA9Cu3a5xvUTT6gOHOj6wLt3V+3UyYX+ffe5ELzvPtX2zfbo9XygI9vO0MK8w6VuLz9fde5ct71rrnHHHMLC3KfdpInr7l++XFV//lm1Xz93DKCoG8k7aOx70Dn36x90fkRft75+fd0/eabGxKhedZVX4IcfSvSL+fj22yPN9CKLF7tlU6YcVXT0aNWrI6ZqYVycbp80V0H12WdP5h01wagqBL2qamJiol5wwQWq6vrlzznnHE1MTNShQ4dqhw4dTjjoyzoY+9xzz2nHjh01KSlJL7vsMs3OztZZs2YVdxMlJyfrwoULS92+BX2IKyhwo158uuv9kpWlOmGC6vXXuy58cC39r75Szd6cq4XfL9C89yfppA8L9KKL3Prrr1fdnH5Yf75kpK7lTP35rlfcgQxVffRR12vkewxCMzPditzcI8smTnQbW7bsyLKtW92y1147qo6DBqlOjr3drXv5Ze3bV7VLlxN8g0zQqipBHwws6E25duxwrf369Y805KOjVevVc/dbt1a97TbXpVSvnmqflpv0rDMPH9Wrs337kfI9e7oDpzuffcMtSEo6EuxFB3R37Try5IIC1w/10ENH1atDB9WUuItd/5Wqvvqqais26r//Xbnvh6kaLOj9d6JBH3InTJnyNWoETz4J6ekweTKMGQMjR8J117kTfdetc5d3XbYMevWC+ZviuWNkBGE+/1ri4iAlBf78Z/f4j3+EJk/dxZsDp1OQsRm6d4enn3YnjvXuffQJamFhpF94CzPWdShelJ/vXrd17mp3XUhgxMXrWBHWmY9v/MimxjFV0rJly+jatetRt969ewe6Wscq6xsgUDdr0VcthYWqP/547DHaktavd33/ERGqzSO36/y2N6qCFjzy6DFlc3JU4+Jc18+2bW7Z2rWqEeRpdnxn1RdecAvz8jS3S0/Nlgb6i46ZeuBABe+cqVJWrlxZfHDSlK2wsNBa9KZiiRy58PrxtG0L//gHrF0LV94Wx6VZ73MF0+k09j5ef/3osi+8ADt2QJQeZLo3n+maNZBPJKs+/AlGj3YLIyOJmjSBmBoHuX/lbdx1p6I2/XTIio6OJjs72/Upm1KpKtnZ2URHR5/Q86SqvanJycm6aFGlXIDKnEYHD8KMGW4KoLlz3ZQ9Q4e6KW4SEuCts1/hhh8f5KYBu/j4s1qMGQP33ee+ABo1KrGxN9+EkSO5ize4Zf5dnHNOIPbIVLbDhw+TkZFBbm5uoKtSpUVHRxMfH09kZORRy0Vksaoml/Ycu+SPqRQ1a8I118CgQXD55fDb30KrVjBhgpt1c8CdbYkakUfW7KXs3duHNWvgoZqv0vDaT9xsbeHhRzZ2550cnjKdxDkrmD4dC/oQFRkZSdu2bQNdjZBkXTemUkVGugO+Z50FgwfD+PFw993Q5Jeu4ZF0eCGzZrlZOC+o/QOyfv3RIQ8gQuSMqUw87w1mzAjAThgT5CzoTaWLiXHdOLVru2sFPPoo0Lw52rQpfaMWMXWq66Nvr2vcN0JpoqK44gr4aWkhmzad1uobE/Qs6M1p0bq1uwjX999D/fpumSQnc17NRUyfDpmZSvN9q6F9+zK3MerdbrzNCGbOPE2VNiZEWNCb0yY+vkSODx9O9vUj2btXacx2og/tKbtFD0TH1SUp6mfrvjHmBFnQm8AZNIh2Y0ZSu7ZQm/3s7TcQunUrs7icdRYdZDWzZ7tRPcYY/1jQm4CquXUDQ/uuZaOcQcQXM49c3b00HTpQNzeLqAM7+eqr01ZFY4KeBb0JrPPP538in+TdsUrNmuWU9bp1kqJWW/eNMSfAgt4EVnIyMWsXMXTaVW7A/fEkJcHdd9OhT31mzMDOkjXGTxb0JrB69HBjK+fPh1q1jl+2ZUt49VW63tCBjRth1arTUkNjgp4FvQmsZO+M7W3bjju0stjhw/Rvtxlws2saY8pnUyCYwOrR48j94wytLHbDDbRfvhJYRVpapdXKmJBiLXoTWI0awfDh7r4/Lfr27QnfsI642MMW9Mb4yVr0JvDuuMPNh9yhQ/llzzoLDh+mT9MNpKX58cVgjPGvRS8iA0RktYikisjDpayPEpGJ3voFItKmxPpWIrJPRB6omGqbkJKcDG+/fWRuhOPxvgx6xaxm48bKrZYxoaLcoBeRcOANYCDQEbhRRDqWKHY7sEtV2wFjgOdLrP8b8NmpV9dUe14/fqfI1aSl2RBLY/zhT4u+F5CqqutVNQ/4EBhUoswgYLx3fzJwsYgIgIgMBjYAKyqmyqZaq18fXn6Zfb0uZt8+2LUr0BUypurzJ+hbAL4Tw2Z4y0oto6r5QA7QUETqAA8BTx3vBURkhIgsEpFFO3bs8Lfuprq6915q9XVz4tgBWWPKV9mjbp4ExqjqvuMVUtW3VTVZVZPj4uIquUom6GVl0WnHXADrpzfGD/6MuskEWvo8jveWlVYmQ0QigBggG+gNXCMiLwCxQKGI5KpqictFG3MC/vd/OWv0aOqTTVpag0DXxpgqz5+gXwgkiEhbXKDfANxUosw04FZgPnANMEfdVcfPKyogIk8C+yzkzSnzRt4kRa8hLc0uIGtMecoNelXNF5FRwOdAOPCuqq4QkaeBRao6DRgLTBCRVGAn7svAmMrRrBkAneK2W9eNMX7w64QpVZ0JzCyx7HGf+7nAteVs48mTqJ8xx4qJAaBN/d18ZwdjjSmXTYFggo8X9C3r5tioG2P8YEFvgk/9+jBlCrv6XsHOnbB3b6ArZEzVZkFvgk9EBAwZQmy3toCNpTemPBb0JjjNnUvH/QsBC3pjymOzV5rgdNddJLTrDEyyoDemHNaiN8EpJoboQznUqGEtemPKY0FvglNMDLInh1atbBoEY8pjQW+CU0wM5OTQurW16I0pjwW9CU4W9Mb4zYLeBKcHHoB//5vWrWHrVsjNDXSFjKm6bNSNCU7elabarHIP09P9u7a4MdWRtehNcFq3DsaNo22TA4B13xhzPBb0Jjh9+y3cdhstI7YAsGVLgOtjTBVmQW+CkzexWQw5AOTkBLIyxlRtFvQmOHlBX7fQJfzu3YGsjDFVmwW9CU5e0Efsz6F2bQt6Y47Hgt4EJy/oyckhNtaC3pjjsaA3wSk+HlJSYPBgC3pjymHj6E1wqlEDkpIALOiNKYe16E3weucdmD3bgt6YcljQm+D1xBPwwQcW9MaUw4LeBC9vYjMLemOOz4LeBK8SQa8a6AoZUzVZ0Jvg5RP0hYWwb1+gK2RM1WRBb4KXF/RFQ+qt+8aY0tnwShO8XnsNRIj9yj3MyYGWLQNaI2OqJAt6E7waNwbcOHqwFr0xZbGuGxO8vv8eHnuM+rXzAAt6Y8piQW+C18KF8OyzNAhzCW9Bb0zp/Ap6ERkgIqtFJFVEHi5lfZSITPTWLxCRNt7yXiKS4t2WisiQiq2+qda8PptYsamKjTmecoNeRMKBN4CBQEfgRhHpWKLY7cAuVW0HjAGe95YvB5JVtSswAPiHiNhxAVMxbE56Y/ziT4u+F5CqqutVNQ/4EBhUoswgYLx3fzJwsYiIqh5Q1XxveTRgp7SYiuMFfeSBHGrVsqA3piz+BH0LYJPP4wxvWallvGDPARoCiEhvEVkBLAN+5xP8xURkhIgsEpFFO3bsOPG9MNWTzUlvjF8q/WCsqi5Q1U5AT+AREYkupczbqpqsqslxcXGVXSUTKhITYc8em5PemHL4E/SZgO9pKPHeslLLeH3wMUC2bwFVXQXsAxJPtrLGHCUiAurWhbAwC3pjjsOfoF8IJIhIWxGpAdwATCtRZhpwq3f/GmCOqqr3nAgAEWkNdAA2VkjNjVGF0aNh+nQLemOOo9wRMKqaLyKjgM+BcOBdVV0hIk8Di1R1GjAWmCAiqcBO3JcBQD/gYRE5DBQCd6lqVmXsiKmGROCttyA/n9jYK1m9OtAVMqZq8muoo6rOBGaWWPa4z/1c4NpSnjcBmHCKdTSmbLGxdjDWmHLYmbEmuNmc9MaUy4LeBDefoC8ogP37A10hY6oeC3oT3GJi4NAhm8HSmOOwoDfB7dNP4euvi4M+Jyew1TGmKrKgN8EtzP0Ttha9MWWzoDfBbepUGDrUgt6Y47CgN8Ft5UoYP576NXMBC3pjSmNBb4Kb15SPwaYqNqYsFvQmuHkzWFrQG1M2C3oT3Lygr3Ewh5o1LeiNKY0FvQluDRpAw4bFY+kt6I05ll3WzwS3c8+FLDdPngW9MaWzFr0JGRb0xpTOWvQm+N11F8THExv7R+xKlMYcy1r0JvgtXQpffGEtemPKYEFvgl/79rB2rQW9MWWwoDfBLyEBNm+mca19Nie9MaWwoDfBr317ANrkp5KfDwcOBLg+xlQxFvQm+HXoAF27Uj/KJbx13xhzNAt6E/wSE+HHHznU41zA5qQ3piQLehMybKpiY0pn4+hNaLj3XnouWQ9Mt6A3pgRr0ZvQkJtLveXzAWvRG1OSBb0JDe3bE747m/rsZMuWQFfGmKrFgt6EBm+IZf+ma1mwIMB1MaaKsaA3ocEL+svaruHbb+2kKWN8WdCb0NC2LVx5JS27NWLzZti4MdAVMqbqsKA3oaFGDZg2jZYjBgLw7bcBro8xVYgFvQkpiWcdpl49C3pjfPkV9CIyQERWi0iqiDxcyvooEZnorV8gIm285ZeIyGIRWeb9vahiq2+Mj8cfJ7xJI87prRb0xvgoN+hFJBx4AxgIdARuFJGOJYrdDuxS1XbAGOB5b3kWcKWqdgZuBSZUVMWNOUbjxrBnD5clbWX5chtPb0wRf1r0vYBUVV2vqnnAh8CgEmUGAeO9+5OBi0VEVPVHVd3sLV8B1BSRqIqouDHHSEgA4IIWa1GF+fMDXB9jqgh/gr4FsMnncYa3rNQyqpoP5AANS5S5GliiqodKvoCIjBCRRSKyaIddC86cLG+IZaeI1YSHWz+9MUVOy8FYEemE6865o7T1qvq2qiaranJcXNzpqJIJRa1bQ+3aRK1ZRteuFvTGFPEn6DOBlj6P471lpZYRkQggBsj2HscDnwC3qOq6U62wMWUKC4NHH4ULL6RvX1iwAA4fDnSljAk8f4J+IZAgIm1FpAZwAzCtRJlpuIOtANcAc1RVRSQWmAE8rKrWvjKV75FHYMgQ+vaFgwchJSXQFTIm8MoNeq/PfRTwObAKmKSqK0TkaRH5lVdsLNBQRFKB+4CiIZijgHbA4yKS4t0aV/heGFOksBDWrqVf0l4AvvsuwPUxpgrwaz56VZ0JzCyx7HGf+7nAtaU871ng2VOsozH+W7wYevWi+ZQp1Ks3hNTUQFfImMCzM2NNaElMdH31KSm0bAnp6YGukDGBZ0FvQkvNmnDWWZCSQqtWsGlT+U8xJtRZ0JvQ07VrcYvegt4YC3oTipKSID2dhIY7ycpyo2+Mqc7s4uAm9Fx1FSQk0DQrGoCMjOLZEYyplqxFb0JPQgJcdRXN29UCrPvGGAt6E5p++IGz0v8DWNAbY103JjQ98QTNMrcAKRb0ptqzFr0JTUlJhP28kmYN8yzoTbVnQW9CU9eucPgw/eNWWdCbas+C3oSmrl0B6FPTum6MsaA3oSkhAaKi6MQKC3pT7VnQm9AUHg6LFrF88J/IyYG9ewNdIWMCx4LehK7ERJok1ANsiKWp3izoTehauJC+nz+GUGizWJpqzYLehK7Fi2k1/lmas9la9KZas6A3oatdOwASSLWgN9WaBb0JXd5MZskxay3oTbVmQW9CV3w81KhBl1rWojfVmwW9CV3h4XDmmZwRkX7CQb93L2zbVjnVMuZ0s6A3oe3775lyzfts2gSq/j/tgQfg0ksrr1rGnE4W9Ca01atHy1bCwYOwc6f/T1u5EtasObEvB2OqKgt6E9oWL2bIJ7fQhK0n1H2Tlga5ubB7d+VVzZjTxYLehLZdu2g9bwId+NnvoM/Ph8xMd3/LlsqrmjGniwW9CW3eWPp2JzCWfvNmKCw8ct+YYGdBb0Jby5ZojRp0CFvLxo3+PcV3ugRr0ZtQYEFvQlt4OHLGGSTXT+U///HvKb5Bby16Ewos6E3oS0wkvlkhKSmwfn35xYuCPirKWvQmNFjQm9D30UeET/sEgClTyi+eng4NGkDr1taiN6HBr6AXkQEislpEUkXk4VLWR4nIRG/9AhFp4y1vKCJzRWSfiLxesVU3xn9t20K3bv4HfatW0Ly5Bb0JDeUGvYiEA28AA4GOwI0i0rFEsduBXaraDhgDPO8tzwUeAx6osBobc6JWrYKLLiUpaOYAABMvSURBVOLuXguYP7/88E5Pd6355s2t68aEBn9a9L2AVFVdr6p5wIfAoBJlBgHjvfuTgYtFRFR1v6p+gwt8YwIjOhrmzmVAi2UATJ16/OJFLfpmzdyXgp0da4KdP0HfAvAdgZzhLSu1jKrmAzlAQ38rISIjRGSRiCzasWOHv08zxj+tWkGNGjTbs5oOHeDjj8sumpPjbkVdN7m57rExwaxKHIxV1bdVNVlVk+Pi4gJdHRNqwsOhZ0+YO5erroL//heyskovWjTipqhFD9ZPb4KfP0GfCbT0eRzvLSu1jIhEADFAdkVU0JgKMXAgLF7MdRdsp6AApk8vvZhv0Ddv7u5bP70Jdv4E/UIgQUTaikgN4AZgWoky04BbvfvXAHNUrWfTVCGXXw4DBtAlfictW8K0kv+CPdaiN6EoorwCqpovIqOAz4Fw4F1VXSEiTwOLVHUaMBaYICKpwE7clwEAIrIRqAfUEJHBwKWqurLid8WY4+jWDT77DAEuuQQ++QQKClyvjq/0dIiMhKZNYf9+t8xa9CbYlRv0AKo6E5hZYtnjPvdzgWvLeG6bU6ifMRVrxw4uOr8B774bztKl0L370avT090VCMPCoG5dqFPHWvQm+FWJg7HGnBYzZkCTJlzScAkAc+ceW6RoaGURG0tvQoEFvak+evcGoPHiz2jf3v+gtxa9CXYW9Kb6aNTIDbOcNYsLL4R589xFRooUXXDEN+ibNbMWvQl+FvSmehkwABYs4LKeO9m7FxYvPrJqyxZ3gLa0Fr2NITPBzILeVC8DB0JhIRcWfAkc3X2Tlub+lmzRHzxoZ8ea4GZBb6qXnj3hrbeIvaIfnTodHfS+Y+iL2ElTJhRY0JvqJTwc7rgDmjfnwgvhm28gL8+tKgr6lj7ngdtJUyYUWNCb6ic3F8aN4+r4BRw4AD/84Banp0P9+m78PN99Bx98YC16ExL8OmHKmJASFgb33Uefy36FSG+efRZE4KuvoGNH3JHXESMgNZVmawYA9a1Fb4KatehN9VOjBgweTNRnU+mbfIjPP4eNG+G3v4V//hOYPx9WrIBDh6g76yPq1LEWvQluFvSmerruOtizhxm//w/p6e4iVK++6k2J8Pbbrv/mnXfgqqvspCkT9CzoTfV08cUQG0u9WZOOOvgKwF/+ApMmwe23Q6NGdtKUCXoW9KZ6qlEDhgyBjIxjz4Zq2tSdWAUwdizXH3rPWvQmqFnQm+rrrbdgzhx3JBZc4A8fDl98caTM++9z/eqn2bBeWbEiMNU05lRZ0Jvqq0YN9/fll+H1193VSMaOdUdmi9xyCw12rePiWvN58MGA1NKYU2ZBb8z06XD33TB4MNSuDTfeeGTdVVdBzZq80HkCM2fCl18GrprGnCwLemNmz3bDKZ966siImyJ168LVV5P003sMbraA++93E58F3BtvYD8xjL+kql3aNTk5WRctWhToahhzxObNcPPNfHrlP7jyD+14910YNiyA9SkogAjvXMfsbGjQIICVMVWFiCxW1eTS1tmZscaUp3lzmD2bXyr0/kAZ9/ufKFhVk2sv3klMo0g3+L7ogK4nIwP27IFDh1wmJyYeU+TkZWVBbCzs3u2umnXzzRW0YROqrOvGGD+JwPTkp5i3pyvDXzyLmAF9IDmZPedeBnv3ArBhg+vWb9kSOnVy3wFdurhfAIfzFL7/3qW/v15/Hf7976OXNWniWvLNm8PUqRW4hyZUWYvemBMQ9+gISGxC+q66TP26IRmzf6bX998y5rI6JPeEsf/IR8MjeOIJN29OVJSbUeHN5/dw2xe/pf+WSXDJJS68a9Y8/ou99hrccw/ExMD69a6L5vBhF/JNm8KLL0Jc3OnZcRPcVLVK3Xr06KHGBIudO1XHjFFNSFCNJ1131GqpO//nTdX8/KPKvfdchm6mqc5q9GstFFG99FLVAweOv/Gvv1YdNEhVRHX0aLfs009Vw8JUv/22kvbIBCtgkZaRqwEP9pI3C3oTjAoKVHf+sFb1ggvcf6uEBNUuXVSbNFE9dEhVVae8t1ejolSH8q4WIPr9DX/TtDTVwsISG9u16+jHt9yiGh2ts97ZpOkX3Kxav37xNjUlRXXKlMrfQVPlHS/orY/emAoQFgb1e7ZzZ9p+9BHEx0Pr1jBoUHGf/JCb67BhA3T4yzBuiZ/LuR/eQ+vWMLjul1zdO4Mpf/iagt/c6q528uqrxdveee9TfHTGQ/xmeBQxX03l67irOJDvnez1wgtuSuUTGfO5fr0bnukdV6jqqsRw1mBX1jdAoG7WojfVQWGh6g8/qP79tcO6u1ZT9ysAdI/U1ZX979AVn67X6dNVX35ZtXFj1chI1ck3fawK+gv+ox07qn70keq8uyepgk66e55+8IHqnDmqmZnHeVFV1TvucK/Xvr3qTz+dtn0+GTNnuh8wc+cGuiZVH9Z1Y0wVtn69Fj7xpC6/b6xekLy3KPOLb926qS5dqqo33aQK+p/PDmvjxm5dHfZoLjX079yhMexSUD0rbI0uaned5nXvpRofrxob674pJk1SVdWcnzfrwj9O0fzGTVVr1lQdO7aU/qPA27VLtXlzt59JSccc9jAlHC/o7YQpY6oQVXfB8t27oUULd2ve3HUNFQ+zHDSI3bth9Wp36cPWI39J1Jcz2frwy6y85F7+OzaVX79/ORnhbajfuQUNWtUhtkVt9l95Iy99mcQ//+l6bRqzjU/r3UTPPXNY8cZXtP/t+URG+lRm3z43JYTvCQCqHF74IxEtmiItmlfqezF8OIwbB6NHw/PPu2mIbrutUl8yqB3vhKmAt+BL3qxFb8wJ+vln18fj0w2zcqXqL3+px/w6CA93PwxmzlR95hnVc3rm67VMVCjU2rVVH+82TRf2u1cPtEss7t4pmDxFt2xR/eK+z3RtTHdV0ENE6uKuwzRt1spK2aUvvnAv/9BD7sdGnz6qTZuq7t3r1k/7JF8HN/ten3lkf3C19LdtU12woFI2jbXojameduxwV89atQp27YKbboJWrY4tM28eLPwsi8ffbY1oId/Qj6XR59Av/yv+XPAQ0/UKbuY9Hot4jiXn3k3UhlVcumksi+nB7Qlfc/bZ8IetDxGWf4jCrTuI2LWdWvl7+LLxr/mqyz20ijvIn768AKlbh7Ba0RQiKMK2Hr/kwC2/o3lzN63Q5s2waZObYy46Gn5cotSsJSz45jDnnaf88f48ui8dR6cvXyZWcmilafS7tDb/eq+ARk3CT+5N2rzZncqclOROfDgBW7a497ZvXz+eumEDXHopHDgA69ZBXh48/DDceSd07nxydfdxvBa9X0EvIgOAV4Bw4B1V/UuJ9VHAe0APIBu4XlU3euseAW4HCoB7VPXz472WBb0xAZSWxuaCJsz+Npp589z0DY0aufOyzu1dQI9kQcLdYL3MlB18NiGLWWlns27lIeasako4BeTUiONg3cbkRccwu8G1TKhxO3sy9vDatuuozT6iyfViXnmOR5jMtZxJKl9zHnuox0FqohJGYswmIt9/DwYOhM8+g8svJ59wIiggtcm5tHp1NONzBnPPyAKW0I1GUfvID4ukgAj21G3BqqTr2XjR7TSL3sW5Mx4lNiuVvPwwMiLasCa3FYvPvI4uV7Xjl+GzaPDrgRTWiGJbi+5sju1EdL0abB/6IDFdWlNv21rqrP2Rw7FxbMmKZPO2cDatzeUfqy9gxUphMJ9wYZ1F1L9tCIOf6UFd3eO+PM4+m8OHYXfKRvambqP5qCGQm8t7181gbaM+JGz8D8Om/orI/Fy2NO/Bz32GUTjsdi7+ZfRJfXSnFPQiEg6sAS4BMoCFwI2qutKnzF1AF1X9nYjcAAxR1etFpCPwAdALaA58CbRX1TIHTFnQGxOc8vNdd354GQ3r/HzYts39sggPd7e8PNi6Ffb/sIJ2016iXvgB6kQcpHZ0ATXaxruho927w5o17H57Ep9O3E/D2wYx8Klzire75L97WXHtk9Tat51IySdK8mhyKJ0phYN4pvBPRJFLJi1YzxkAtGEjcWTx15qPMfrg00SRy5CwafQo/IE+zOcM1hNBPhczm2V0YRSv8Rr3HLM/t/ZNJXHQmVwx/1HaT32ecC1gJ/VpwC5+DutIcs0V7N8PS+hGN1LIoAWX8TmrwzsVt/4bFGZx9aH3uVXH0YwtPHB9Bv/34clNWHCqQd8HeFJVL/MePwKgqs/5lPncKzNfRCKArUAc8LBvWd9yZb2eBb0xpiKouonlsrNh5073t149N8Fc3bD9FIZFkLIqii++cOWSk6FnT4qvEZyRAdu3Q/6uvURmbiR6XxZNG+XTtFE+DZtGEt6vjztYDZCdzfpXp7Pto3lsr3MGm5r1ZkO7S4iNhe6bP6XpwfUU/OoqmvWMp3nzo78MVd0X3oEN26BJE+rXP7n9PdWgvwYYoKrDvcc3A71VdZRPmeVemQzv8TqgN/Ak8L2q/p+3fCzwmapOLvEaI4ARAK1ateqRlpZ2MvtpjDHV1vGCvkqcGauqb6tqsqomx9kkTcYYU6H8CfpMoKXP43hvWallvK6bGNxBWX+ea4wxphL5E/QLgQQRaSsiNYAbgGklykwDbvXuXwPM8cZ1TgNuEJEoEWkLJAA/VEzVjTHG+KPcw7uqmi8io4DPccMr31XVFSLyNG6A/jRgLDBBRFKBnbgvA7xyk4CVQD4w8ngjbowxxlQ8O2HKGGNCQJU/GGuMMabyWNAbY0yIs6A3xpgQV+X66EVkB3AqZ0w1ArIqqDrBojruM1TP/bZ9rj5OdL9bq2qpJyJVuaA/VSKyqKwDEqGqOu4zVM/9tn2uPipyv63rxhhjQpwFvTHGhLhQDPq3A12BAKiO+wzVc79tn6uPCtvvkOujN8YYc7RQbNEbY4zxYUFvjDEhLmSCXkQGiMhqEUkVkYcDXZ/KICItRWSuiKwUkRUicq+3vIGI/EdE1np/T/IaNVWbiISLyI8i8qn3uK2ILPA+84ne7KohQ0RiRWSyiPwsIqtEpE91+KxF5A/ev+/lIvKBiESH4mctIu+KyHbvwk1Fy0r9fMV51dv/n0Sk+4m8VkgEvXdd2zeAgUBH4EbverWhJh+4X1U7AucAI739fBiYraoJwGzvcSi6F1jl8/h5YIyqtgN24S5CH0peAWapagcgCbfvIf1Zi0gL4B4gWVUTcTPm3kBoftb/Cwwosaysz3cgbpr3BNzV+P5+Ii8UEkGPu/h4qqquV9U84ENgUIDrVOFUdYuqLvHu78X9x2+B29fxXrHxwODA1LDyiEg88EvgHe+xABcBRZelDKn9FpEYoD9uCnBUNU9Vd1MNPmvc9Ok1vYsY1QK2EIKftarOw03r7qusz3cQ8J463wOxItLM39cKlaBvAWzyeZzhLQtZItIG6AYsAJqo6hZv1VagSYCqVZleBh4ECr3HDYHdqprvPQ61z7wtsAMY53VXvSMitQnxz1pVM4G/Aum4gM8BFhPan7Wvsj7fU8q4UAn6akVE6gAfA79X1T2+67wre4XUmFkRuQLYrqqLA12X0ygC6A78XVW7Afsp0U0Top91fVzrtS3QHKjNsd0b1UJFfr6hEvTV5tq0IhKJC/l/qeoUb/G2op9x3t/tgapfJekL/EpENuK65S7C9V/Hej/vIfQ+8wwgQ1UXeI8n44I/1D/rXwAbVHWHqh4GpuA+/1D+rH2V9fmeUsaFStD7c13boOf1S48FVqnq33xW+V6z91bg36e7bpVJVR9R1XhVbYP7bOeo6q+BubhrFEOI7beqbgU2ichZ3qKLcZfkDOnPGtdlc46I1PL+vRftd8h+1iWU9flOA27xRt+cA+T4dPGUT1VD4gZcDqwB1gGPBro+lbSP/XA/5X4CUrzb5bj+6tnAWuBLoEGg61qJ78EFwKfe/TNwF5tPBT4CogJdvwre167AIu/zngrUrw6fNfAU8DOwHJgARIXiZw18gDsOcRj3C+72sj5fQHAjC9cBy3Cjkvx+LZsCwRhjQlyodN0YY4wpgwW9McaEOAt6Y4wJcRb0xhgT4izojTEmxFnQm2pJRApEJMXnVmGTg4lIG98ZCY0JtIjyixgTkg6qatdAV8KY08Fa9Mb4EJGNIvKCiCwTkR9EpJ23vI2IzPHmAp8tIq285U1E5BMRWerdzvU2FS4i//TmVf9CRGoGbKdMtWdBb6qrmiW6bq73WZejqp2B13GzZgK8BoxX1S7Av4BXveWvAv9V1STcXDQrvOUJwBuq2gnYDVxdyftjTJnszFhTLYnIPlWtU8ryjcBFqrrem0Buq6o2FJEsoJmqHvaWb1HVRiKyA4hX1UM+22gD/EfdxSMQkYeASFV9tvL3zJhjWYvemGNpGfdPxCGf+wXY8TATQBb0xhzrep+/87373+FmzgT4NfC1d382cCcUX9M25nRV0hh/WSvDVFc1RSTF5/EsVS0aYllfRH7Ctcpv9Jbdjbva02jclZ+GecvvBd4WkdtxLfc7cTMSGlNlWB+9MT68PvpkVc0KdF2MqSjWdWOMMSHOWvTGGBPirEVvjDEhzoLeGGNCnAW9McaEOAt6Y4wJcRb0xhgT4v4fTTf3eA6ydocAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-1H4KDojOJT",
        "colab_type": "text"
      },
      "source": [
        "`loss`와 `val_loss`는 40에포크를 넘어가면서 매우 가파르게 줄어들어 0에 가까워집니다. `val_loss`는 변동폭이 `loss`보다 크지만 전체적으로는 계속 감소하는 경향을 보입니다. 학습이 매우 잘 된 것으로 보입니다.\n",
        "\n",
        "이번에는 실제로 테스트 데이터에 얼마나 정확하게 값을 예측하는지 확인해봅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzYco1QpoTl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "818f05d1-4310-4de9-ec2f-eaf2778137b5"
      },
      "source": [
        "model.evaluate(X[2560:], Y[2560:])\n",
        "prediction=model.predict(X[2560:2560+5])\n",
        "\n",
        "# 5개 테스트 데이터에 대한 예측을 표시합니다. \n",
        "for i in range(5): \n",
        "  print(Y[2560+i], '\\t', prediction[i][0], '\\tdiff:', abs(prediction[i][0] - Y[2560+i]))\n",
        "\n",
        "prediction = model.predict(X[2560:])\n",
        "fail = 0\n",
        "for i in range(len(prediction)):\n",
        "  # 오차가 0.04 이상이면 오답입니다. \n",
        "  if abs(prediction[i][0] - Y[2560+i]) > 0.04:\n",
        "    fail +=1\n",
        "\n",
        "print('correctness:', (440-fail)/440*100, '%')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 0s 13ms/step - loss: 3.9453e-04\n",
            "0.009316712705671063 \t 0.02192213 \tdiff: 0.012605417432010898\n",
            "0.1595728709352136 \t 0.15154022 \tdiff: 0.008032651151430648\n",
            "0.343633615267837 \t 0.33932722 \tdiff: 0.004306399119460513\n",
            "0.047836290850227836 \t 0.03347582 \tdiff: 0.014360470875090126\n",
            "0.07471709800989841 \t 0.06377047 \tdiff: 0.01094662500651096\n",
            "correctness: 95.9090909090909 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIplmAzGoXTf",
        "colab_type": "text"
      },
      "source": [
        "테스트 데이터에 대한 `loss`는 0에 가까운 값이 나오고, 다섯 개의 샘플에 대한 오차도 0.04를 넘는 값이 없습니다. 또한 정확도 역시, 95.9%로 거의 96%에 가까운 것을 확인할 수 있습니다. \n",
        "\n",
        "곱셈문제를 푸는데 있어서 `LSTM`이 보다 적합하다는 것을 알 수 있습니다. \n",
        "\n",
        "다음 포스트에서는 `GRU`레이어와 `임베딩`레이어에 대해 학습하도록 합니다. \n",
        "\n",
        "## IV. 연습 파일\n",
        "- [구글 Colab에서 직접 연습해보자](https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch7_1_2_RNN_theory(1).ipynb) \n",
        "\n",
        "## VI. Reference\n",
        "\n",
        "김환희. (2020). 시작하세요! 텐서플로 2.0 프로그래밍: 기초 이론부터 실전 예제까지 한번에 끝내는 머신러닝, 딥러닝 핵심 가이드. 서울: 위키북스."
      ]
    }
  ]
}