{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch7_4_naturalLanguageGeneration(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNY+4CfIY594gkylru/uXfl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch7_4_naturalLanguageGeneration(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_qDfvmcn_Za",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "title: \"Tensorflow 2.0 Tutorial ch7.4 - (1) 단어 단위 생성\"\n",
        "date: 2020-04-26T20:20:30+09:00\n",
        "tags:\n",
        "  - \"Deep Learning\"\n",
        "  - \"Python\"\n",
        "  - \"Google Colab\"\n",
        "  - \"Tensorflow 2.0\"\n",
        "  - \"Binary Classification\"\n",
        "  - \"Classification\"\n",
        "  - \"Sentiment Analysis\"\n",
        "  - \"Natural Language Generation\"\n",
        "  - \"자연어 생성\"\n",
        "  - \"텐서플로 2.0\"\n",
        "  - \"텐서플로 2.0 튜토리얼\"\n",
        "  - \"Tensorflow 2.0 Tutorial\"\n",
        "categories:\n",
        "  - \"Deep Learning\"\n",
        "  - \"딥러닝\"\n",
        "  - \"텐서플로 2.0\"\n",
        "  - \"Python\"\n",
        "  - \"Tensorflow 2.0\"\n",
        "  - \"텐서플로 2.0 튜토리얼\"\n",
        "  - \"Tensorflow 2.0 Tutorial\"\n",
        "menu: \n",
        "  python:\n",
        "    name: Tensorflow 2.0 Tutorial ch7.4 - (1) 단어 단위 생성\n",
        "---\n",
        "\n",
        "## 공지\n",
        "\n",
        "- 본 Tutorial은 교재 `시작하세요 텐서플로 2.0 프로그래밍`의 강사에게 국비교육 강의를 듣는 사람들에게 자료 제공을 목적으로 제작하였습니다. \n",
        "\n",
        "- 강사의 주관적인 판단으로 압축해서 자료를 정리하였기 때문에, 자세하게 공부를 하고 싶으신 분은 반드시 교재를 구매하실 것을 권해드립니다. \n",
        "\n",
        "![](/img/tensorflow2.0/book.jpg)<!-- -->\n",
        "\n",
        "\n",
        "- 본 교재 외에 강사가 추가한 내용에 대한 Reference를 확인하셔서, 추가적으로 학습하시는 것을 권유드립니다. \n",
        "\n",
        "\n",
        "## Tutorial\n",
        "\n",
        "이전 강의가 궁금하신 분들은 아래에서 선택하여 추가 학습 하시기를 바랍니다. \n",
        "\n",
        "- [Google Colab Tensorflow 2.0 Installation](https://chloevan.github.io/python/tensorflow2.0/googlecolab/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.1 - 난수 생성 및 시그모이드 함수](https://chloevan.github.io/python/tensorflow2.0/ch3_3_1_random_signoid/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.2 - 난수 생성 및 시그모이드 함수 편향성](https://chloevan.github.io/python/tensorflow2.0/ch3_3_2_random_signoid_bias/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.3 - 첫번째 신경망 네트워크 - AND](https://chloevan.github.io/python/tensorflow2.0/ch3_3_3_network_and/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.4 - 두번째 신경망 네트워크 - OR](https://chloevan.github.io/python/tensorflow2.0/ch3_3_4_network_or/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.5 - 세번째 신경망 네트워크 - XOR](https://chloevan.github.io/python/tensorflow2.0/ch3_3_5_network_xor/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.1 - 선형회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_1_linear_regression/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.2 - 다항회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_2_multiple_linear_regression/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.3 - 딥러닝 네트워크를 이용한 회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_3_regression_with_deeplearning/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.4 - 보스턴 주택 가격 데이터세트](https://chloevan.github.io/python/tensorflow2.0/ch4_4_boston_housing_deeplearning/)\n",
        "- [Tensorflow 2.0 Tutorial ch5.1 - 분류](https://chloevan.github.io/python/tensorflow2.0/ch5_1_binary_classification/)\n",
        "- [Tensorflow 2.0 Tutorial ch5.2 - 다항분류](https://chloevan.github.io/python/tensorflow2.0/ch5_2_multi_classification/)\n",
        "- [Tensorflow 2.0 Tutorial ch5.3 - Fashion MNIST](https://chloevan.github.io/python/tensorflow2.0/ch5_3_fashion_mnist/)\n",
        "- [Tensorflow 2.0 Tutorial ch6.1-2 - CNN 이론](https://chloevan.github.io/python/tensorflow2.0/ch6_1_2_cnn_theory/)\n",
        "- [Tensorflow 2.0 Tutorial ch6.3 - Fashion MNIST with CNN 실습](https://chloevan.github.io/python/tensorflow2.0/ch6_3_fashion_mnist_with_cnn/)\n",
        "- [Tensorflow 2.0 Tutorial ch6.4 - 모형의 성능 높이기](https://chloevan.github.io/python/tensorflow2.0/ch6_4_improve_performance/)\n",
        "- [Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (1)](https://chloevan.github.io/python/tensorflow2.0/ch7_1_2_rnn_theory1/)\n",
        "- [Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (2)](https://chloevan.github.io/python/tensorflow2.0/ch7_1_2_rnn_theory2/)\n",
        "- [Tensorflow 2.0 Tutorial ch7.3 - 긍정, 부정 감성 분석](https://chloevan.github.io/python/tensorflow2.0/ch7_3_sentimentanalysis/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcD2TKoazzaJ",
        "colab_type": "text"
      },
      "source": [
        "## I. 개요\n",
        "\n",
        "테슬라 `AI Director`인 안드레아 카르파티(`Andrej Karpathy`)는 `The Unreasonable Effectiveness of Recurrent Neural Networks`라는 글을 개인 블로그에 작성했는데, 짧게 요약하면 문자 단위의 순환 신경망이 셰익스피어의 희곡, 소스코드, `Latex`등을 재생산하는데 순환 신경망이 효과적이라는 것을 보여줍니다. \n",
        "\n",
        "이를 바탕으로, 한글 원본 텍스트를 자소 단위와 단어 단위로 나눠서 순환 신경망으로 생성해보도록 합니다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4iORyGw2waj",
        "colab_type": "text"
      },
      "source": [
        "## II. 단어 단위 생성\n",
        "이번에 작업할 원본 텍스트는 국사편찬위원회에서 제공하는 조선왕조실록 국문 번역본입니다. 먼저 [데이터](https://raw.githubusercontent.com/greentec/greentec.github.io/master/public/other/data/chosundynasty/corpus.txt)를 다운로드 합니다.[^1]\n",
        "\n",
        "이제 본격적으로 소스코드를 작성하도록 합니다. \n",
        "\n",
        "\n",
        "[^1]: 교재에서는 전체 데이터(400MB) 중, 62MB만 잘라낸 파일을 사용했습니다. [공공데이터포털](https://www.data.go.kr/)에서 조선왕조실록 데이터를 다운로드 받을 수 있습니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GMR8X5K3C3n",
        "colab_type": "text"
      },
      "source": [
        "### (1) 데이터 로드\n",
        "데이터 파일은 약 62MB 정도입니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1gWw2ee1tMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 텐서플로 2 버전 선택\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEdUWPfI1th4",
        "colab_type": "code",
        "outputId": "8a5cb36f-2773-482f-c4e6-af4cfa8cfc29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "path_to_train_file = tf.keras.utils.get_file('input.txt', 'https://raw.githubusercontent.com/greentec/greentec.github.io/master/public/other/data/chosundynasty/corpus.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/greentec/greentec.github.io/master/public/other/data/chosundynasty/corpus.txt\n",
            "62013440/62012502 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLH4ycbituC3",
        "colab_type": "text"
      },
      "source": [
        "데이터를 메모리에 불러옵니다. 인코딩 형식으로 `utf-8`을 지정한 뒤 텍스트가 총 몇 자인지 확인해봅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlgmgrnU4JBi",
        "colab_type": "code",
        "outputId": "1cfde3ea-d983-4aec-be71-e43184e4d2ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_text = open(path_to_train_file, 'rb').read().decode(encoding='utf-8')\n",
        "print('Length of text: {} characters'.format(len(train_text)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 26265493 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DHFgqSmucnd",
        "colab_type": "text"
      },
      "source": [
        "처음 100자를 확인한 후, 실제 [데이터](https://raw.githubusercontent.com/greentec/greentec.github.io/master/public/other/data/chosundynasty/corpus.txt)와 일치하는지 확인해봅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6kMNC6xuevX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bc12e654-1354-4085-9d1c-e0c281e51395"
      },
      "source": [
        "print(train_text[:100])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 \n",
            "태조 강헌 지인 계운 성문 신무 대왕(太祖康獻至仁啓運聖文神武大王)의 성은 이씨(李氏)요, 휘\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w94Ew_dS5XVW",
        "colab_type": "text"
      },
      "source": [
        "일치하는지 확인이 완료되었다면, 다음 소스코드를 진행하면 됩니다. 다운로드 시, 링크가 달라졌다면, 꼭 댓글을 남겨주시기를 바랍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-loW86g7e7W",
        "colab_type": "text"
      },
      "source": [
        "### (2) 데이터 문자열 전처리\n",
        "한자가 많은 부분을 차지하는데, 여기에서는 단어 기반의 생성을 위해 한자와 한자가 들어간 괄호는 생략합니다. 이전 포스트에 비슷하지만, 조금 다릅니다. 특히 영문 관련 처리는 모두 삭제했습니다. 또한, 한자와 함께 들어가 있는 괄호도 같이 삭제합니다. 또한, 개행 문자('\\n')의 보존을 위해 텍스트를 먼저 개행 문자로 나눈 뒤에 다시 합칠 때 개행 문자를 추가합니다. 꼭 주의해서 사용자 정의 함수 `def`를 작성하시기를 바랍니다. 이렇게 정제 과정을 거치면 한자어와 괄호는 보이지 않을 것입니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPhQPzLA7c3M",
        "colab_type": "code",
        "outputId": "84070dbf-82de-4779-dd82-8350d7e13b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import re\n",
        "# From https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "def clean_str(string):    \n",
        "    string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \"\", string)\n",
        "    string = re.sub(r\"\\)\", \"\", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    string = re.sub(r\"\\'{2,}\", \"\\'\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string)\n",
        "\n",
        "    return string\n",
        "\n",
        "\n",
        "train_text = train_text.split('\\n')\n",
        "train_text = [clean_str(sentence) for sentence in train_text]\n",
        "train_text_X = []\n",
        "for sentence in train_text:\n",
        "    train_text_X.extend(sentence.split(' '))\n",
        "    train_text_X.append('\\n')\n",
        "    \n",
        "train_text_X = [word for word in train_text_X if word != '']\n",
        "\n",
        "print(train_text_X[:20])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['태조', '이성계', '선대의', '가계', '목조', '이안사가', '전주에서', '삼척', '의주를', '거쳐', '알동에', '정착하다', '\\n', '태조', '강헌', '지인', '계운', '성문', '신무', '대왕']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjodf39_8WJM",
        "colab_type": "text"
      },
      "source": [
        "결과가 잘 나온 것을 확인할 수 있습니다. 이제 단어를 토큰화합니다. 이 때, `Tokenizer`를 쓰지 않고, 직접 토큰화합니다. 단어의 수가 너무 많고, 모든 단어를 사용할 것이기 때문에, 단어의 빈도 수로 단어를 정렬하는 `Tokenizer`를 쓰면 불필요하게 많은 계산 시간을 쓰게 된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfrTDxx-YUg-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "f67102e0-5d7a-4924-9f47-9acf9584ccca"
      },
      "source": [
        "vocab = sorted(set(train_text_X))\n",
        "vocab.append('UNK')\n",
        "print ('{} unique words'.format(len(vocab)))\n",
        "\n",
        "# vocab list를 숫자로 맵핑하고, 반대도 실행합니다.\n",
        "word2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2word = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([word2idx[c] for c in train_text_X])\n",
        "\n",
        "# word2idx 의 일부를 알아보기 쉽게 print 해봅니다.\n",
        "print('{')\n",
        "for word,_ in zip(word2idx, range(10)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(word), word2idx[word]))\n",
        "print('  ...\\n}')\n",
        "\n",
        "print('index of UNK: {}'.format(word2idx['UNK']))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "332640 unique words\n",
            "{\n",
            "  '\\n':   0,\n",
            "  '!' :   1,\n",
            "  ',' :   2,\n",
            "  '000명으로':   3,\n",
            "  '001':   4,\n",
            "  '002':   5,\n",
            "  '003':   6,\n",
            "  '004':   7,\n",
            "  '005':   8,\n",
            "  '006':   9,\n",
            "  ...\n",
            "}\n",
            "index of UNK: 332639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPV-FhUNZzHW",
        "colab_type": "text"
      },
      "source": [
        "2번째 줄에서 텍스트에 들어간 각 단어가 중복되지 않는 리스트를 만든 다음에, 3번째 줄에서는 텍스트에 존재하지 않는 토큰을 `UNK`를 넣습니다. 총 단어 수는 332,640이고, 이 중 `UNK`의 인덱스는 332,639입니다. 이 인덱스는 나중에 임의의 문장을 입력할 때 텍스트에 미리 준비돼 있지 않았던 단어를 쓸 수 있기 때문에 그에 대한 토큰으로 쓰일 것입니다. \n",
        "\n",
        "토큰화가 잘 되었는지 확인하기 위해 처음 20개의 단어에 대한 토큰을 출력합니다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOaIBB59aKWn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "03246d2a-981d-412f-b62e-82327ecf6b64"
      },
      "source": [
        "print(train_text_X[:20])\n",
        "print(text_as_int[:20])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['태조', '이성계', '선대의', '가계', '목조', '이안사가', '전주에서', '삼척', '의주를', '거쳐', '알동에', '정착하다', '\\n', '태조', '강헌', '지인', '계운', '성문', '신무', '대왕']\n",
            "[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027\n",
            " 190295 256129      0 299305  25624 273553  36147 163996 180466  84413]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZCuSazTaQy-",
        "colab_type": "text"
      },
      "source": [
        "`태조`는 299,305로 `이성계`는 229,634로, 개행 문자는 0으로 토큰 변환이 된 것을 확인할 수 있습니다. 이제 학습을 위한 데이터세트를 만듭니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_wUGxppglGl",
        "colab_type": "text"
      },
      "source": [
        "### (3) 데이터셋 생성\n",
        "\n",
        "이 때, 기존과는 다르게, `train_X`, `train_Y`를 넘파이 `array`로 만드는 방식이 아닌 `tf.data.Dataset`를 이용해봅니다. `Dataset`의 장점은 간단한 코드로 데이터 섞기, 배치(`batch`)수만큼 자르기, 다른 `Dataset`에 매핑하기 등을 빠르게 수행할 수 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIAJvkZ9bCMa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "808f6ba8-3f25-4037-9475-f40bf6a5ae87"
      },
      "source": [
        "seq_length = 25\n",
        "examples_per_epoch = len(text_as_int) // seq_length\n",
        "sentence_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sentence_dataset = sentence_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "for item in sentence_dataset.take(1):\n",
        "    print(idx2word[item.numpy()])\n",
        "    print(item.numpy())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['태조' '이성계' '선대의' '가계' '목조' '이안사가' '전주에서' '삼척' '의주를' '거쳐' '알동에' '정착하다'\n",
            " '\\n' '태조' '강헌' '지인' '계운' '성문' '신무' '대왕' '의' '성은' '이씨' '요' ',' '휘']\n",
            "[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027\n",
            " 190295 256129      0 299305  25624 273553  36147 163996 180466  84413\n",
            " 224182 164549 230248 210912      2 330313]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZix_2Wuc-_D",
        "colab_type": "text"
      },
      "source": [
        "여기서는 `seq_length`를 25로 설정해서 25개의 단어가 주어졌을 때, 다음 단어를 예측하도록 데이터를 만듭니다.\n",
        "\n",
        "```python\n",
        "sentence_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W323CbUTdoFm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "`Dataset`를 생성하는 코드는 위와 같이 한 줄로 간단합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0AxO4BXdW1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_dataset = sentence_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVzWe-68eIi7",
        "colab_type": "text"
      },
      "source": [
        "- `Dataset`에 쓰이는 `batch()`함수는 `Dataset`에서 한번에 반환하는 데이터의 숫자를 지정합니다. \n",
        "- 여기에서는 `seq_length_1`을 지정했는데, 처음의 25개 단어와 그 뒤에 오는 정답이 될 1단어를 합쳐서 함께 반환하기 위해서입니다. \n",
        "- `drop_remainder=True`옵션으로 남는 부분은 버리도록 합니다. 그러면 출력해서 의도한대로 26단어가 반환되는 것을 확인할 수 있습니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL1fkF_pezum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "6717fddb-6725-49a0-986f-eb4be0e7bad0"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    return [chunk[:-1], chunk[-1]]\n",
        "\n",
        "train_dataset = sentence_dataset.map(split_input_target)\n",
        "for x,y in train_dataset.take(1):\n",
        "    print(idx2word[x.numpy()])\n",
        "    print(x.numpy())\n",
        "    print(idx2word[y.numpy()])\n",
        "    print(y.numpy())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['태조' '이성계' '선대의' '가계' '목조' '이안사가' '전주에서' '삼척' '의주를' '거쳐' '알동에' '정착하다'\n",
            " '\\n' '태조' '강헌' '지인' '계운' '성문' '신무' '대왕' '의' '성은' '이씨' '요' ',']\n",
            "[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027\n",
            " 190295 256129      0 299305  25624 273553  36147 163996 180466  84413\n",
            " 224182 164549 230248 210912      2]\n",
            "휘\n",
            "330313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQz1uI_hfOck",
        "colab_type": "text"
      },
      "source": [
        "위 코드의 목적은 기존 `Dataset`으로 새로운 `Dataset`으로 만드는 과정입니다. \n",
        "- 26개의 단어가 각각 입력과 정답으로 묶어서([25단어], 1단어) 형태의 데이터를 반환하게 만드는 `Dataset`을 만드는 것입니다.\n",
        "- 먼저, 26개의 단어를 25단어, 1단어로 잘라주는 함수를 `split_input_target(chunk)`로 정의한 다음 이 함수를 기존 `sentence_dataset`에 `map()`함수(참조: 텐서플로 [map 함수](http://bit.ly/2GZTyW1))를 이용해 새로운 데이터세트인 `train_dataset`를 만듭니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkFW0iKngxGM",
        "colab_type": "text"
      },
      "source": [
        "### (4) Shuffle, Batch 설정\n",
        "`Dataset`의 데이터를 섞고, `batch_size`를 다시 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pApUvhpg7KE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqZ4v_1TiN9s",
        "colab_type": "text"
      },
      "source": [
        "주의: 이 부분은 교재와 조금 상이합니다! 따라서, 위 코드로 구현 바랍니다. \n",
        "\n",
        "빠른 학습을 위해 한번에 512개의 데이터를 학습하게 하고, 데이터를 섞을 때의 `BUFFER_SIZE`는 10,000으로 설정합니다. `tf.data`는 이론적으로 무한한 데이터에 대해 대응 가능하기 때문에 한번에 모든 데이터를 섞지 않습니다. 따라서, 버퍼에 일정한 양의 데이터를 올려놓고 섞는데, 그 사이즈를 `10,000`으로 설정한 것입니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIJValuJisln",
        "colab_type": "text"
      },
      "source": [
        "### (5) 생성 모델 정의\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHx4idkpivdz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "d47d0e22-c784-4155-d2ce-8f9c66320e32"
      },
      "source": [
        "total_words = len(vocab)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 100, input_length=seq_length),\n",
        "    tf.keras.layers.LSTM(units=100, return_sequences=True),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.LSTM(units=100),\n",
        "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 25, 100)           33264000  \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 25, 100)           80400     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 25, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 332640)            33596640  \n",
            "=================================================================\n",
            "Total params: 67,021,440\n",
            "Trainable params: 67,021,440\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJNqp9KjjUW1",
        "colab_type": "text"
      },
      "source": [
        "위 소스코드를 통해서 얻고자 하는 결과물은 주어진 입력에 대해 332,640개의 단어 중 어떤 단어를 선택해야 하는지 고르게 하는 기능을 하게 만듭니다.\n",
        "\n",
        "이제 학습을 시킵니다. 그런데, 이번에 학습시키는 방법은 기존과 조금 다르니 주의깊게 소스코드를 확인하시기를 바랍니다. \n",
        "\n",
        "또한, 학습시간이 GPU로 할당해도 매우 길기 때문에, 모형 학습 시간 계획을 잘 세우기를 바랍니다. (참고: 안 에포크당 10분 정도 소요됨)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTSKD1kIkCqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def testmodel(epoch, logs):\n",
        "    if epoch % 5 != 0 and epoch != 49:\n",
        "        return\n",
        "    test_sentence = train_text[0]\n",
        "\n",
        "    next_words = 100\n",
        "    for _ in range(next_words):\n",
        "        test_text_X = test_sentence.split(' ')[-seq_length:]\n",
        "        test_text_X = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_X])\n",
        "        test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
        "\n",
        "        output_idx = model.predict_classes(test_text_X)\n",
        "        test_sentence += ' ' + idx2word[output_idx[0]]\n",
        "    \n",
        "    print()\n",
        "    print(test_sentence)\n",
        "    print()\n",
        "\n",
        "testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2_AfGvusUWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "41e2804c-fa74-4176-9db7-ce1193675f7c"
      },
      "source": [
        "history = model.fit(train_dataset.repeat(), epochs=10, steps_per_epoch=steps_per_epoch, callbacks=[testmodelcb], verbose=2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:From <ipython-input-13-afa2ad52312b>:14: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "\n",
            " 태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
            "\n",
            "533/533 - 411s - loss: 8.9367 - accuracy: 0.0722\n",
            "Epoch 2/10\n",
            "533/533 - 407s - loss: 8.3943 - accuracy: 0.0737\n",
            "Epoch 3/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c89b02876f61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestmodelcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    850\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMRMxUV5jpz6",
        "colab_type": "text"
      },
      "source": [
        "먼저 모델을 학습시키면서 모델의 생성 결과물을 확인하기 위해 `testmodel`이라는 이름으로 콜백 함수를 정의합니다. \n",
        "\n",
        "- 조금 더 콜백 함수에 대해서 정의를 내리면, 먼저 임의의 문장을 입력한 다음, `seq_length`만큼의 단어 25개를 선택합니다. \n",
        "\n",
        "```python\n",
        "test_test_X = test_sentence.split(' ')[-seg_length:]\n",
        "```\n",
        "\n",
        "- 그 다음에는 문장의 단어를 인덱스 토큰으로 바꿉니다. 이 때 사전에 등록되어 있지 않은 단어의 경우에는 `UNK` 토큰 값으로 바꿉니다. \n",
        "\n",
        "```python\n",
        "test_text_X = pad_sequences([test_text_X], maxlen = seq_length, padding='pre', value=word2idx['UNK'])\n",
        "```\n",
        "\n",
        "- `pad_sequences()`로 문장의 앞쪽이 빈 자리가 있을 경우 25단어가 채워지도록 패딩을 넣습니다. 이 때 패딩 갑인 `value`인수에는 앞에서 지정했던 `UNK` 토큰의 값인 `word2idx['UNK']`를 사용합니다. \n",
        "\n",
        "```python\n",
        "output_idx = model.predict_classes(test_text_X)\n",
        "```\n",
        "\n",
        "- `model.predict()`는 `Dense`레이어의 332,640개의 값을 반환하기 때문에 출력을 간결하게 하기 위해 `model.predict_classes()`함수를 사용합니다. 이 함수는 출력 중에서 가장 값이 큰 인덱스를 반환합니다. \n",
        "\n",
        "```python\n",
        "test_sentence += ' ' + idx2word[output_idx[0]]\n",
        "```\n",
        "\n",
        "- 출력 단어는 test_sentence의 끝 부분에 저장되어 다음 스텝의 입력에 활용됩니다. \n",
        "\n",
        "- 이렇게 정의된 콜백 함수는 `testmodelcb`라는 이름으로 저장되어 `tf.keras`의 `model.fit()`의 `callbacks`인수에 포함됩니다. \n",
        "\n",
        "- 마지막으로 모형을 학습시킵니다.\n",
        "\n",
        "```python\n",
        "history = model.fit(train_dataset.repeat(), epochs=10, steps_per_epoch=steps_per_epoch, callbacks=[testmodelcb], verbose=2)\n",
        "```\n",
        "\n",
        "- 학습을 시킬 때도 위에서 정의한 `Dataset`을 활용합니다. 그런데 주목해야 할 점은 `Dataset`에서 데이터를 끊임없이 반환하도록 `repeat()`함수를 사용합나다. \n",
        "- 넘파이 `array` 형태의 데이터를 사용할 때는 에포크마다 데이터를 한번씩 순회시켰지만, `Dataset`을 사용하면 데이터의 시작과 끝을 알 수 없기 때문에 에포크에 데이터를 얼마나 학습시키질를 `steps_per_epoch` 인수로 지정합니다. \n",
        "\n",
        "학습 결과를 살펴보면 처음에는 의미없는 단어가 반복되지만, 학습을 진행하면서 문맥이 연결되는 것을 확인할 수 있습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBUeLea2uKNS",
        "colab_type": "text"
      },
      "source": [
        "### (6) 임의의 문장을 사용한 단어 생성 확인\n",
        "이제 임의의 문장을 넣어 학습이 잘 되는지 난중일기의 한 구절을 입력해봅니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xst0S8kUuU8M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "84eff6f7-f46d-46d7-ef10-4df36392da41"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "test_sentence = '동헌에 나가 공무를 본 후 활 십오 순을 쏘았다'\n",
        "\n",
        "next_words = 100\n",
        "for _ in range(next_words):\n",
        "    test_text_X = test_sentence.split(' ')[-seq_length:]\n",
        "    test_text_X = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_X])\n",
        "    test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
        "    \n",
        "    output_idx = model.predict_classes(test_text_X)\n",
        "    test_sentence += ' ' + idx2word[output_idx[0]]\n",
        "\n",
        "print(test_sentence)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "동헌에 나가 공무를 본 후 활 십오 순을 쏘았다 그 그 그 그 그 그 그 그 그 그 그 그 그 그 그 깊어 깊어 깊어 깊어 깊어 감격함이 깊어 그 그 깊어 깊어 감격함이 감격함이 깊어 깊어 깊어 깊어 감격함이 감격함이 깊어 깊어 깊어 깊어 깊어 감격함이 감격함이 깊어 깊어 깊어 깊어 깊어 감격함이 깊어 깊어 깊어 감격함이 감격함이 깊어 깊어 깊어 깊어 감격함이 감격함이 깊어 깊어 깊어 깊어 깊어 감격함이 감격함이 깊어 깊어 깊어 깊어 깊어 감격함이 깊어 깊어 깊어 감격함이 감격함이 깊어 깊어 깊어 깊어 감격함이 감격함이 깊어 깊어 깊어 깊어 깊어 감격함이 감격함이 깊어 깊어 깊어 깊어 깊어 감격함이 깊어 깊어 깊어 감격함이 감격함이\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey9Vrg7avMWd",
        "colab_type": "text"
      },
      "source": [
        "전체적인 문장의 의미가 통하는 건 아닙니다만, 부분 부분에서는 자연스럽게 연결되는 단어들이 보이는 것을 확인할 수 있습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIplmAzGoXTf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## III. 연습 파일\n",
        "- [구글 Colab에서 직접 연습해보자](https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch7_3_sentimentAnalysis.ipynb) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QXGf2VuqEeL",
        "colab_type": "text"
      },
      "source": [
        "## IV. Reference\n",
        "\n",
        "김환희. (2020). 시작하세요! 텐서플로 2.0 프로그래밍: 기초 이론부터 실전 예제까지 한번에 끝내는 머신러닝, 딥러닝 핵심 가이드. 서울: 위키북스.\n",
        "\n",
        "Karpathy, A. (2015). The Unreasonable Effectiveness of Recurrent Neural Networks. Retrieved April 26, 2020, from http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
      ]
    }
  ]
}