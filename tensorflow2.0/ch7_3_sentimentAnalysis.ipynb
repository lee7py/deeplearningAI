{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch7_3_sentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNhvSQ+PvMjtbhzQwgNX9mC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch7_3_sentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_qDfvmcn_Za",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "title: \"Tensorflow 2.0 Tutorial ch7.3 - 긍정, 부정 감성 분석\"\n",
        "date: 2020-04-25T11:08:30+09:00\n",
        "tags:\n",
        "  - \"Deep Learning\"\n",
        "  - \"Python\"\n",
        "  - \"Google Colab\"\n",
        "  - \"Tensorflow 2.0\"\n",
        "  - \"Binary Classification\"\n",
        "  - \"Classification\"\n",
        "  - \"Sentiment Analysis\"\n",
        "  - \"긍정, 부정 감성분석\"\n",
        "  - \"텐서플로 2.0\"\n",
        "  - \"텐서플로 2.0 튜토리얼\"\n",
        "  - \"Tensorflow 2.0 Tutorial\"\n",
        "categories:\n",
        "  - \"Deep Learning\"\n",
        "  - \"딥러닝\"\n",
        "  - \"텐서플로 2.0\"\n",
        "  - \"Python\"\n",
        "  - \"Tensorflow 2.0\"\n",
        "  - \"텐서플로 2.0 튜토리얼\"\n",
        "  - \"Tensorflow 2.0 Tutorial\"\n",
        "menu: \n",
        "  python:\n",
        "    name: Tensorflow 2.0 Tutorial ch7.3 - 긍정, 부정 감성 분석\n",
        "---\n",
        "\n",
        "## 공지\n",
        "\n",
        "- 본 Tutorial은 교재 `시작하세요 텐서플로 2.0 프로그래밍`의 강사에게 국비교육 강의를 듣는 사람들에게 자료 제공을 목적으로 제작하였습니다. \n",
        "\n",
        "- 강사의 주관적인 판단으로 압축해서 자료를 정리하였기 때문에, 자세하게 공부를 하고 싶으신 분은 반드시 교재를 구매하실 것을 권해드립니다. \n",
        "\n",
        "![](/img/tensorflow2.0/book.jpg)<!-- -->\n",
        "\n",
        "\n",
        "- 본 교재 외에 강사가 추가한 내용에 대한 Reference를 확인하셔서, 추가적으로 학습하시는 것을 권유드립니다. \n",
        "\n",
        "\n",
        "## Tutorial\n",
        "\n",
        "이전 강의가 궁금하신 분들은 아래에서 선택하여 추가 학습 하시기를 바랍니다. \n",
        "\n",
        "- [Google Colab Tensorflow 2.0 Installation](https://chloevan.github.io/python/tensorflow2.0/googlecolab/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.1 - 난수 생성 및 시그모이드 함수](https://chloevan.github.io/python/tensorflow2.0/ch3_3_1_random_signoid/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.2 - 난수 생성 및 시그모이드 함수 편향성](https://chloevan.github.io/python/tensorflow2.0/ch3_3_2_random_signoid_bias/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.3 - 첫번째 신경망 네트워크 - AND](https://chloevan.github.io/python/tensorflow2.0/ch3_3_3_network_and/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.4 - 두번째 신경망 네트워크 - OR](https://chloevan.github.io/python/tensorflow2.0/ch3_3_4_network_or/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.5 - 세번째 신경망 네트워크 - XOR](https://chloevan.github.io/python/tensorflow2.0/ch3_3_5_network_xor/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.1 - 선형회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_1_linear_regression/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.2 - 다항회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_2_multiple_linear_regression/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.3 - 딥러닝 네트워크를 이용한 회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_3_regression_with_deeplearning/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.4 - 보스턴 주택 가격 데이터세트](https://chloevan.github.io/python/tensorflow2.0/ch4_4_boston_housing_deeplearning/)\n",
        "- [Tensorflow 2.0 Tutorial ch5.1 - 분류](https://chloevan.github.io/python/tensorflow2.0/ch5_1_binary_classification/)\n",
        "- [Tensorflow 2.0 Tutorial ch5.2 - 다항분류](https://chloevan.github.io/python/tensorflow2.0/ch5_2_multi_classification/)\n",
        "- [Tensorflow 2.0 Tutorial ch5.3 - Fashion MNIST](https://chloevan.github.io/python/tensorflow2.0/ch5_3_fashion_mnist/)\n",
        "- [Tensorflow 2.0 Tutorial ch6.1-2 - CNN 이론](https://chloevan.github.io/python/tensorflow2.0/ch6_1_2_cnn_theory/)\n",
        "- [Tensorflow 2.0 Tutorial ch6.3 - Fashion MNIST with CNN 실습](https://chloevan.github.io/python/tensorflow2.0/ch6_3_fashion_mnist_with_cnn/)\n",
        "- [Tensorflow 2.0 Tutorial ch6.4 - 모형의 성능 높이기](https://chloevan.github.io/python/tensorflow2.0/ch6_4_improve_performance/)\n",
        "- [Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (1)](https://chloevan.github.io/python/tensorflow2.0/ch7_1_2_rnn_theory1/)\n",
        "- [Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (2)](https://chloevan.github.io/python/tensorflow2.0/ch7_1_2_rnn_theory2/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcD2TKoazzaJ",
        "colab_type": "text"
      },
      "source": [
        "## I. 개요\n",
        "\n",
        "감성 분석은 입력된 자연어 안의 주관적 의견, 감정 등을 찾아내는 문제입니다. 문장의 긍정/부정이나 긍정/중립/부정을 분류합니다. \n",
        "\n",
        "영화 리뷰나 음식점 리뷰는 데이터의 양이 많고 별점을 함께 달기 때문에 긍정/중립/부정 라벨링이 쉬워서 극성 감성 분석에 쉽게 적용이 가능합니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4iORyGw2waj",
        "colab_type": "text"
      },
      "source": [
        "## II. 감정분석 소스 작성 및 설명\n",
        "네이버의 박은정 박사가 2015년에 발표한 \"Naver Sentiment Movie Corpus v1.0\"을 이용해 긍정/부정 감성 분석을 해봅니다. \n",
        "\n",
        "여기에는 훈련 데이터로 15만개, 테스트 데이터로 5만개로 총 20만개의 리뷰가 있습니다. \n",
        "\n",
        "리뷰 중 10만 개븐 별점이 1-4로 부정적인 리뷰이고, 나머지 10만개는 9-10으로 긍정적인 리뷰입니다. 별점 5-8에 해당하는 리뷰는 중립적이라고도 볼 수 있지만, 데이터 세트에서는 제외합니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GMR8X5K3C3n",
        "colab_type": "text"
      },
      "source": [
        "### (1) 데이터 로드\n",
        "박은정 박사 깃허브에 올라와 있는 데이터를 가져오도록 합니다. 별도 파일을 로컬로 내려받을 필요 없이 직접 깃허브에서 가져와서 구글 코랩에 연동할 수 있도록 하는 코드입니다. 특히 딥러닝 예제는 일반적인 머신러닝과 달리 데이터의 양이 클 수 밖에 없습니다. 그러니, 꼭 참조하시기를 바랍니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1gWw2ee1tMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 텐서플로 2 버전 선택\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEdUWPfI1th4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b9a41305-7cae-4123-ca4d-1bac690c05fe"
      },
      "source": [
        "path_to_train_file = tf.keras.utils.get_file('train.txt', 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\n",
        "path_to_test_file = tf.keras.utils.get_file('test.txt', 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
            "14630912/14628807 [==============================] - 0s 0us/step\n",
            "Downloading data from https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
            "4898816/4893335 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2I3uySc2tBG",
        "colab_type": "text"
      },
      "source": [
        "다운로드가 완료되면 데이터를 메모리에 불러옵니다. 이 때 데이터가 어떻게 생겼는지 간단하게 확인해 볼 수 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlgmgrnU4JBi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "74b6763c-a81d-40c0-befb-e9145e0cafaf"
      },
      "source": [
        "train_text = open(path_to_train_file, 'rb').read().decode(encoding='utf-8')\n",
        "test_text = open(path_to_test_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# 텍스트가 총 몇 자인지 확인해봅니다. \n",
        "print('Length of text: {} characters'.format(len(train_text)))\n",
        "print(('Length of text: {} characters'.format(len(test_text))))\n",
        "\n",
        "# 처음 300자를 확인해봅니다. \n",
        "print(train_text[:300])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 6937271 characters\n",
            "Length of text: 2318260 characters\n",
            "id\tdocument\tlabel\n",
            "9976970\t아 더빙.. 진짜 짜증나네요 목소리\t0\n",
            "3819312\t흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\t1\n",
            "10265843\t너무재밓었다그래서보는것을추천한다\t0\n",
            "9045019\t교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\t0\n",
            "6483659\t사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\t1\n",
            "5403919\t막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.\t0\n",
            "7797314\t원작의\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w94Ew_dS5XVW",
        "colab_type": "text"
      },
      "source": [
        "데이터의 각 행은 탭 문자(\\t)로 구분되어 있습니다. 처음의 `id`는 각 데이터의 고유한 `id`이고, document는 실제 리뷰 내용입니다. `label`은 긍정/부정을 나타내는 값으로, 0은 부정, 1은 긍정입니다. \n",
        "\n",
        "책에는 기술되어 있지 않은 실무적인 내용을 조금 기술합니다 (물론 필자의 주관적인 견해입니다). 국내외 딥러닝의 연구 및 적용 사례는 대기업 수준에서는 활발하게 이루어지고 있지만, 실제 대다수가 사용되어야 할 일반적인 쇼핑몰 등에서는 거의 사용되지 않고 있습니다. \n",
        "\n",
        "감정 분석의 비즈니스 가치가 일반적인 소규모 기업에서는 매우 작을수도 있습니다. 그러나, 그럼에도 불구하고, 감정 분석은 각 제품 또는 기업의 이미지 개선에 많은 도움을 주는 것은 뻔합니다. \n",
        "\n",
        "그런데, 여기서 데이터상으로 말씀을 드리면, 위 데이터셋은 매우 깔끔하게 처리된 데이터셋입니다. 데이터셋 전처리의 End-Point는 위 데이터셋으로 진행하지만, 문제는 `Labeling`입니다. \n",
        "\n",
        "그런데, 초기 댓글에는 라벨링이 존재하지 않습니다. 즉, 이 때 초기 데이터셋을 뽑아서 라벨링을 진행해주셔야 합니다. (수동으로)\n",
        "\n",
        "초기 수동으로 뽑은 데이터로 학습을 시킨 후, 계속 나오는 댓글을 테스트 데이터로 확인 후 재 라벨링하는 방법으로 오류를 개선하고 데이터층을 쌓는 노력을 계속해야 합니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-loW86g7e7W",
        "colab_type": "text"
      },
      "source": [
        "### (2) 학습을 위한 정답 데이터 (Y) 만들기\n",
        "이제 학습을 위한 훈련 데이터와 테스트 데이터를 만들어 봅니다. 입력(X)에 해당하는 자연어의 처리는 복잡한 과정이기 때문에 조금 뒤에서 다루도록 하고 일단 0, 1만 존재하는 출력(Y)부터 처리해 봅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPhQPzLA7c3M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "652dd87c-a925-44ef-f3da-65cbc3017ff5"
      },
      "source": [
        "# 7.21 학습을 위한 정답 데이터(Y) 만들기\n",
        "train_Y = np.array([[int(row.split('\\t')[2])] for row in train_text.split('\\n')[1:] if row.count('\\t') > 0])\n",
        "test_Y = np.array([[int(row.split('\\t')[2])] for row in test_text.split('\\n')[1:] if row.count('\\t') > 0])\n",
        "\n",
        "print(train_Y.shape, test_Y.shape)\n",
        "print(train_Y[:5])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150000, 1) (50000, 1)\n",
            "[[0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjodf39_8WJM",
        "colab_type": "text"
      },
      "source": [
        "train_Y, test_Y를 구하는 방법은, \n",
        "- 먼저 각 텍스트를 개행 문자(`\\n`)로 분리한 다음 헤더에 해당하는 부분(id document label)을 제외한 나머지([1:])에 대해 각 행을 처리합니다. \n",
        "- 각 행은 탭 문자(`\\t`)로 나눠진 후에 2번째 원소(파이썬은 0부터 숫자를 셉니다. 실제로는 3번째 원소입니다)를 정수(integer)로 변환해서 저장합니다. \n",
        "- 마지막에는 `np.array`로 결과 리스트를 감싸서 네트워크에 입력하기 쉽게 만듭니다. \n",
        "\n",
        "훈련 데이터의 `Y`의 첫 원소 다섯 개를 출력해보면 정답 라벨이 잘 들어있음을 확인할 수 있습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f617xRjg9qXJ",
        "colab_type": "text"
      },
      "source": [
        "### (3) 훈련 데이터의 정제\n",
        "입력으로 쓸 자연어를 토큰화(Tokenization)하고 정제(Cleansing)를 해야 합니다. 토큰화는 자연어를 처리 가능한 작은 단위로 나누는 것이고, 여기서는 단어를 사용하는 것이기 때문에 띄어쓰기 단위로 나누면 됩니다. \n",
        "\n",
        "정제란 원하지 않는 입력이나 불필요한 기호 등을 제거하는 것입니다. 정제를 위한 함수로는 김윤 박사의 `CNN_sentence` 깃허브 저장소[^1]의 코드를 사용합니다. \n",
        "- 원 소스코드는 `process_data.py`에 있습니다. \n",
        "\n",
        "````markdown\n",
        "import re\n",
        "\n",
        "def clean_str(string, TREC=False):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Every dataset is lower cased except for TREC\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
        "    string = re.sub(r\",\", \" , \", string) \n",
        "    string = re.sub(r\"!\", \" ! \", string) \n",
        "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
        "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
        "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
        "\n",
        "    return string.strip() if TREC else string.strip().lower()\n",
        "````\n",
        "\n",
        "우선 위 코드를 기반으로 코드를 작성합니다. \n",
        "\n",
        "[^1]: 출처: https://github.com/yoonkim/cnn_sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXTNEHAS-zgp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f273e6af-8e90-49b5-d3dc-eb88318ca722"
      },
      "source": [
        "# 7.22 train 데이터의 입력(X)에 대한 정제(Cleaning)\n",
        "import re\n",
        "# From https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "\n",
        "def clean_str(string):    \n",
        "    string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    string = re.sub(r\"\\'{2,}\", \"\\'\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string)\n",
        "\n",
        "    return string.lower()\n",
        "\n",
        "train_text_X = [row.split('\\t')[1] for row in train_text.split('\\n')[1:] if row.count('\\t') > 0]\n",
        "train_text_X = [clean_str(sentence) for sentence in train_text_X]\n",
        "# 문장을 띄어쓰기 단위로 단어 분리\n",
        "sentences = [sentence.split(' ') for sentence in train_text_X]\n",
        "for i in range(5):\n",
        "    print(sentences[i])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['아', '더빙', '진짜', '짜증나네요', '목소리']\n",
            "['흠', '포스터보고', '초딩영화줄', '오버연기조차', '가볍지', '않구나']\n",
            "['너무재밓었다그래서보는것을추천한다']\n",
            "['교도소', '이야기구먼', '솔직히', '재미는', '없다', '평점', '조정']\n",
            "['사이몬페그의', '익살스런', '연기가', '돋보였던', '영화', '!', '스파이더맨에서', '늙어보이기만', '했던', '커스틴', '던스트가', '너무나도', '이뻐보였다']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-kO2aXWA1PA",
        "colab_type": "text"
      },
      "source": [
        "`clean_str(string)` 함수는 다수의 정규표현식을 사용하고 있습니다만 첫 줄을 제외하면 세 번째 인수인 `string`에서 첫 번째 인수에 해당하는 내용을 찾아서 두 번째 인수로 단순히 교체해주는 것입니다. \n",
        "\n",
        "```python\n",
        "re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "```\n",
        "\n",
        "위 정규표현식에서 특이한 점은 대괄호([])로 묶은 부분의 처음에 `^`가 들어가 있다는 점입니다. 이 기호는 대괄호 안의 내용을 찾은 다음에, 그에 포함되지 않는 나머지 모두를 선택한다는 뜻입니다. 즉, 한글, 영문, 숫자, 괄호, 쉽표, 느낌표, 물음표, 작은따옴표('), 역따옴표(`)를 제외한 나머지는 모두 찾아서 공백(\" \")으로 바꾸겠다는 뜻입니다.[^2]\n",
        "\n",
        "훈련 데이터의 처음 다섯 개를 출력해보면 구두점(.) 기호가 삭제된 것을 확인할 수 있습니다. 그런데, 아시다시피, 네트워크에 입력하려면 데이터의 크기(문장의 길이)는 동일해야 하는데, 그렇지 않습니다. 긴 문장은 줄이고, 짧은 문장에는 공백을 의미하는 (`padding`)을 채워 넣어야 합니다. \n",
        "\n",
        "각 문장의 길이를 그래프로 그려봅니다. \n",
        "\n",
        "[^2]: 파이썬 정규표현식 온라인 테스트: https://regex101.com/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0fTydmvCUTL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "03caafee-8d99-48d5-fe18-e17af3e02265"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "sentence_len = [len(sentence) for sentence in sentences]\n",
        "sentence_len.sort()\n",
        "plt.plot(sentence_len)\n",
        "plt.show()\n",
        "\n",
        "print(sum([int(l<=25) for l in sentence_len]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "142587\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCHaPAgzCjlz",
        "colab_type": "text"
      },
      "source": [
        "### (4) 단어의 정제 및 문장 전처리\n",
        "\n",
        "그래프의 Y축은 문장의 단어 개수입니다. 15만 개의 문장 중에서 대부분이 40단어 이하로 구성되어 있음을 확인할 수 있습니다. 특히 25단어 이하인 문장의 수는 `142,587`개로 전체의 95% 정도입니다. 따라서 기준이 되는 문장의 길이를 25단어로 잡고 이 이상은 생략, 이 이하는 패딩으로 길이를 25로 맞춰면 임베딩 레이어에 넣을 준비가 끝납니다. \n",
        "\n",
        "또 하나 고려해야 하는 것은 각 단어의 최대 길이를 조정하는 일입니다. 예를 들면, 훈련 데이터의 5번째 문장에서 `스파이더맨에서`라는 단어가 있는데, 이 단어는 엄밀히 말하면 조사를 제거하면 `스파이더맨`이라는 한 단어가 됩니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oknr0DkXDwu2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "658eebc2-b60a-4bc6-8aa4-2bcfaf3ca890"
      },
      "source": [
        "sentence_new = []\n",
        "for sentence in sentences:\n",
        "  sentence_new.append([word[:5] for word in sentence][:25])\n",
        "\n",
        "sentences = sentence_new\n",
        "\n",
        "for i in range(5):\n",
        "  print(sentences[i])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['아', '더빙', '진짜', '짜증나네요', '목소리']\n",
            "['흠', '포스터보고', '초딩영화줄', '오버연기조', '가볍지', '않구나']\n",
            "['너무재밓었']\n",
            "['교도소', '이야기구먼', '솔직히', '재미는', '없다', '평점', '조정']\n",
            "['사이몬페그', '익살스런', '연기가', '돋보였던', '영화', '!', '스파이더맨', '늙어보이기', '했던', '커스틴', '던스트가', '너무나도', '이뻐보였다']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpuXnZPoECPE",
        "colab_type": "text"
      },
      "source": [
        "단어의 길이가 최대 다섯 글자로 줄어든 것을 확인 할 수 있습니다. \n",
        "\n",
        "이제 앞에서 설명한 작업 중에서 짧은 문장을 같은 길이의 문장(25단어)으로 바꾸기 위한 패딩을 넣기 위해 `tf.keras`에서 제공하는 `pad_sequences`를 사용해봅니다. 또 모든 단어를 사용하지 않고 출현 빈도가 가장 높은 일부 단어만 사용하기 위해 `Tokenizer`도 같이 병행해서 사용합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWJt5dMvEYbX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "4b94928d-c827-446a-cf23-43bf48411aa9"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=2000)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "train_X = tokenizer.texts_to_sequences(sentences)\n",
        "train_X = pad_sequences(train_X, padding='post')\n",
        "\n",
        "print(train_X[:5])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  25  884    8 1111    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]\n",
            " [ 588    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]\n",
            " [  71  346   31   35    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]\n",
            " [ 106    4    2  869  573    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7jB-LzuE-LZ",
        "colab_type": "text"
      },
      "source": [
        "- `Tokenizer`는 데이터에 출현하는 모든 단어의 개수를 세고 빈도 수로 정렬해서 `num_words`에 지정된 만큼만 숫자로 반환하고 나머지는 0으로 반환합니다. \n",
        "- `tokenizer.fit_on_texts(sentences)`는 `Tokenizer`에 데이터를 실제로 입력합니다.\n",
        "- `tokenizer.texts_to_sequences(sentence)`는 문장을 입력받아 숫자를 반환합니다. \n",
        "- 마지막으로 `pad_sequences()`는 입력된 데이터에 패딩을 더합니다. \n",
        "- `pad_sequences()`의 인수에는 `pre` & `post`가 있는데, `pre`는 문장의 앞에 패딩을 넣고, `post`는 문장의 뒤에 패딩을 넣습니다. 여기에서는 `post`를 사용합니다. \n",
        "\n",
        "이렇게 정제된 데이터는 보시다시피 숫자로 변환이 되는 것입니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTv87pyoGZzk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "c98c023b-12f7-4bc4-be0b-172bcfda73f0"
      },
      "source": [
        "for idx, word in enumerate(range(1,26), 1):\n",
        "  print(idx, tokenizer.index_word[word])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 \n",
            "2 !\n",
            "3 ,\n",
            "4 영화\n",
            "5 \\?\n",
            "6 너무\n",
            "7 정말\n",
            "8 진짜\n",
            "9 이\n",
            "10 그냥\n",
            "11 왜\n",
            "12 이런\n",
            "13 더\n",
            "14 수\n",
            "15 영화를\n",
            "16 다\n",
            "17 잘\n",
            "18 보고\n",
            "19 좀\n",
            "20 영화는\n",
            "21 영화가\n",
            "22 그\n",
            "23 봤는데\n",
            "24 본\n",
            "25 아\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt2eGHLFJqOw",
        "colab_type": "text"
      },
      "source": [
        "각 번호마다 매칭되는 한글을 보려면 위와 같은 코드로 구현이 가능합니다. `range(1, 26)`에서 26을 바꾸면 원하는 범위까지 출력이 가능합니다. 이제 본격적으로 딥러닝 소스코드를 구현해서 모형을 만들도록 합니다. \n",
        "\n",
        "### (5) 딥러닝 모형 정의 및 학습 \n",
        "이제 실제로 네트워크를 정의하고 학습시켜봅니다. 먼저 임베딩 레이어와 `LSTM`레이어를 연결한 뒤 마지막에 `Dense`레이어의 소프트맥스 활성화 함수를 사용해 긍정/부정을 분류하는 네트워크를 정의합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSrAPo6EKj_f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "6f76fe1b-9e49-4c66-ca53-05d3fcb76ce7"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(20000, 300, input_length=25), \n",
        "  tf.keras.layers.LSTM(units=50), \n",
        "  tf.keras.layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 25, 300)           6000000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 50)                70200     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 102       \n",
            "=================================================================\n",
            "Total params: 6,070,302\n",
            "Trainable params: 6,070,302\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxnBwsBqLH5P",
        "colab_type": "text"
      },
      "source": [
        "input_length 인수가 중요합니다. 데이터 전처리를 25기준으로 정해놨기 때문에, `input_length`로 정의했습니다. \n",
        "\n",
        "`희소행렬(sparse_categorical_crossentropy)`에 관한 내용은 튜토리얼 5장-6장을 확인하여 주시기 바랍니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaV-2qSdLlW5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "5aafb70c-b731-4417-a18a-1db20b36b715"
      },
      "source": [
        "history = model.fit(train_X, train_Y, epochs=5, batch_size=128, validation_split=0.2)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "938/938 [==============================] - 69s 73ms/step - loss: 0.4898 - accuracy: 0.7383 - val_loss: 0.4517 - val_accuracy: 0.7685\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 68s 72ms/step - loss: 0.4481 - accuracy: 0.7649 - val_loss: 0.4469 - val_accuracy: 0.7707\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 68s 73ms/step - loss: 0.4358 - accuracy: 0.7708 - val_loss: 0.4551 - val_accuracy: 0.7669\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 68s 73ms/step - loss: 0.4277 - accuracy: 0.7756 - val_loss: 0.4567 - val_accuracy: 0.7589\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 69s 73ms/step - loss: 0.4196 - accuracy: 0.7802 - val_loss: 0.4511 - val_accuracy: 0.7699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onCsCiU2Lt8a",
        "colab_type": "text"
      },
      "source": [
        "데이터가 많기 때문에 한번에 학습하는 데이터의 양인 batch_size를 128로 설정했고, 5에포크만 학습을 시킵니다. 학습 과정에서 `loss`는 꾸준히 감소하지만 `val_loss`는 점점 증가하는 것을 확인할 수 있습니다. 이는 네트워크가 과적합되고 있다는 것을 의미합니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNjSAPPJMFNA",
        "colab_type": "text"
      },
      "source": [
        "### (6) 모형 결과 시각화\n",
        "학습 결과를 시각화로 확인해봅니다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNtgqQNEMT5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], 'b-', label='loss')\n",
        "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], 'g-', label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], 'k--', label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylim(0.7, 1)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-Ur2rMrMWJW",
        "colab_type": "text"
      },
      "source": [
        "왼쪽 그래프에서 `val_loss`는 증가하는데 비해, 오른쪽 그래프에서는 `val_accuracy`가 떨어지는 것으로 보아 네트워크가 과적합되는 것으로 보입니다. \n",
        "\n",
        "과적합의 이유는 임베딩 레이어를 랜덤한 값에서부터 시작해서 학습시키기 때문에 각 단어를 나타내는 벡터의 품질이 좋지 않아서입니다. 이를 개선하기 위해서는 임베딩 레이어를 별도로 학습시켜서 네트워크에 불러와서 사용하거나 `RNN`이 아닌 `CNN`을 사용하는 방법이 있습니다. \n",
        "\n",
        "이 부분은 추후 자료가 정리가 되면 추가적으로 기술하도록 합니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iAOmkMbNgCt",
        "colab_type": "text"
      },
      "source": [
        "### (7) 학습 결과 테스트\n",
        "학습된 네트워크에서 테스트 데이터는 어떻게 평가를 할까요? 확인을 위해 `test_text`에도 `train_text`와 같은 변환 과정을 거쳐서 `test_X`를 만듭니다. \n",
        "\n",
        "여기에서 한가지 주목해야 하는 것은 `train_X`를 만들 때 학습시킨 `Tokenizer`를 어떤 변경 없이 그대로 사용한다는 것입니다. \n",
        "\n",
        "이렇게 하는 이유는 테스트 데이터는 우리 손에 없다는 가정하에 작업을 진행합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX3M5iFyOCdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36fec4e2-863e-4776-a76b-995f89c4f137"
      },
      "source": [
        "test_text_X = [row.split('\\t')[1] for row in test_text.split('\\n')[1:] if row.count('\\t') > 0]\n",
        "test_text_X = [clean_str(sentence) for sentence in test_text_X]\n",
        "\n",
        "# 문장을 띄어쓰기 단위로 단어 분리\n",
        "sentences = [sentence.split(' ') for sentence in test_text_X]\n",
        "sentence_new = []\n",
        "for sentence in sentences:\n",
        "  sentence_new.append([word[:5] for word in sentence][:25])\n",
        "\n",
        "sentences = sentence_new\n",
        "\n",
        "test_X = tokenizer.texts_to_sequences(sentences)\n",
        "test_X = pad_sequences(test_X, padding='post')\n",
        "\n",
        "model.evaluate(test_X, test_Y, verbose=0)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4587577283382416, 0.7637199759483337]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTUAGFfoPIrZ",
        "colab_type": "text"
      },
      "source": [
        "테스트 데이터의 정확도는 약 76%로 나왔습니다. 이는 검증 데이터와 비슷한 값입니다. 그렇다면 임의의 문장에 대한 감성 분석은 어떨까요? 순환 신경망이 입력의 변화에 따라 값이 변한다는 것을 확인하기 위해 하나의 문장을 잘라서 앞에서부터 차례로 입력합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3ZwyL3rPcjl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "27b1e145-204c-424d-b789-21831a37d099"
      },
      "source": [
        "test_sentence = '재미있을 줄 알았는데 완전 실망했다. 너무 졸리고 돈이 아까웠다.'\n",
        "test_sentence = test_sentence.split(' ')\n",
        "test_sentences = []\n",
        "now_sentence = []\n",
        "for word in test_sentence:\n",
        "    now_sentence.append(word)\n",
        "    test_sentences.append(now_sentence[:])\n",
        "    \n",
        "test_X_1 = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_X_1 = pad_sequences(test_X_1, padding='post', maxlen=25)\n",
        "prediction = model.predict(test_X_1)\n",
        "for idx, sentence in enumerate(test_sentences):\n",
        "    print(sentence)\n",
        "    print(prediction[idx])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['재미있을']\n",
            "[0.5659361  0.43406394]\n",
            "['재미있을', '줄']\n",
            "[0.55835325 0.44164675]\n",
            "['재미있을', '줄', '알았는데']\n",
            "[0.56154287 0.4384571 ]\n",
            "['재미있을', '줄', '알았는데', '완전']\n",
            "[0.5674858  0.43251416]\n",
            "['재미있을', '줄', '알았는데', '완전', '실망했다.']\n",
            "[0.5674858  0.43251416]\n",
            "['재미있을', '줄', '알았는데', '완전', '실망했다.', '너무']\n",
            "[0.64270234 0.35729766]\n",
            "['재미있을', '줄', '알았는데', '완전', '실망했다.', '너무', '졸리고']\n",
            "[0.64270234 0.35729766]\n",
            "['재미있을', '줄', '알았는데', '완전', '실망했다.', '너무', '졸리고', '돈이']\n",
            "[0.98596686 0.0140331 ]\n",
            "['재미있을', '줄', '알았는데', '완전', '실망했다.', '너무', '졸리고', '돈이', '아까웠다.']\n",
            "[0.98596686 0.0140331 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMMVRf6vPkAS",
        "colab_type": "text"
      },
      "source": [
        "출력은 문장의 변화에 따른 감성 분석 예측 결과입니다. 단어의 문장이 길어지면 길어질수록 정확도가 올라가는데, 특이한 것이 있다면, `너무`라는 단어 `졸리고`가 나왔을 때 99%의 확률로 부정적 감성을 예측하는 것을 볼 수 있습니다. \n",
        "\n",
        "영화의 특성상, `졸립다`라는 특성은 사실 굉장히 많은 부정적인 뜻을 내포하기 때문에 어쩌면 대표적인 단어일 수도 있다는 생각을 해봅니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIplmAzGoXTf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## III. 연습 파일\n",
        "- [구글 Colab에서 직접 연습해보자](https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch7_3_sentimentAnalysis.ipynb) \n",
        "\n",
        "## IV. Reference\n",
        "\n",
        "김환희. (2020). 시작하세요! 텐서플로 2.0 프로그래밍: 기초 이론부터 실전 예제까지 한번에 끝내는 머신러닝, 딥러닝 핵심 가이드. 서울: 위키북스."
      ]
    }
  ]
}