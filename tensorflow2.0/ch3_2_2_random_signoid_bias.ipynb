{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch3_2_2_random_signoid_bias.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPfjXKdoOrMFRBYHLzQjrA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch3_2_2_random_signoid_bias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36chBXAjgL6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sY4zY8bgrM8",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "title: \"Tensorflow 2.0 Tutorial ch3.2.2 - 난수 생성 및 시그모이드 함수 편향성\"\n",
        "date: 2020-04-05T11:20:30+09:00\n",
        "tags:\n",
        "  - \"Deep Learning\"\n",
        "  - \"Python\"\n",
        "  - \"Google Colab\"\n",
        "  - \"Tensorflow 2.0\"\n",
        "  - \"시그노이드\"\n",
        "  - \"Random Signoid\"\n",
        "  - \"Random Signoid Bias\"\n",
        "  - \"텐서플로 2.0\"\n",
        "  - \"텐서플로 2.0 튜토리얼\"\n",
        "  - \"Tensorflow 2.0 Tutorial\"\n",
        "categories:\n",
        "  - \"Deep Learning\"\n",
        "  - \"딥러닝\"\n",
        "  - \"텐서플로 2.0\"\n",
        "  - \"Python\"\n",
        "  - \"Tensorflow 2.0\"\n",
        "  - \"텐서플로 2.0 튜토리얼\"\n",
        "  - \"Tensorflow 2.0 Tutorial\"\n",
        "menu: \n",
        "  python:\n",
        "    name: Tensorflow 2.0 Tutorial ch3.2 - 난수 생성 및 시그모이드 함수\n",
        "---\n",
        "\n",
        "## 공지\n",
        "\n",
        "- 본 Tutorial은 `시작하세요 텐서플로 2.0 프로그래밍`의 강사에게 국비교육 강의를 듣는 사람들에게 자료 제공을 목적으로 제작하였습니다. \n",
        "- 강사의 주관적인 판단으로 압축해서 자료를 정리하였기 때문에, 자세하게 공부를 하고 싶은 반드시 교재를 구매하실 것을 권해드립니다. \n",
        "- 본 교재 외에 강사가 추가한 내용에 대한 Reference를 확인하셔서, 추가적으로 학습하시는 것을 권유드립니다. \n",
        "\n",
        "## Tutorial\n",
        "\n",
        "이전 강의가 궁금하신 분들은 아래에서 선택하여 추가 학습 하시기를 바랍니다. \n",
        "\n",
        "- [Google Colab Tensorflow 2.0 Installation](https://chloevan.github.io/python/tensorflow2.0/googlecolab/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.2 - 난수 생성 및 시그모이드 함수](https://chloevan.github.io/python/tensorflow2.0/ch3_2_random_signoid/)\n",
        "\n",
        "## I. 편향성 (Bias)\n",
        "\n",
        "지난 시간에 경사하강법 원리를 통해 오차가 적어지는 것을 확인할 수 있었다. \n",
        "\n",
        "$w = w + x \\times a \\times error$\n",
        "\n",
        "여기에서 입력으로 0을 넣게 되면 출력으로 1을 얻는 뉴런은 어떻게 만들 수 있을까? 지난시간에 배운 내용으로 확인해보자. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mLI7M5unSqW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88a3647b-694e-453f-e3c9-4501d1b54df8"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_Q1nj4io7x3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def sigmoid(x): \n",
        "  return 1 / (1 + math.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgsJMMi1o4NX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "63e0ebdb-ad83-4265-c360-835c3ccc86c1"
      },
      "source": [
        "x = 0\n",
        "y = 1\n",
        "w = tf.random.normal([1], 0, 1)\n",
        "\n",
        "for i in range(1000): \n",
        "  output = sigmoid(x * w)\n",
        "  error = y - output\n",
        "  w = w + x * 0.1 * error\n",
        "\n",
        "  if i % 100 == 99:\n",
        "    print(i, error, output)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 0.5 0.5\n",
            "199 0.5 0.5\n",
            "299 0.5 0.5\n",
            "399 0.5 0.5\n",
            "499 0.5 0.5\n",
            "599 0.5 0.5\n",
            "699 0.5 0.5\n",
            "799 0.5 0.5\n",
            "899 0.5 0.5\n",
            "999 0.5 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-SSCMdgpTYZ",
        "colab_type": "text"
      },
      "source": [
        "경사 하강법의 원리를 적용했지만, `error`가 변하지 않았고, 출력도 변하지 않았다. 수식을 기업하면, `x = 0` 이므로, `w`에 더해지는 값은 없다. `1,000`번의 실행 동안 `w`값은 변하지 않았다. \n",
        "\n",
        "> 여기에서, 책은 간단히 짚고 넘어갔지만, 왜 딥러닝에서 수학적인 원리가 중요한지 알 수 있다. 일반적으로 개발자 분들은 이러한 부분들을 크게 생각 안하는 경우가 많지만, 기능을 구현하는 것 보다 더 중요한 건 코드를 짤 때, 수학적인 원리도 같이 고민해야 하는 경우가 많다. 머신러닝도 그렇지만, 딥러닝도 다양한 인자들이 존재하고, 이를 이해하려면 공식문서를 잘 참고해야 하며, 더 좋은 성과 및 퍼포먼스를 내려면 결국엔 관련 Thesis, 즉 논문을 읽어낼줄 알아야 한다. 그래야 성과가 나온다. 딥러닝은 1년 단위로 논문이 나오는 걸 잊지 말자! \n",
        "\n",
        "그럼 어떻게 해결해야 할까? 이러한 경우를 방지하기 위해 `bias` 라는 개념이 존재한다. 간단히 설명하면 x가 0이 들어오면 대안으로 1이 들어온 것처럼 코드를 작성하는 것이다. \n",
        "\n",
        "수식에서는 관용적으로 `bias`의 앞 글자인 `b`를 쓴다. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGe8CnwerksE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "2770a8be-f863-4824-8fa9-47156b5106b9"
      },
      "source": [
        "x = 0\n",
        "y = 1\n",
        "w = tf.random.normal([1], 0, 1)\n",
        "b = tf.random.normal([1], 0, 1)\n",
        "\n",
        "for i in range(1000): \n",
        "  output = sigmoid(x * w + 1 * b)\n",
        "  error = y - output\n",
        "  w = w + x * 0.1 * error\n",
        "  b = b + 1 * 0.1 * error\n",
        "\n",
        "  if i % 100 == 99:\n",
        "    print(i, error, output)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 0.09964988125873331 0.9003501187412667\n",
            "199 0.051655392657922405 0.9483446073420776\n",
            "299 0.03453200474689977 0.9654679952531002\n",
            "399 0.025856559869467777 0.9741434401305322\n",
            "499 0.020637549126996557 0.9793624508730034\n",
            "599 0.017159579370033873 0.9828404206299661\n",
            "699 0.014678743824020679 0.9853212561759793\n",
            "799 0.01282129876032978 0.9871787012396702\n",
            "899 0.011379070366788868 0.9886209296332111\n",
            "999 0.010227240359459322 0.9897727596405407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxuChwP8r_xk",
        "colab_type": "text"
      },
      "source": [
        "편향을 나타내는 `b`가 추가되었고, `w`처럼 초기화를 진행한다. 그리고, `sigmoid()` 안에 각 입력에 가중치와 편향을 곱해서 더해준 뒤 시그모이드 함수를 취한다. 기대출력과 실제출력의 차이인 `error`로 `w`와 `b`를 각각 업데이트해서 뉴런을 학습시킨다. \n",
        "\n",
        "프로그램을 실행한 결과, `error`는 0에 가까워지고, `output`은 기대출력인 1에 가까워진다. \n",
        "\n",
        "이번 시간까지는 간단하게 워밍업을 진행하였고, 다음 시간부터는 순차적으로 신경망 네트워크의 기본 원리에 대해 학습하도록 한다. \n",
        "\n",
        "## II. Reference\n",
        "\n",
        "김환희. (2020). 시작하세요! 텐서플로 2.0 프로그래밍: 기초 이론부터 실전 예제까지 한번에 끝내는 머신러닝, 딥러닝 핵심 가이드. 서울: 위키북스."
      ]
    }
  ]
}