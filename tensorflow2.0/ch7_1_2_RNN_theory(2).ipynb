{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch7_1_2_RNN_theory(2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNqgkjuxg5VMkzGCquC16IA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch7_1_2_RNN_theory(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_qDfvmcn_Za",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "title: \"Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (2)\"\n",
        "date: 2020-04-23T10:08:30+09:00\n",
        "tags:\n",
        "  - \"Deep Learning\"\n",
        "  - \"Python\"\n",
        "  - \"Google Colab\"\n",
        "  - \"Tensorflow 2.0\"\n",
        "  - \"Binary Classification\"\n",
        "  - \"Classification\"\n",
        "  - \"순환 신경망\"\n",
        "  - \"GRU 레이어\"\n",
        "  - \"RNN\"\n",
        "  - \"임베딩 레이어\"\n",
        "  - \"텐서플로 2.0\"\n",
        "  - \"텐서플로 2.0 튜토리얼\"\n",
        "  - \"Image Augmentation\"\n",
        "  - \"Tensorflow 2.0 Tutorial\"\n",
        "categories:\n",
        "  - \"Deep Learning\"\n",
        "  - \"딥러닝\"\n",
        "  - \"텐서플로 2.0\"\n",
        "  - \"Python\"\n",
        "  - \"Tensorflow 2.0\"\n",
        "  - \"텐서플로 2.0 튜토리얼\"\n",
        "  - \"Tensorflow 2.0 Tutorial\"\n",
        "menu: \n",
        "  python:\n",
        "    name: Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (2)\n",
        "---\n",
        "\n",
        "## 공지\n",
        "\n",
        "- 본 Tutorial은 교재 `시작하세요 텐서플로 2.0 프로그래밍`의 강사에게 국비교육 강의를 듣는 사람들에게 자료 제공을 목적으로 제작하였습니다. \n",
        "\n",
        "- 강사의 주관적인 판단으로 압축해서 자료를 정리하였기 때문에, 자세하게 공부를 하고 싶으신 분은 반드시 교재를 구매하실 것을 권해드립니다. \n",
        "\n",
        "![](/img/tensorflow2.0/book.jpg)<!-- -->\n",
        "\n",
        "\n",
        "- 본 교재 외에 강사가 추가한 내용에 대한 Reference를 확인하셔서, 추가적으로 학습하시는 것을 권유드립니다. \n",
        "\n",
        "\n",
        "## Tutorial\n",
        "\n",
        "이전 강의가 궁금하신 분들은 아래에서 선택하여 추가 학습 하시기를 바랍니다. \n",
        "\n",
        "- [Google Colab Tensorflow 2.0 Installation](https://chloevan.github.io/python/tensorflow2.0/googlecolab/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.1 - 난수 생성 및 시그모이드 함수](https://chloevan.github.io/python/tensorflow2.0/ch3_3_1_random_signoid/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.2 - 난수 생성 및 시그모이드 함수 편향성](https://chloevan.github.io/python/tensorflow2.0/ch3_3_2_random_signoid_bias/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.3 - 첫번째 신경망 네트워크 - AND](https://chloevan.github.io/python/tensorflow2.0/ch3_3_3_network_and/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.4 - 두번째 신경망 네트워크 - OR](https://chloevan.github.io/python/tensorflow2.0/ch3_3_4_network_or/)\n",
        "- [Tensorflow 2.0 Tutorial ch3.3.5 - 세번째 신경망 네트워크 - XOR](https://chloevan.github.io/python/tensorflow2.0/ch3_3_5_network_xor/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.1 - 선형회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_1_linear_regression/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.2 - 다항회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_2_multiple_linear_regression/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.3 - 딥러닝 네트워크를 이용한 회귀](https://chloevan.github.io/python/tensorflow2.0/ch4_3_regression_with_deeplearning/)\n",
        "- [Tensorflow 2.0 Tutorial ch4.4 - 보스턴 주택 가격 데이터세트](https://chloevan.github.io/python/tensorflow2.0/ch4_4_boston_housing_deeplearning/)\n",
        "- [Tensorflow 2.0 Tutorial ch5.1 - 분류](https://chloevan.github.io/python/tensorflow2.0/ch5_1_binary_classification/)\n",
        "- [Tensorflow 2.0 Tutorial ch5.2 - 다항분류](https://chloevan.github.io/python/tensorflow2.0/ch5_2_multi_classification/)\n",
        "- [Tensorflow 2.0 Tutorial ch5.3 - Fashion MNIST](https://chloevan.github.io/python/tensorflow2.0/ch5_3_fashion_mnist/)\n",
        "- [Tensorflow 2.0 Tutorial ch6.1-2 - CNN 이론](https://chloevan.github.io/python/tensorflow2.0/ch6_1_2_cnn_theory/)\n",
        "- [Tensorflow 2.0 Tutorial ch6.3 - Fashion MNIST with CNN 실습](https://chloevan.github.io/python/tensorflow2.0/ch6_3_fashion_mnist_with_cnn/)\n",
        "- [Tensorflow 2.0 Tutorial ch6.4 - 모형의 성능 높이기](https://chloevan.github.io/python/tensorflow2.0/ch6_4_improve_performance/)\n",
        "- [Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (1)](https://chloevan.github.io/python/tensorflow2.0/ch7_1_2_rnn_theory1/)\n",
        "\n",
        "\n",
        "## I. 개요\n",
        "\n",
        "GRU(`Gated Recurrent Unit`)레이어는 `LSTM`레이어와 비슷한 역할을 하지만 구조가 더 간단하기 때문에 계산상의 이점이 있고, 어떤 문제에서는 `LSTM` 레이어보다 좋은 성능을 보여주기도 합니다.[^1][^2] 셀로 나타낸 `GRU` 레이어의 계산 흐름은 `LSTM`과 비슷하지만, 조금 축약된 모습을 보입니다.[^3] \n",
        "\n",
        "![](/img/tensorflow2.0/tutorial_07_01_2/tutorial_02_RNN_LSTM_GRU.png)\n",
        "\n",
        "자세한 수식 및 이론 설명은 교재를 구매하셔서 194-5페이지를 참고하시기를 바랍니다. GRU의 성능이 LSTM보다 실제로 성능이 좋은지, 수식이 줄었기 때문에, 또한 연산속도는 빨라졌는지 확인해보도록 합니다. \n",
        "\n",
        "[^1]: Cho, K., Merrienboer, B. V., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). doi: 10.3115/v1/d14-1179\n",
        "\n",
        "[^2]: Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. In NIPS 2014 Workshop on Deep Learning, December 2014\n",
        "\n",
        "[^3]: 교재 195페이지를 확인하면 여러 수식이 나옵니다. 여기서 주목해야 하는 것은 `LSTM`레이어보다 시그모이드 함수가 하나 적게 쓰였는데, 이것은 게이트의 수가 하나 줄어들었다는 것을 의미합니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMy8bqR2h6ny",
        "colab_type": "text"
      },
      "source": [
        "## II. GRU 모델 정의 및 구현\n",
        "\n",
        "[ch7.1 - RNN(1)](https://chloevan.github.io/python/tensorflow2.0/ch7_1_2_rnn_theory1/) 이론에서 배웠던 곱셈 정의 문제를 다시 풀어보도록 합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1gWw2ee1tMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 텐서플로 2 버전 선택\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btwn-Ri2svny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "for i in range(3000): \n",
        "  # 0 ~ 1 범위의 랜덤한 숫자 100개를 만듭니다. \n",
        "  lst = np.random.rand(100)\n",
        "\n",
        "  # 마킹할 숫자 2개의 인덱스를 뽑습니다. \n",
        "  idx = np.random.choice(100, 2, replace=False)\n",
        "\n",
        "  # 마킹 인덱스가 저장된 원-핫 인코딩 벡터를 만듭니다. \n",
        "  zeros=np.zeros(100)\n",
        "  zeros[idx]=1\n",
        "  \n",
        "  # 마킹 인덱스와 랜덤한 숫자를 합쳐서 X에 저장합니다. \n",
        "  X.append(np.array(list(zip(zeros, lst))))\n",
        "  # 마킹 인덱스가 1인 값만 서로 곱해서 Y에 저장합니다. \n",
        "  Y.append(np.prod(lst[idx]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsdhUfZktGnO",
        "colab_type": "code",
        "outputId": "0cb0a542-8c8e-4b3e-d1ce-eb760fa7b6a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.GRU(units=30, return_sequences=True, input_shape=[100,2]), \n",
        "  tf.keras.layers.GRU(units=30), \n",
        "  tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    (None, 100, 30)           3060      \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 30)                5580      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 31        \n",
            "=================================================================\n",
            "Total params: 8,671\n",
            "Trainable params: 8,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikcGN38TupKx",
        "colab_type": "text"
      },
      "source": [
        "기존 `LSTM` 모델 정의 코드에서 `GRU`로 바꿔서 간단히 모델을 정의할 수 있습니다. `GRU`레이어를 사용한 네트워크의 파라미터 수는 `LSTM`레이어의 파라미터 수보다 적습니다. \n",
        "\n",
        "이전 포스트에서 작성했던 파라미터 수를 비교하면 다음과 같습니다. \n",
        "\n",
        "| SimpleRNN \t| LSTM \t| GRU \t|\n",
        "|:---------:\t|--------\t|:-----:\t|\n",
        "| 2,851 \t| 11,311 \t| 8,671 \t|\n",
        "\n",
        "그럼 이제 실제로 학습시켜서 결과가 어떻게 나오는지 확인합니다. 코드 역시, 그 전과 큰 차이점은 없기 때문에 전체 소스코드를 이어서 작성합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rONN9wJNxh71",
        "colab_type": "code",
        "outputId": "ccb7d621-1ed6-42ca-ae0d-8c99531eb2a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "# 모형 학습\n",
        "history = model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)\n",
        "\n",
        "# 모형 학습 시각화 \n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'], 'b-', label='loss')\n",
        "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 모형 테스트 및 결과\n",
        "model.evaluate(X[2560:], Y[2560:])\n",
        "prediction=model.predict(X[2560:2560+5])\n",
        "\n",
        "# 5개 테스트 데이터에 대한 예측을 표시합니다. \n",
        "for i in range(5): \n",
        "  print(Y[2560+i], '\\t', prediction[i][0], '\\tdiff:', abs(prediction[i][0] - Y[2560+i]))\n",
        "\n",
        "prediction = model.predict(X[2560:])\n",
        "fail = 0\n",
        "for i in range(len(prediction)):\n",
        "  # 오차가 0.04 이상이면 오답입니다. \n",
        "  if abs(prediction[i][0] - Y[2560+i]) > 0.04:\n",
        "    fail +=1\n",
        "\n",
        "print('correctness:', (440-fail)/440*100, '%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "64/64 [==============================] - 1s 19ms/step - loss: 0.0542 - val_loss: 0.0464\n",
            "Epoch 2/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0498 - val_loss: 0.0465\n",
            "Epoch 3/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0496 - val_loss: 0.0462\n",
            "Epoch 4/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0496 - val_loss: 0.0466\n",
            "Epoch 5/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0502 - val_loss: 0.0472\n",
            "Epoch 6/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0498 - val_loss: 0.0462\n",
            "Epoch 7/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0497 - val_loss: 0.0465\n",
            "Epoch 8/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0496 - val_loss: 0.0462\n",
            "Epoch 9/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0493 - val_loss: 0.0463\n",
            "Epoch 10/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0491 - val_loss: 0.0464\n",
            "Epoch 11/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0495 - val_loss: 0.0458\n",
            "Epoch 12/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0492 - val_loss: 0.0491\n",
            "Epoch 13/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0498 - val_loss: 0.0457\n",
            "Epoch 14/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0488 - val_loss: 0.0460\n",
            "Epoch 15/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0484 - val_loss: 0.0467\n",
            "Epoch 16/100\n",
            "64/64 [==============================] - 1s 11ms/step - loss: 0.0481 - val_loss: 0.0439\n",
            "Epoch 17/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0239 - val_loss: 0.0153\n",
            "Epoch 18/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0122 - val_loss: 0.0079\n",
            "Epoch 19/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0070 - val_loss: 0.0072\n",
            "Epoch 20/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0058 - val_loss: 0.0060\n",
            "Epoch 21/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0045 - val_loss: 0.0033\n",
            "Epoch 22/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0035 - val_loss: 0.0023\n",
            "Epoch 23/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0030 - val_loss: 0.0022\n",
            "Epoch 24/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0024 - val_loss: 0.0018\n",
            "Epoch 25/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0023 - val_loss: 0.0016\n",
            "Epoch 26/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0020 - val_loss: 0.0027\n",
            "Epoch 27/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0016 - val_loss: 0.0011\n",
            "Epoch 28/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 29/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0015 - val_loss: 0.0012\n",
            "Epoch 30/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0018 - val_loss: 0.0010\n",
            "Epoch 31/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0014 - val_loss: 0.0010\n",
            "Epoch 32/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0011 - val_loss: 9.6678e-04\n",
            "Epoch 33/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0011 - val_loss: 8.7013e-04\n",
            "Epoch 34/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0011 - val_loss: 8.7790e-04\n",
            "Epoch 35/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 9.3853e-04 - val_loss: 6.9370e-04\n",
            "Epoch 36/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 37/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0010 - val_loss: 8.4368e-04\n",
            "Epoch 38/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 9.6391e-04 - val_loss: 6.0684e-04\n",
            "Epoch 39/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 7.8758e-04 - val_loss: 0.0015\n",
            "Epoch 40/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 0.0013 - val_loss: 6.3157e-04\n",
            "Epoch 41/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 9.7273e-04 - val_loss: 5.5738e-04\n",
            "Epoch 42/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 7.0651e-04 - val_loss: 4.9343e-04\n",
            "Epoch 43/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 8.7092e-04 - val_loss: 5.3667e-04\n",
            "Epoch 44/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 6.4929e-04 - val_loss: 5.6655e-04\n",
            "Epoch 45/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 6.5691e-04 - val_loss: 5.3359e-04\n",
            "Epoch 46/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 8.4880e-04 - val_loss: 0.0017\n",
            "Epoch 47/100\n",
            "64/64 [==============================] - 1s 11ms/step - loss: 7.9048e-04 - val_loss: 4.8470e-04\n",
            "Epoch 48/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 5.2750e-04 - val_loss: 4.5355e-04\n",
            "Epoch 49/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 5.2346e-04 - val_loss: 0.0011\n",
            "Epoch 50/100\n",
            "64/64 [==============================] - 1s 11ms/step - loss: 5.2244e-04 - val_loss: 3.9339e-04\n",
            "Epoch 51/100\n",
            "64/64 [==============================] - 1s 11ms/step - loss: 5.1538e-04 - val_loss: 8.5248e-04\n",
            "Epoch 52/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 5.0478e-04 - val_loss: 3.3874e-04\n",
            "Epoch 53/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 5.4496e-04 - val_loss: 5.0151e-04\n",
            "Epoch 54/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 4.3863e-04 - val_loss: 4.1039e-04\n",
            "Epoch 55/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 7.4540e-04 - val_loss: 6.3658e-04\n",
            "Epoch 56/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 5.3960e-04 - val_loss: 3.4416e-04\n",
            "Epoch 57/100\n",
            "64/64 [==============================] - 1s 11ms/step - loss: 3.6213e-04 - val_loss: 5.8370e-04\n",
            "Epoch 58/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 4.8387e-04 - val_loss: 3.6832e-04\n",
            "Epoch 59/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 4.4107e-04 - val_loss: 3.1697e-04\n",
            "Epoch 60/100\n",
            "64/64 [==============================] - 1s 11ms/step - loss: 3.9765e-04 - val_loss: 4.5374e-04\n",
            "Epoch 61/100\n",
            "64/64 [==============================] - 1s 11ms/step - loss: 3.8165e-04 - val_loss: 3.2876e-04\n",
            "Epoch 62/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 6.8677e-04 - val_loss: 2.9420e-04\n",
            "Epoch 63/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 5.2809e-04 - val_loss: 2.7272e-04\n",
            "Epoch 64/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.7761e-04 - val_loss: 2.6852e-04\n",
            "Epoch 65/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 3.0836e-04 - val_loss: 4.0206e-04\n",
            "Epoch 66/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 4.4454e-04 - val_loss: 2.6554e-04\n",
            "Epoch 67/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 5.0457e-04 - val_loss: 2.2702e-04\n",
            "Epoch 68/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 3.1455e-04 - val_loss: 2.5735e-04\n",
            "Epoch 69/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 3.4775e-04 - val_loss: 2.4464e-04\n",
            "Epoch 70/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.5858e-04 - val_loss: 4.1233e-04\n",
            "Epoch 71/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.6686e-04 - val_loss: 3.1173e-04\n",
            "Epoch 72/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 3.2840e-04 - val_loss: 2.2496e-04\n",
            "Epoch 73/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 3.2419e-04 - val_loss: 2.0555e-04\n",
            "Epoch 74/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.3337e-04 - val_loss: 1.6153e-04\n",
            "Epoch 75/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.4482e-04 - val_loss: 1.9017e-04\n",
            "Epoch 76/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 3.5401e-04 - val_loss: 2.0578e-04\n",
            "Epoch 77/100\n",
            "64/64 [==============================] - 1s 11ms/step - loss: 2.8999e-04 - val_loss: 1.8757e-04\n",
            "Epoch 78/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.0261e-04 - val_loss: 1.6617e-04\n",
            "Epoch 79/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.2359e-04 - val_loss: 8.6003e-04\n",
            "Epoch 80/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 3.8440e-04 - val_loss: 3.4750e-04\n",
            "Epoch 81/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.7182e-04 - val_loss: 1.4401e-04\n",
            "Epoch 82/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.7468e-04 - val_loss: 1.6795e-04\n",
            "Epoch 83/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.4761e-04 - val_loss: 1.9169e-04\n",
            "Epoch 84/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.5236e-04 - val_loss: 2.6484e-04\n",
            "Epoch 85/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.3328e-04 - val_loss: 5.1060e-04\n",
            "Epoch 86/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 4.0916e-04 - val_loss: 4.0131e-04\n",
            "Epoch 87/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.9090e-04 - val_loss: 1.8709e-04\n",
            "Epoch 88/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.0704e-04 - val_loss: 1.4023e-04\n",
            "Epoch 89/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.4698e-04 - val_loss: 5.0055e-04\n",
            "Epoch 90/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.5488e-04 - val_loss: 3.7641e-04\n",
            "Epoch 91/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 3.4907e-04 - val_loss: 3.2548e-04\n",
            "Epoch 92/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 4.0255e-04 - val_loss: 4.1547e-04\n",
            "Epoch 93/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 5.1706e-04 - val_loss: 3.9099e-04\n",
            "Epoch 94/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.2027e-04 - val_loss: 1.5537e-04\n",
            "Epoch 95/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.9934e-04 - val_loss: 4.1400e-04\n",
            "Epoch 96/100\n",
            "64/64 [==============================] - 1s 12ms/step - loss: 5.6970e-04 - val_loss: 6.6119e-04\n",
            "Epoch 97/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 3.3246e-04 - val_loss: 1.8468e-04\n",
            "Epoch 98/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 2.1950e-04 - val_loss: 2.9611e-04\n",
            "Epoch 99/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 1.9392e-04 - val_loss: 1.4212e-04\n",
            "Epoch 100/100\n",
            "64/64 [==============================] - 1s 10ms/step - loss: 1.7158e-04 - val_loss: 1.1689e-04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 0s 5ms/step - loss: 1.3132e-04\n",
            "0.03254906333292209 \t 0.028674547 \tdiff: 0.003874516703731644\n",
            "0.0459566112724449 \t 0.040693775 \tdiff: 0.005262836453070817\n",
            "0.5360537450610645 \t 0.5460534 \tdiff: 0.009999664515351503\n",
            "0.14992662254277317 \t 0.14886208 \tdiff: 0.0010645437568768679\n",
            "0.38647776052320765 \t 0.38145044 \tdiff: 0.005027316063292486\n",
            "correctness: 99.31818181818181 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmbYmpqqyGfv",
        "colab_type": "text"
      },
      "source": [
        "정확도는 99.3%로 거의 99%에 가까운 값이 나옵니다. 이 문제에서는 `LSTM` 레이어보다 `GRU`레이어로 더 잘 풀리는 문제입니다. \n",
        "\n",
        "마지막 이론으로 임베딩 레이어에 대해 배우도록 합니다. \n",
        "\n",
        "## III. 임베딩 레이어 기본 이론\n",
        "임베딩 레이어(`Embedding Layer`)는 자연어를 수치화된 정보로 바꾸기 위한 레이어입니다. 자연어는 시간의 흐름에 따라 정보가 연속적으로 이어지는 시퀀스 데이터입니다. 이미지를 픽셀 단위로 잘게 쪼갤 수 있듯이 자연어도 정보를 잘게 쪼갤 수 있습니다. 영어는 문자(`character`), 한글은 문자를 넘어 자소 단위로도 쪼갤 수 있습니다. 과거에는 n-gram보다 단어나 문자 단위의 자연어 처리가 많이 쓰입니다.[^4] \n",
        "\n",
        "임베딩 레이어보다 좀 더 쉬운 기법은 자연어를 구성하는 단위에 대해 정수 인덱스(index)를 저장하는 방법입니다. 좀 더 쉽게 예로 들면, \"This is a big cat\"이라는 문장에 대해 정수 인덱스를 저장하면 처음 나오는 단어부터 인덱스를 저장합니다. \n",
        "\n",
        "| 단어 \t| 인덱스 \t|\n",
        "|:---------:\t|--------\t|\n",
        "| this \t| 0 \t|\n",
        "| is \t| 1 \t|\n",
        "| a \t| 2 \t|\n",
        "| big \t| 3 \t|\n",
        "| cat \t| 4 \t|\n",
        "\n",
        "이렇게 새로운 수치화된 데이터로 변환될 수 있습니다. 이 때 \"This is big.\"이라는 새로운 문장도 [0,1,3]이라는 데이터로 바뀔 수 있습니다. 이렇게 바뀐 데이터는 아래 그림과 같이 원-핫 인코딩을 이용해 단어의 인덱스에 해당하는 원소만 1이고 나머지는 0인 배열로 바뀝니다. \n",
        "\n",
        "![](/img/tensorflow2.0/tutorial_07_01_2/tutorial_02_word_embedding.png)\n",
        "\n",
        "\n",
        "그런데, 인덱스를 사용할 때, 가장 큰 문제점은 사용하는 메모리의 양에 비해 너무 적은 정보량을 표현하는 것이고, 또 한가지 단점은 저장된 단어의 수가 많아질수록 원-핫 인코딩 배열의 두 번째 차원의 크기도 그에 비례해서 늘어나기 때문에 이 데이터가 차지하는 메모리의 양이 더욱 늘어나게 됩니다. 추가적인 이론에 대한 내용은 교재 201-3페이지를 확인하셔서 추가적인 이론 공부를 병행하는 것을 권합니다. \n",
        "\n",
        "임베딩 레이어에 대해 학습시키는 방법은 `Word2Vec`, `GloVe`, `FastText`, `ELMo`등과 같은 방법론이 있습니다. \n",
        "\n",
        "우선 단어 임베딩의 이론적인 코드 부분을 학습하고자 예제(감성분석)를 준비했습니다. 교재에는 조금 부족한 부분이라 판단되어, 텐서플로 공식홈페이지의 내용을 번역 및 축약합니다.[^5] \n",
        "\n",
        "[^4]: N-Gram은 간단하게 예를 들어 설명하면, \"This is it\"이라는 문장을 3개의 문자를 묶은 `3-gram`으로 나타내면 [\"Thi\", \"his\", \"is \", \"s i\", \" is\", \"is \", \"s i\", \" it\", \"it.\"]이라는 배열로 나타낼 수 있습니다. \n",
        "\n",
        "[^5]: Word embeddings, https://www.tensorflow.org/tutorials/text/word_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx7_sRihqnnx",
        "colab_type": "text"
      },
      "source": [
        "### (1) 코드 작성 및 설명\n",
        "먼저 관련 모듈과 데이터를 가져옵니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgBW5Wt2qvcV",
        "colab_type": "code",
        "outputId": "d41fb60b-dc42-4365-a845-93d438b57720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "# 데이터 수집 / 영화 데이터\n",
        "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
        "                          as_supervised=True)\n",
        "train_examples, test_examples = dataset['train'], dataset['test']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset imdb_reviews/subwords8k/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0...\u001b[0m\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0.incomplete70Q64U/imdb_reviews-train.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0.incomplete70Q64U/imdb_reviews-test.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0.incomplete70Q64U/imdb_reviews-unsupervised.tfrecord\n",
            "\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDcyxWC9uB72",
        "colab_type": "text"
      },
      "source": [
        "그리고, encoder를 통해서 실제 텍스트의 단어의 크기를 확인합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3RSLU8BsHpq",
        "colab_type": "code",
        "outputId": "7723b102-01e0-424d-dec5-74ded8603c26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "encoder = info.features['text'].encoder\n",
        "print('Vocabulary size: {}'.format(encoder.vocab_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 8185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbyd1gdPuYgs",
        "colab_type": "text"
      },
      "source": [
        "이론에서 배웠던, 입력된 `Sample String`에 대해 인코딩된 값을 인덱스로 반환합니다. 그리고, 원 문자열도 같이 반환되어 어떻게 변환되는지 확인할 수 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaZAxn-YsNlV",
        "colab_type": "code",
        "outputId": "a5c35232-c921-4d4e-ae23-5ac6862b8cd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sample_string = 'Hello ChloEvan.'\n",
        "encoded_string = encoder.encode(sample_string)\n",
        "print('Encoded string is {}'.format(encoded_string))\n",
        "\n",
        "original_string = encoder.decode(encoded_string)\n",
        "print('The original string: \"{}\"'.format(original_string))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded string is [4025, 222, 6995, 1163, 6275, 8039, 7975]\n",
            "The original string: \"Hello ChloEvan.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c2y_uNmv9lG",
        "colab_type": "text"
      },
      "source": [
        "이제 다시 인덱스로 출력하면 전체 `vocab_size`에서 샘플 문자열이 어떤식으로 구성이 되는지 확인할 수 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0Qzuvqtsa5E",
        "colab_type": "code",
        "outputId": "da12e931-ac12-4ce2-e456-92b51862f72e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "assert original_string == sample_string\n",
        "for index in encoded_string:\n",
        "  print('{} ----> {}'.format(index, encoder.decode([index])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4025 ----> Hell\n",
            "222 ----> o \n",
            "6995 ----> Ch\n",
            "1163 ----> lo\n",
            "6275 ----> Eva\n",
            "8039 ----> n\n",
            "7975 ----> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-XRVF1OwOkl",
        "colab_type": "text"
      },
      "source": [
        "이제 모형을 위해 학습 데이터를 준비합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IANO0ANzsh0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = (train_examples\n",
        "                 .shuffle(BUFFER_SIZE)\n",
        "                 .padded_batch(BATCH_SIZE, padded_shapes=([None],[])))\n",
        "\n",
        "test_dataset = (test_examples\n",
        "                .padded_batch(BATCH_SIZE,  padded_shapes=([None],[])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwo9WI_hySXH",
        "colab_type": "text"
      },
      "source": [
        "그런데, 아래 코드에서 `pad`라는 개념이 보일겁니다. `pad`은 공백을 의미합니다. `padding`의 개념이 있는 것은, 자연어에는 미리 정해놓을 수 없을 정도로 많은 단어가 존재하기 때문에, 보통은 정수 인덱스로 저장하지 않는 단어에 대한 임베딩 값을 별도로 마련합니다. 즉, 임베딩 레이어의 행 수가 10,000이라면 9,999는 미리 지정된 단어의 개수이고, 나머지 1은 지정되지 않은 단어를 위한 값입니다. 이것이 padding의 개념입니다. \n",
        "\n",
        "`padded_batch`의 함수를 사용함으로써, UNK값으로 0을 넣어줍니다. 실제로 `padded_batch`가 어떻게 구현이 되는지 확인해봅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjGZGSetyffg",
        "colab_type": "code",
        "outputId": "eb270a16-eae5-49dc-f2b2-f8ae63e46e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "train_batch, train_labels = next(iter(train_dataset))\n",
        "train_batch.numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  62,    9,   41, ...,    0,    0,    0],\n",
              "       [ 134,  142, 7968, ...,    0,    0,    0],\n",
              "       [  12, 6130,    7, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [ 684,  807,  455, ...,    0,    0,    0],\n",
              "       [ 373,    6,    1, ...,    6, 1803, 7975],\n",
              "       [  62,    9,   45, ...,    0,    0,    0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X0nMopBymfD",
        "colab_type": "text"
      },
      "source": [
        "위 값에 0이 있는 것을 확인할 수 있습니다. `padded_batch`를 함으로써 일종의 길이의 정규화를 진행한다고 보면 됩니다. \n",
        "\n",
        "아래 코드는 모델 정의 및 학습에 관한 내용입니다. 튜토리얼에서 반복적으로 나오는 코드이기 때문에 여기에서는 설명을 생략합니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHrNNHbnsq7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyYEiZF3tTfD",
        "colab_type": "code",
        "outputId": "241452ba-6d3c-4840-ae14-8bb8bdec00bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "history = model.fit(train_dataset, epochs=10,\n",
        "                    validation_data=test_dataset, \n",
        "                    validation_steps=30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 44s 113ms/step - loss: 0.6474 - accuracy: 0.5586 - val_loss: 0.4672 - val_accuracy: 0.7865\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 44s 113ms/step - loss: 0.3531 - accuracy: 0.8537 - val_loss: 0.3419 - val_accuracy: 0.8589\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.2525 - accuracy: 0.9042 - val_loss: 0.3268 - val_accuracy: 0.8651\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.2089 - accuracy: 0.9218 - val_loss: 0.3332 - val_accuracy: 0.8656\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 45s 116ms/step - loss: 0.1824 - accuracy: 0.9350 - val_loss: 0.3999 - val_accuracy: 0.8130\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 0.1625 - accuracy: 0.9419 - val_loss: 0.3684 - val_accuracy: 0.8661\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 45s 116ms/step - loss: 0.1455 - accuracy: 0.9504 - val_loss: 0.3698 - val_accuracy: 0.8630\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 0.1342 - accuracy: 0.9538 - val_loss: 0.4048 - val_accuracy: 0.8594\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 45s 116ms/step - loss: 0.1222 - accuracy: 0.9594 - val_loss: 0.4135 - val_accuracy: 0.8599\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 44s 114ms/step - loss: 0.1397 - accuracy: 0.9510 - val_loss: 0.4372 - val_accuracy: 0.8542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLFu8aIozTC1",
        "colab_type": "text"
      },
      "source": [
        "모형의 학습이 끝나면 실제로 잘 학습되는지 그래프를 작성합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnTDNxTszWE4",
        "colab_type": "code",
        "outputId": "381462b8-a427-4d9c-a373-d8d46e9976df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "plt.plot(history.history['loss'], 'b-', label='loss')\n",
        "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 16s 42ms/step - loss: 0.4282 - accuracy: 0.8523\n",
            "Test Loss: 0.4281919002532959\n",
            "Test Accuracy: 0.8522800207138062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIplmAzGoXTf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## IV. 연습 파일\n",
        "- [구글 Colab에서 직접 연습해보자](https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch7_1_2_RNN_theory(2).ipynb) \n",
        "\n",
        "## VI. Reference\n",
        "\n",
        "김환희. (2020). 시작하세요! 텐서플로 2.0 프로그래밍: 기초 이론부터 실전 예제까지 한번에 끝내는 머신러닝, 딥러닝 핵심 가이드. 서울: 위키북스."
      ]
    }
  ]
}